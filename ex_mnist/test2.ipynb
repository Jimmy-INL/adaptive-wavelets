{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "from model_mnist import LeNet5\n",
    "from visualize import *\n",
    "import dset_mnist as dset\n",
    "import foolbox\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# disentangled vae\n",
    "sys.path.append('../disentangling-vae')\n",
    "from collections import defaultdict\n",
    "import vae_trim, vae_trim_viz\n",
    "from disvae.utils.modelIO import save_model, load_model, load_metadata\n",
    "from disvae.models.losses import get_loss_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-Attr-Pen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 6 # total correlation reg\n",
    "args.attr_lamb = 0 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "# L1Loss = torch.nn.MSELoss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "# saliency map\n",
    "saliency = InputXGradient(DecoderEncoder(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 50.501938\n",
      "Average loss: 45.67456041673607 - Average reg: 0\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 43.096237\n",
      "Average loss: 12.729268006932761 - Average reg: 0\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 45.908401\n",
      "Average loss: 8.173371142161681 - Average reg: 0\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 35.695770\n",
      "Average loss: 5.3074500077823075 - Average reg: 0\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 39.225220\n",
      "Average loss: 3.670491330405034 - Average reg: 0\n",
      "Train Epoch: 5 [29984/60000 (100%)]\tLoss: 35.095947\n",
      "Average loss: 2.3702857336764143 - Average reg: 0\n",
      "Train Epoch: 6 [29984/60000 (100%)]\tLoss: 46.835098\n",
      "Average loss: 1.344481185555204 - Average reg: 0\n",
      "Train Epoch: 7 [29984/60000 (100%)]\tLoss: 31.409912\n",
      "Average loss: 0.5583316664705907 - Average reg: 0\n",
      "Train Epoch: 8 [29984/60000 (100%)]\tLoss: 36.655212\n",
      "Average loss: -0.08227057548474147 - Average reg: 0\n",
      "Train Epoch: 9 [29984/60000 (100%)]\tLoss: 37.332611\n",
      "Average loss: -0.7664251429185684 - Average reg: 0\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()    \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7869764543545525 0.4399063771785195\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)    \n",
    "    \n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)     \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    reg = 0\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "#         attributions = torch.div(saliency.attribute(s, target=i),s)[:,col_idx]\n",
    "        attributions = saliency.attribute(s, target=i)[:,col_idx]\n",
    "        reg += L1Loss(attributions, torch.zeros_like(attributions))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
