{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "from model_mnist import LeNet5\n",
    "from visualize import *\n",
    "import dset_mnist as dset\n",
    "import foolbox\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# disentangled vae\n",
    "sys.path.append('../disentangling-vae')\n",
    "from collections import defaultdict\n",
    "import vae_trim, vae_trim_viz\n",
    "from disvae.utils.modelIO import save_model, load_model, load_metadata\n",
    "from disvae.models.losses import get_loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-Attr-Pen-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 0 # total correlation reg\n",
    "args.attr_lamb = 0.001 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 178.832336\n",
      "Average loss: 225.70974871344657 - Average reg: 0.0010709765868987253\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 174.249680\n",
      "Average loss: 182.90000163975046 - Average reg: 0.0011176248453358916\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 176.482620\n",
      "Average loss: 178.3071426196393 - Average reg: 0.001090005140946205\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 172.607880\n",
      "Average loss: 175.87265733666004 - Average reg: 0.0010700983435674104\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 170.556198\n",
      "Average loss: 174.0766552109708 - Average reg: 0.0010565968333626353\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.11531616731494 1.0372125168344868\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-Attr-Pen-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 0 # total correlation reg\n",
    "args.attr_lamb = 0.1 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 165.872238\n",
      "Average loss: 227.0549631667798 - Average reg: 0.10116995455248397\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 184.104904\n",
      "Average loss: 182.547677029933 - Average reg: 0.1074968228922851\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 175.710739\n",
      "Average loss: 177.87978973063323 - Average reg: 0.10524751053753692\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 184.838989\n",
      "Average loss: 175.2153774660025 - Average reg: 0.10432412872500003\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 167.554230\n",
      "Average loss: 173.47549109875774 - Average reg: 0.10305916701457393\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.6711936251187 1.0244821412985259\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-Attr-Pen-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 0 # total correlation reg\n",
    "args.attr_lamb = 50 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 209.359573\n",
      "Average loss: 260.4856673446037 - Average reg: 12.300184852532995\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 202.528900\n",
      "Average loss: 210.9487006506686 - Average reg: 12.4859372781538\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 204.641327\n",
      "Average loss: 203.65534685212157 - Average reg: 11.08942149290398\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 199.168137\n",
      "Average loss: 200.07705140266336 - Average reg: 10.401542523268189\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 187.701416\n",
      "Average loss: 198.0178859360945 - Average reg: 9.98408572861889\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.61602703493034 0.19825433282010846\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-Attr-Pen-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 0 # total correlation reg\n",
    "args.attr_lamb = 200 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 238.438553\n",
      "Average loss: 290.3483241392351 - Average reg: 17.01171578120575\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 214.250137\n",
      "Average loss: 238.6353965873149 - Average reg: 11.175237426879818\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 213.338028\n",
      "Average loss: 226.92110200731486 - Average reg: 8.544843212119552\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 191.839844\n",
      "Average loss: 220.50440168889094 - Average reg: 7.37843575762279\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 196.375214\n",
      "Average loss: 216.28087433772302 - Average reg: 6.5540944284467555\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.98592105043977 0.030306410858594278\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
