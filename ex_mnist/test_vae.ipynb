{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "from model_mnist import LeNet5\n",
    "from visualize import *\n",
    "import dset_mnist as dset\n",
    "import foolbox\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# disentangled vae\n",
    "sys.path.append('../disentangling-vae')\n",
    "from collections import defaultdict\n",
    "import vae_trim, vae_trim_viz\n",
    "from disvae.utils.modelIO import save_model, load_model, load_metadata\n",
    "from disvae.models.losses import get_loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE-Attr-Pen-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"VAE\"\n",
    "args.reg_anneal = 0\n",
    "args.attr_lamb = 1 # change of attribute wrt other attributes\n",
    "# name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "name = args.loss + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 158.294495\n",
      "Average loss: 200.27238618895444 - Average reg: 0.9039921596137954\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 167.514496\n",
      "Average loss: 162.65034253866688 - Average reg: 0.9991621267058448\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 160.389450\n",
      "Average loss: 158.34996434518777 - Average reg: 0.9760078649912307\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 152.696457\n",
      "Average loss: 155.9498725028943 - Average reg: 0.9629841467210734\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 142.821411\n",
      "Average loss: 154.39593377347185 - Average reg: 0.9548810851345184\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.3707662879277 0.9546791959418925\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE-Attr-Pen-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"VAE\"\n",
    "args.reg_anneal = 0\n",
    "args.attr_lamb = 100 # change of attribute wrt other attributes\n",
    "# name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "name = args.loss + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 210.608841\n",
      "Average loss: 239.94518593989454 - Average reg: 10.854900648853164\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 196.884308\n",
      "Average loss: 201.20241733705566 - Average reg: 7.1664520896065715\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 199.069855\n",
      "Average loss: 194.75057225339194 - Average reg: 6.270021577887952\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 181.511642\n",
      "Average loss: 190.6632037294953 - Average reg: 5.664612972914283\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 192.748352\n",
      "Average loss: 187.72429473161188 - Average reg: 5.033098313600015\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        # penalize change in one attribute wrt the other attributes\n",
    "        if args.attr_lamb > 0:\n",
    "            s = deepcopy(latent_dist[0].detach())\n",
    "            s = s.requires_grad_(True)\n",
    "            s_output = m(s)\n",
    "            attr_reg = 0\n",
    "            for i in range(model.latent_dim):\n",
    "                col_idx = np.arange(model.latent_dim)!=i\n",
    "                gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                                retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "                gradient_pairwise = gradients[:,col_idx]\n",
    "                attr_reg += args.attr_lamb * L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "        loss += attr_reg\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_reg += attr_reg.item()      \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, mean_epoch_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.76879495649194 0.04623346317059068\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e8dc6a2b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(recon_batch[0,0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(recon_batch[0,0].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
