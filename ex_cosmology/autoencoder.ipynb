{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "import torchvision.utils as vutils\n",
    "import models\n",
    "from visualize import *\n",
    "from data import *\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data_path = './cosmo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "img_size = 256\n",
    "class_num = 1\n",
    "\n",
    "# cosmo dataset\n",
    "transformer = transforms.Compose([ToTensor()])\n",
    "mnu_dataset = MassMapsDataset(opj(data_path, 'cosmological_parameters.txt'),  \n",
    "                              opj(data_path, 'z1_256'),\n",
    "                              transform=transformer)\n",
    "train_set = torch.utils.data.Subset(mnu_dataset, [_ for _ in range(0,40000)])\n",
    "test_set = torch.utils.data.Subset(mnu_dataset, [_ for _ in range(40001,50000)])\n",
    "\n",
    "# dataloader\n",
    "# data_loader = torch.utils.data.DataLoader(mnu_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# load model\n",
    "model = models.load_model(model_name='resnet18', device=device, inplace=False, data_path=data_path).to(device)\n",
    "model = model.eval()\n",
    "# freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1, bias=True, with_bn=True):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "\n",
    "        conv_mod = nn.Conv2d(in_channels,\n",
    "                             out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             padding=padding,\n",
    "                             stride=stride,\n",
    "                             bias=bias,\n",
    "                             dilation=dilation, )\n",
    "\n",
    "        if with_bn:\n",
    "            self.block_unit = nn.Sequential(conv_mod,\n",
    "                                          nn.BatchNorm2d(out_channels),\n",
    "                                          nn.ReLU(inplace=True))\n",
    "        else:\n",
    "            self.block_unit = nn.Sequential(conv_mod, \n",
    "                                          nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_unit(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDown2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvDown2, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_channels, out_channels, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_channels, out_channels, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        unpooled_shape = x.size()\n",
    "        x, indices = self.maxpool_with_argmax(x)\n",
    "        return x, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class ConvDown3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvDown3, self).__init__()\n",
    "        self.conv1 = conv2DBatchNormRelu(in_channels, out_channels, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(out_channels, out_channels, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(out_channels, out_channels, 3, 1, 1)\n",
    "        self.maxpool_with_argmax = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        unpooled_shape = x.size()\n",
    "        x, indices = self.maxpool_with_argmax(x)\n",
    "        return x, indices, unpooled_shape\n",
    "\n",
    "\n",
    "class ConvUp2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvUp2, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_channels, in_channels, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_channels, out_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, indices, output_shape):\n",
    "        x = self.unpool(input=x, indices=indices, output_size=output_shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConvUp3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvUp3, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv1 = conv2DBatchNormRelu(in_channels, in_channels, 3, 1, 1)\n",
    "        self.conv2 = conv2DBatchNormRelu(in_channels, in_channels, 3, 1, 1)\n",
    "        self.conv3 = conv2DBatchNormRelu(in_channels, out_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, indices, output_shape):\n",
    "        x = self.unpool(input=x, indices=indices, output_size=output_shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.down1 = ConvDown2(self.in_channels, 64)\n",
    "        self.down2 = ConvDown2(64, 128)\n",
    "        self.down3 = ConvDown3(128, 256)\n",
    "#         self.down4 = ConvDown3(256, 512)\n",
    "#         self.down5 = ConvDown3(512, 512)\n",
    "\n",
    "#         self.up5 = ConvUp3(512, 512)\n",
    "#         self.up4 = ConvUp3(512, 256)\n",
    "        self.up3 = ConvUp3(256, 128)\n",
    "        self.up2 = ConvUp2(128, 64)\n",
    "        self.up1 = ConvUp2(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x, indices_1, unpool_shape1 = self.down1(x)\n",
    "        x, indices_2, unpool_shape2 = self.down2(x)\n",
    "        x, indices_3, unpool_shape3 = self.down3(x)\n",
    "#         x, indices_4, unpool_shape4 = self.down4(x)\n",
    "#         x, indices_5, unpool_shape5 = self.down5(x)\n",
    "\n",
    "#         x = self.up5(x, indices_5, unpool_shape5)\n",
    "#         x = self.up4(x, indices_4, unpool_shape4)\n",
    "        x = self.up3(x, indices_3, unpool_shape3)\n",
    "        x = self.up2(x, indices_2, unpool_shape2)\n",
    "        x = self.up1(x, indices_1, unpool_shape1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def init_vgg16_params(self, vgg16):\n",
    "        blocks = [self.down1, self.down2, self.down3, self.down4, self.down5]\n",
    "\n",
    "        ranges = [[0, 4], [5, 9], [10, 16], [17, 23], [24, 29]]\n",
    "        features = list(vgg16.features.children())\n",
    "\n",
    "        vgg_layers = []\n",
    "        for _layer in features:\n",
    "            if isinstance(_layer, nn.Conv2d):\n",
    "                vgg_layers.append(_layer)\n",
    "\n",
    "        merged_layers = []\n",
    "        for idx, conv_block in enumerate(blocks):\n",
    "            if idx < 2:\n",
    "                units = [conv_block.conv1.block_unit, conv_block.conv2.block_unit]\n",
    "            else:\n",
    "                units = [\n",
    "                    conv_block.conv1.block_unit,\n",
    "                    conv_block.conv2.block_unit,\n",
    "                    conv_block.conv3.block_unit,\n",
    "                ]\n",
    "            for _unit in units:\n",
    "                for _layer in _unit:\n",
    "                    if isinstance(_layer, nn.Conv2d):\n",
    "                        merged_layers.append(_layer)\n",
    "\n",
    "        assert len(vgg_layers) == len(merged_layers)\n",
    "\n",
    "        for l1, l2 in zip(vgg_layers, merged_layers):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "    \n",
    "def ensure_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)    \n",
    "    \n",
    "def save_checkpoint(epoch, model, optimizer, val_loss, is_best, save_foler='./models/autoencoder'):\n",
    "    ensure_folder(save_folder)\n",
    "    state = {'model': model,\n",
    "             'optimizer': optimizer}\n",
    "    filename = '{0}/checkpoint_{1}_{2:.3f}.tar'.format(save_folder, epoch, val_loss)\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, '{}/BEST_checkpoint.tar'.format(save_folder))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, model, optimizer):\n",
    "    # Ensure dropout layers are in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    # Batches\n",
    "    for i_batch, data in enumerate(train_loader):\n",
    "        inputs, params = data['image'], data['params']\n",
    "        # Set device options\n",
    "        if device == 'cuda':\n",
    "            inputs = inputs.to(device)\n",
    "            params = params.to(device)        \n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Model output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(inputs, outputs)\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer.step(closure)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        losses.append(loss.item())\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Print status\n",
    "        if i_batch % 50 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(inputs), len(train_loader.dataset),\n",
    "                       100. * i_batch / len(train_loader), loss.data.item()), end='')\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return losses\n",
    "\n",
    "\n",
    "def valid(val_loader, model):\n",
    "    model.eval()  # eval mode (no dropout or batchnorm)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = [] \n",
    "    \n",
    "    running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i_batch, data in enumerate(val_loader):\n",
    "            inputs, params = data['image'], data['params']\n",
    "            # Set device options\n",
    "            if device == 'cuda':\n",
    "                inputs = inputs.to(device)\n",
    "                params = params.to(device)               \n",
    "\n",
    "            # Model output\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(inputs, outputs)\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            losses.append(loss.item())   \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Print status\n",
    "            if i_batch % 50 == 0:\n",
    "                print('\\rVal Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, i_batch * len(inputs), len(val_loader.dataset),\n",
    "                           100. * i_batch / len(val_loader), loss.data.item()), end='')\n",
    "                \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# # model\n",
    "# t = Net().to(device)\n",
    "\n",
    "# # Setup Adam optimizers\n",
    "# optimizer = optim.Adam(t.parameters(), lr=0.001)\n",
    "\n",
    "# best_loss = 1e10\n",
    "\n",
    "# epochs_since_improvement = 0\n",
    "\n",
    "# num_epochs = 5\n",
    "\n",
    "# data = iter(train_loader).next()\n",
    "# inputs, params = data['image'], data['params']\n",
    "# # Set device options\n",
    "# if device == 'cuda':\n",
    "#     inputs = inputs.to(device)\n",
    "#     params = params.to(device)  \n",
    "# t(inputs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 0.000292"
     ]
    }
   ],
   "source": [
    "# model\n",
    "t = Net().to(device)\n",
    "\n",
    "# Setup Adam optimizers\n",
    "optimizer = optim.Adam(t.parameters(), lr=0.001)\n",
    "\n",
    "best_loss = 1e10\n",
    "\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(1, num_epochs):\n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if epochs_since_improvement == 20:\n",
    "        break\n",
    "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "        adjust_learning_rate(optimizer, 0.8)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(epoch, train_loader, t, optimizer)\n",
    "\n",
    "    # One epoch's validation\n",
    "    val_loss = valid(val_loader, t)\n",
    "    print('\\n * LOSS - {loss:.3f}\\n'.format(loss=val_loss))\n",
    "\n",
    "    # Check if there was an improvement\n",
    "    is_best = val_loss < best_loss\n",
    "    best_loss = min(best_loss, val_loss)\n",
    "\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(epoch, model, optimizer, val_loss, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
