{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "import torchvision.utils as vutils\n",
    "import models\n",
    "from visualize import *\n",
    "from data import *\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data_path = './cosmo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "img_size = 256\n",
    "class_num = 1\n",
    "\n",
    "# cosmo dataset\n",
    "transformer = transforms.Compose([ToTensor()])\n",
    "mnu_dataset = MassMapsDataset(opj(data_path, 'cosmological_parameters.txt'),  \n",
    "                              opj(data_path, 'z1_256'),\n",
    "                              transform=transformer)\n",
    "\n",
    "# dataloader\n",
    "data_loader = torch.utils.data.DataLoader(mnu_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# load model\n",
    "model = models.load_model(model_name='resnet18', device=device, inplace=False, data_path=data_path).to(device)\n",
    "model = model.eval()\n",
    "# freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transform, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)  \n",
    "#         self.conv2 = nn.Conv2d(32, 10, kernel_size=4, stride=1, padding=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x, indices = self.pool(x)\n",
    "        x = self.unpool(x, indices)\n",
    "        # add second hidden layer\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool(x)  # compressed representation\n",
    "                \n",
    "        return x\n",
    "    \n",
    "class Transform_i(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transform_i, self).__init__()        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "#         self.t_conv1 = nn.ConvTranspose2d(10, 32, kernel_size=2, stride=2, padding=0, output_padding=0, bias=False)\n",
    "        self.t_conv1 = nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2, padding=0, output_padding=0, bias=False)\n",
    "#         self.u_conv1 = nn.Conv2d(10, 32, kernel_size=7, padding=0)\n",
    "#         self.u_conv1 = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## decode ##\n",
    "        # upsample, followed by a conv layer, with relu activation function  \n",
    "        # this function is called `upsample` in some PyTorch versions\n",
    "#         x = F.interpolate(x, scale_factor=4, mode='nearest')\n",
    "        x = self.t_conv1(x)\n",
    "        # upsample again, output should have a sigmoid applied\n",
    "#         x = F.interpolate(x, scale_factor=3, mode='nearest')\n",
    "#         x = self.u_conv2(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "class Mask(nn.Module):\n",
    "    def __init__(self, img_size=128):\n",
    "        super(Mask, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.initialize()\n",
    "#         self.mask = nn.Parameter(torch.clamp(abs(torch.randn(img_size, img_size)), 0, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.mul(self.mask, x)   \n",
    "    \n",
    "    def initialize(self):\n",
    "        self.mask = nn.Parameter(torch.ones(64, 32, self.img_size, self.img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transforms\n",
    "t = Transform().to(device)\n",
    "transform_i = Transform_i().to(device)\n",
    "# initialize\n",
    "# t.conv1.weight.data = model.conv1.weight.data\n",
    "# transform_i.convt1.weight.data = model.conv1.weight.data\n",
    "\n",
    "# mask\n",
    "mask = Mask().to(device)\n",
    "\n",
    "# transform_i.load_state_dict(torch.load('./models/conv_filters_pen'))\n",
    "# prepend transformation\n",
    "model_t = TrimModel(model, transform_i, use_residuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# l1-loss\n",
    "l1loss = nn.L1Loss()\n",
    "\n",
    "# Setup Adam optimizers\n",
    "optimizer_t = optim.Adam(t.parameters(), lr=0.0005)\n",
    "optimizer_i = optim.Adam(transform_i.parameters(), lr=0.0005)\n",
    "optimizer = optim.Adam(mask.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [0/100000 (0%)]\tLoss: -5.604468\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [640/100000 (1%)]\tLoss: -7.400812\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [1280/100000 (1%)]\tLoss: -8.881898\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [1920/100000 (2%)]\tLoss: -11.784495\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [2560/100000 (3%)]\tLoss: -13.392062\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [3200/100000 (3%)]\tLoss: -17.279856\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [3840/100000 (4%)]\tLoss: -19.928925\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [4480/100000 (4%)]\tLoss: -24.010805\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [5120/100000 (5%)]\tLoss: -25.532461\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [5760/100000 (6%)]\tLoss: -29.783724\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [6400/100000 (6%)]\tLoss: -32.005527\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [7040/100000 (7%)]\tLoss: -36.946018\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [7680/100000 (8%)]\tLoss: -43.748417\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [8320/100000 (8%)]\tLoss: -48.121059\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [8960/100000 (9%)]\tLoss: -48.791954\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [9600/100000 (10%)]\tLoss: -51.143723\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [10240/100000 (10%)]\tLoss: -54.839333\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [10880/100000 (11%)]\tLoss: -61.295849\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [11520/100000 (12%)]\tLoss: -70.870789\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [12160/100000 (12%)]\tLoss: -74.820229\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [12800/100000 (13%)]\tLoss: -77.761063\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [13440/100000 (13%)]\tLoss: -79.850388\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [14080/100000 (14%)]\tLoss: -94.740013\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [14720/100000 (15%)]\tLoss: -96.593552\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [16000/100000 (16%)]\tLoss: -101.367363\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [16640/100000 (17%)]\tLoss: -108.452209\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [17280/100000 (17%)]\tLoss: -112.333847\n",
      "Train Epoch: 24/100 (0/2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [19840/100000 (20%)]\tLoss: -142.170151\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [20480/100000 (20%)]\tLoss: -144.664444\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [21120/100000 (21%)]\tLoss: -151.448975\n",
      "Train Epoch: 55/100 (0/2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [24320/100000 (24%)]\tLoss: -209.499222\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [24960/100000 (25%)]\tLoss: -198.975250\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [25600/100000 (26%)]\tLoss: -212.850006\n",
      "Train Epoch: 20/100 (0/2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [28800/100000 (29%)]\tLoss: -254.890671\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [29440/100000 (29%)]\tLoss: -269.308075\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [30080/100000 (30%)]\tLoss: -272.146149\n",
      "Train Epoch: 84/100 (0/2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [33280/100000 (33%)]\tLoss: -337.911438\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [33920/100000 (34%)]\tLoss: -348.982971\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [34560/100000 (35%)]\tLoss: -363.630310\n",
      "Train Epoch: 63/100 (0/2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [37120/100000 (37%)]\tLoss: -423.718262\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [40960/100000 (41%)]\tLoss: -474.741425\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [41600/100000 (42%)]\tLoss: -518.848755\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [42240/100000 (42%)]\tLoss: -486.027893\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [42880/100000 (43%)]\tLoss: -491.147095\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [43520/100000 (44%)]\tLoss: -540.843384\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [44160/100000 (44%)]\tLoss: -496.207672\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [44800/100000 (45%)]\tLoss: -568.972778\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [45440/100000 (45%)]\tLoss: -574.791565\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [46080/100000 (46%)]\tLoss: -592.252014\n",
      "Train Epoch: 99/100 (0/2)\n",
      "Train Epoch: 0 [46720/100000 (47%)]\tLoss: -604.397827\n",
      "Train Epoch: 6/100 (0/2)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-644732cfbdb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# mask training stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rTrain Epoch: {}/100 ({}/{})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mlosses_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "# Lists to keep track of progress\n",
    "losses = []\n",
    "num_epochs = 2\n",
    "\n",
    "lamb_l1 = 100.0\n",
    "lamb = 0.1\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, params = data['image'], data['params']\n",
    "        if device == 'cuda':\n",
    "            inputs = inputs.to(device)\n",
    "            params = params.to(device)\n",
    "\n",
    "        losses_inner = []\n",
    "        for ii in range(100):\n",
    "            # update masks    \n",
    "            im_mask = mask(t(inputs))\n",
    "            output_ = model(transform_i(im_mask))\n",
    "            # loss\n",
    "            loss = -output_[:,1].sum() + lamb_l1 * l1loss(mask.mask, torch.zeros_like(mask.mask))\n",
    "            # zero grad\n",
    "            optimizer.zero_grad()\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # Update G\n",
    "            optimizer.step()\n",
    "            # projection\n",
    "            mask.mask.data = torch.clamp(mask.mask.data, 0, 1)\n",
    "\n",
    "            # mask training stats\n",
    "            print('\\rTrain Epoch: {}/100 ({}/{})'.format(ii, epoch, num_epochs), end='') \n",
    "            losses_inner.append(loss.item())\n",
    "            \n",
    "        outputs = transform_i(t(inputs))\n",
    "        # loss\n",
    "        loss = criterion(inputs, outputs)\n",
    "        # interp\n",
    "        im_mask = mask(t(inputs))\n",
    "        output_ = model(transform_i(im_mask))\n",
    "        loss += -lamb * output_[:,1].sum()\n",
    "        # zero grad\n",
    "        t.zero_grad()\n",
    "        transform_i.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # Update G\n",
    "        optimizer_t.step()\n",
    "        optimizer_i.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 10 == 0:\n",
    "            print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(inputs), len(data_loader.dataset),\n",
    "                       100. * i / len(data_loader), loss.data.item()))\n",
    "            torch.save(t.state_dict(), './models/transform_')\n",
    "            torch.save(transform_i.state_dict(), './models/transform_i_')\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        losses.append(loss.item())\n",
    "        # Setup Adam optimizer\n",
    "        mask.initialize()\n",
    "        mask = mask.to(device)\n",
    "        optimizer = optim.Adam(mask.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss versus training iterations\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator Loss During Training\")\n",
    "plt.plot(losses, label=\"G\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz filters\n",
    "viz_filters(t.conv1.weight, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz filters\n",
    "viz_filters(transform_i.t_conv1.weight, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = mnu_dataset[25000]['image'].to(device).unsqueeze(0)\n",
    "viz_im_r(im, transform_i(t(im)))\n",
    "print(torch.norm(im - transform_i(t(im))).item()**2/28**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
