<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>awd.adaptive_wavelets.lowlevel API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>awd.adaptive_wavelets.lowlevel</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pywt
import torch
import torch.nn.functional as F
from torch.autograd import Function

from awd.adaptive_wavelets.utils import reflect


def roll(x, n, dim, make_even=False):
    if n &lt; 0:
        n = x.shape[dim] + n

    if make_even and x.shape[dim] % 2 == 1:
        end = 1
    else:
        end = 0

    if dim == 0:
        return torch.cat((x[-n:], x[:-n + end]), dim=0)
    elif dim == 1:
        return torch.cat((x[:, -n:], x[:, :-n + end]), dim=1)
    elif dim == 2 or dim == -2:
        return torch.cat((x[:, :, -n:], x[:, :, :-n + end]), dim=2)
    elif dim == 3 or dim == -1:
        return torch.cat((x[:, :, :, -n:], x[:, :, :, :-n + end]), dim=3)


def mypad(x, pad, mode=&#39;constant&#39;, value=0):
    &#34;&#34;&#34; Function to do numpy like padding on tensors. Only works for 2-D
    padding.

    Inputs:
        x (tensor): tensor to pad
        pad (tuple): tuple of (left, right, top, bottom) pad sizes
        mode (str): &#39;symmetric&#39;, &#39;wrap&#39;, &#39;constant, &#39;reflect&#39;, &#39;replicate&#39;, or
            &#39;zero&#39;. The padding technique.
    &#34;&#34;&#34;
    if mode == &#39;symmetric&#39;:
        # Vertical only
        if pad[0] == 0 and pad[1] == 0:
            m1, m2 = pad[2], pad[3]
            l = x.shape[-2]
            xe = reflect(np.arange(-m1, l + m2, dtype=&#39;int32&#39;), -0.5, l - 0.5)
            return x[:, :, xe]
        # horizontal only
        elif pad[2] == 0 and pad[3] == 0:
            m1, m2 = pad[0], pad[1]
            l = x.shape[-1]
            xe = reflect(np.arange(-m1, l + m2, dtype=&#39;int32&#39;), -0.5, l - 0.5)
            return x[:, :, :, xe]
        # Both
        else:
            m1, m2 = pad[0], pad[1]
            l1 = x.shape[-1]
            xe_row = reflect(np.arange(-m1, l1 + m2, dtype=&#39;int32&#39;), -0.5, l1 - 0.5)
            m1, m2 = pad[2], pad[3]
            l2 = x.shape[-2]
            xe_col = reflect(np.arange(-m1, l2 + m2, dtype=&#39;int32&#39;), -0.5, l2 - 0.5)
            i = np.outer(xe_col, np.ones(xe_row.shape[0]))
            j = np.outer(np.ones(xe_col.shape[0]), xe_row)
            return x[:, :, i, j]
    elif mode == &#39;periodic&#39;:
        # Vertical only
        if pad[0] == 0 and pad[1] == 0:
            xe = np.arange(x.shape[-2])
            xe = np.pad(xe, (pad[2], pad[3]), mode=&#39;wrap&#39;)
            return x[:, :, xe]
        # Horizontal only
        elif pad[2] == 0 and pad[3] == 0:
            xe = np.arange(x.shape[-1])
            xe = np.pad(xe, (pad[0], pad[1]), mode=&#39;wrap&#39;)
            return x[:, :, :, xe]
        # Both
        else:
            xe_col = np.arange(x.shape[-2])
            xe_col = np.pad(xe_col, (pad[2], pad[3]), mode=&#39;wrap&#39;)
            xe_row = np.arange(x.shape[-1])
            xe_row = np.pad(xe_row, (pad[0], pad[1]), mode=&#39;wrap&#39;)
            i = np.outer(xe_col, np.ones(xe_row.shape[0]))
            j = np.outer(np.ones(xe_col.shape[0]), xe_row)
            return x[:, :, i, j]

    elif mode == &#39;constant&#39; or mode == &#39;reflect&#39; or mode == &#39;replicate&#39;:
        return F.pad(x, pad, mode, value)
    elif mode == &#39;zero&#39;:
        return F.pad(x, pad)
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))


def afb1d(x, h0, h1, mode=&#39;zero&#39;, dim=-1):
    &#34;&#34;&#34; 1D analysis filter bank (along one dimension only) of an image

    Inputs:
        x (tensor): 4D input with the last two dimensions the spatial input
        h0 (tensor): 4D input for the lowpass filter. Should have shape (1, 1,
            h, 1) or (1, 1, 1, w)
        h1 (tensor): 4D input for the highpass filter. Should have shape (1, 1,
            h, 1) or (1, 1, 1, w)
        mode (str): padding method
        dim (int) - dimension of filtering. d=2 is for a vertical filter (called
            column filtering but filters across the rows). d=3 is for a
            horizontal filter, (called row filtering but filters across the
            columns).

    Returns:
        lohi: lowpass and highpass subbands concatenated along the channel
            dimension
    &#34;&#34;&#34;
    C = x.shape[1]
    # Convert the dim to positive
    d = dim % 4
    s = (2, 1) if d == 2 else (1, 2)
    N = x.shape[d]
    # If h0, h1 are not tensors, make them. If they are, then assume that they
    # are in the right order
    if not isinstance(h0, torch.Tensor):
        h0 = torch.tensor(np.copy(np.array(h0).ravel()[::-1]),
                          dtype=torch.float, device=x.device)
    if not isinstance(h1, torch.Tensor):
        h1 = torch.tensor(np.copy(np.array(h1).ravel()[::-1]),
                          dtype=torch.float, device=x.device)
    L = h0.numel()
    L2 = L // 2
    shape = [1, 1, 1, 1]
    shape[d] = L
    # If h aren&#39;t in the right shape, make them so
    if h0.shape != tuple(shape):
        h0 = h0.reshape(*shape)
    if h1.shape != tuple(shape):
        h1 = h1.reshape(*shape)
    h = torch.cat([h0, h1] * C, dim=0)

    if mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        if x.shape[dim] % 2 == 1:
            if d == 2:
                x = torch.cat((x, x[:, :, -1:]), dim=2)
            else:
                x = torch.cat((x, x[:, :, :, -1:]), dim=3)
            N += 1
        x = roll(x, -L2, dim=d)
        pad = (L - 1, 0) if d == 2 else (0, L - 1)
        lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)
        N2 = N // 2
        if d == 2:
            lohi[:, :, :L2] = lohi[:, :, :L2] + lohi[:, :, N2:N2 + L2]
            lohi = lohi[:, :, :N2]
        else:
            lohi[:, :, :, :L2] = lohi[:, :, :, :L2] + lohi[:, :, :, N2:N2 + L2]
            lohi = lohi[:, :, :, :N2]
    else:
        # Calculate the pad size
        outsize = pywt.dwt_coeff_len(N, L, mode=mode)
        p = 2 * (outsize - 1) - N + L
        if mode == &#39;zero&#39;:
            # Sadly, pytorch only allows for same padding before and after, if
            # we need to do more padding after for odd length signals, have to
            # prepad
            if p % 2 == 1:
                pad = (0, 0, 0, 1) if d == 2 else (0, 1, 0, 0)
                x = F.pad(x, pad)
            pad = (p // 2, 0) if d == 2 else (0, p // 2)
            # Calculate the high and lowpass
            lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)
        elif mode == &#39;symmetric&#39; or mode == &#39;reflect&#39; or mode == &#39;periodic&#39;:
            pad = (0, 0, p // 2, (p + 1) // 2) if d == 2 else (p // 2, (p + 1) // 2, 0, 0)
            x = mypad(x, pad=pad, mode=mode)
            lohi = F.conv2d(x, h, stride=s, groups=C)
        else:
            raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))

    return lohi


def sfb1d(lo, hi, g0, g1, mode=&#39;zero&#39;, dim=-1):
    &#34;&#34;&#34; 1D synthesis filter bank of an image tensor
    &#34;&#34;&#34;
    C = lo.shape[1]
    d = dim % 4
    # If g0, g1 are not tensors, make them. If they are, then assume that they
    # are in the right order
    if not isinstance(g0, torch.Tensor):
        g0 = torch.tensor(np.copy(np.array(g0).ravel()),
                          dtype=torch.float, device=lo.device)
    if not isinstance(g1, torch.Tensor):
        g1 = torch.tensor(np.copy(np.array(g1).ravel()),
                          dtype=torch.float, device=lo.device)
    L = g0.numel()
    shape = [1, 1, 1, 1]
    shape[d] = L
    N = 2 * lo.shape[d]
    # If g aren&#39;t in the right shape, make them so
    if g0.shape != tuple(shape):
        g0 = g0.reshape(*shape)
    if g1.shape != tuple(shape):
        g1 = g1.reshape(*shape)

    s = (2, 1) if d == 2 else (1, 2)
    g0 = torch.cat([g0] * C, dim=0)
    g1 = torch.cat([g1] * C, dim=0)
    if mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        y = F.conv_transpose2d(lo, g0, stride=s, groups=C) + \
            F.conv_transpose2d(hi, g1, stride=s, groups=C)
        if d == 2:
            y[:, :, :L - 2] = y[:, :, :L - 2] + y[:, :, N:N + L - 2]
            y = y[:, :, :N]
        else:
            y[:, :, :, :L - 2] = y[:, :, :, :L - 2] + y[:, :, :, N:N + L - 2]
            y = y[:, :, :, :N]
        y = roll(y, 1 - L // 2, dim=dim)
    else:
        if mode == &#39;zero&#39; or mode == &#39;symmetric&#39; or mode == &#39;reflect&#39; or \
                mode == &#39;periodic&#39;:
            pad = (L - 2, 0) if d == 2 else (0, L - 2)
            y = F.conv_transpose2d(lo, g0, stride=s, padding=pad, groups=C) + \
                F.conv_transpose2d(hi, g1, stride=s, padding=pad, groups=C)
        else:
            raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))

    return y


def mode_to_int(mode):
    if mode == &#39;zero&#39;:
        return 0
    elif mode == &#39;symmetric&#39;:
        return 1
    elif mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        return 2
    elif mode == &#39;constant&#39;:
        return 3
    elif mode == &#39;reflect&#39;:
        return 4
    elif mode == &#39;replicate&#39;:
        return 5
    elif mode == &#39;periodic&#39;:
        return 6
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))


def int_to_mode(mode):
    if mode == 0:
        return &#39;zero&#39;
    elif mode == 1:
        return &#39;symmetric&#39;
    elif mode == 2:
        return &#39;periodization&#39;
    elif mode == 3:
        return &#39;constant&#39;
    elif mode == 4:
        return &#39;reflect&#39;
    elif mode == 5:
        return &#39;replicate&#39;
    elif mode == 6:
        return &#39;periodic&#39;
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))


class AFB2D(Function):
    &#34;&#34;&#34; Does a single level 2d wavelet decomposition of an input. Does separate
    row and column filtering by two calls to
    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0_row: row lowpass
        h1_row: row highpass
        h0_col: col lowpass
        h1_col: col highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*4, H, W)
    &#34;&#34;&#34;

    @staticmethod
    def forward(x, h0_row, h1_row, h0_col, h1_col, mode):
        mode = int_to_mode(mode)
        lohi = afb1d(x, h0_row, h1_row, mode=mode, dim=3)
        y = afb1d(lohi, h0_col, h1_col, mode=mode, dim=2)
        s = y.shape
        y = y.reshape(s[0], -1, 4, s[-2], s[-1])
        low = y[:, :, 0].contiguous()
        highs = y[:, :, 1:].contiguous()
        return low, highs


class AFB1D(Function):
    &#34;&#34;&#34; Does a single level 1d wavelet decomposition of an input.

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0: lowpass
        h1: highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        x0: Tensor of shape (N, C, L&#39;) - lowpass
        x1: Tensor of shape (N, C, L&#39;) - highpass
    &#34;&#34;&#34;

    @staticmethod
    def forward(x, h0, h1, mode):
        mode = int_to_mode(mode)

        # Make inputs 4d
        x = x[:, :, None, :]
        h0 = h0[:, :, None, :]
        h1 = h1[:, :, None, :]

        lohi = afb1d(x, h0, h1, mode=mode, dim=3)
        x0 = lohi[:, ::2, 0].contiguous()
        x1 = lohi[:, 1::2, 0].contiguous()
        return x0, x1


class SFB2D(Function):
    &#34;&#34;&#34; Does a single level 2d wavelet decomposition of an input. Does separate
    row and column filtering by two calls to
    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0_row: row lowpass
        h1_row: row highpass
        h0_col: col lowpass
        h1_col: col highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*4, H, W)
    &#34;&#34;&#34;

    @staticmethod
    def forward(low, highs, g0_row, g1_row, g0_col, g1_col, mode):
        mode = int_to_mode(mode)

        lh, hl, hh = torch.unbind(highs, dim=2)
        lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)
        hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)
        y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)
        return y


class SFB1D(Function):
    &#34;&#34;&#34; Does a single level 1d wavelet decomposition of an input.

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        low (torch.Tensor): Lowpass to reconstruct of shape (N, C, L)
        high (torch.Tensor): Highpass to reconstruct of shape (N, C, L)
        g0: lowpass
        g1: highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*2, L&#39;)
    &#34;&#34;&#34;

    @staticmethod
    def forward(low, high, g0, g1, mode):
        mode = int_to_mode(mode)
        # Make into a 2d tensor with 1 row
        low = low[:, :, None, :]
        high = high[:, :, None, :]
        g0 = g0[:, :, None, :]
        g1 = g1[:, :, None, :]

        return sfb1d(low, high, g0, g1, mode=mode, dim=3)[:, :, 0]


def prep_filt_sfb2d(g0_col, g1_col, g0_row=None, g1_row=None, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the sfb2d function.  In
    particular, makes the tensors the right shape. It does not mirror image them
    as as sfb2d uses conv2d_transpose which acts like normal convolution.

    Inputs:
        g0_col (array-like): low pass column filter bank
        g1_col (array-like): high pass column filter bank
        g0_row (array-like): low pass row filter bank. If none, will assume the
            same as column filter
        g1_row (array-like): high pass row filter bank. If none, will assume the
            same as column filter
        device: which device to put the tensors on to

    Returns:
        (g0_col, g1_col, g0_row, g1_row)
    &#34;&#34;&#34;
    g0_col, g1_col = prep_filt_sfb1d(g0_col, g1_col, device)
    if g0_row is None:
        g0_row, g1_row = g0_col, g1_col
    else:
        g0_row, g1_row = prep_filt_sfb1d(g0_row, g1_row, device)

    g0_col = g0_col.reshape((1, 1, -1, 1))
    g1_col = g1_col.reshape((1, 1, -1, 1))
    g0_row = g0_row.reshape((1, 1, 1, -1))
    g1_row = g1_row.reshape((1, 1, 1, -1))

    return g0_col, g1_col, g0_row, g1_row


def prep_filt_sfb1d(g0, g1, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the sfb1d function. In
    particular, makes the tensors the right shape. It does not mirror image them
    as as sfb2d uses conv2d_transpose which acts like normal convolution.

    Inputs:
        g0 (array-like): low pass filter bank
        g1 (array-like): high pass filter bank
        device: which device to put the tensors on to

    Returns:
        (g0, g1)
    &#34;&#34;&#34;
    g0 = np.array(g0).ravel()
    g1 = np.array(g1).ravel()
    t = torch.get_default_dtype()
    g0 = torch.tensor(g0, device=device, dtype=t).reshape((1, 1, -1))
    g1 = torch.tensor(g1, device=device, dtype=t).reshape((1, 1, -1))

    return g0, g1


def prep_filt_afb2d(h0_col, h1_col, h0_row=None, h1_row=None, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the afb2d function.  In
    particular, makes the tensors the right shape. It takes mirror images of
    them as as afb2d uses conv2d which acts like normal correlation.

    Inputs:
        h0_col (array-like): low pass column filter bank
        h1_col (array-like): high pass column filter bank
        h0_row (array-like): low pass row filter bank. If none, will assume the
            same as column filter
        h1_row (array-like): high pass row filter bank. If none, will assume the
            same as column filter
        device: which device to put the tensors on to

    Returns:
        (h0_col, h1_col, h0_row, h1_row)
    &#34;&#34;&#34;
    h0_col, h1_col = prep_filt_afb1d(h0_col, h1_col, device)
    if h0_row is None:
        h0_row, h1_row = h0_col, h1_col
    else:
        h0_row, h1_row = prep_filt_afb1d(h0_row, h1_row, device)

    h0_col = h0_col.reshape((1, 1, -1, 1))
    h1_col = h1_col.reshape((1, 1, -1, 1))
    h0_row = h0_row.reshape((1, 1, 1, -1))
    h1_row = h1_row.reshape((1, 1, 1, -1))
    return h0_col, h1_col, h0_row, h1_row


def prep_filt_afb1d(h0, h1, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the afb2d function.  In
    particular, makes the tensors the right shape. It takes mirror images of
    them as as afb2d uses conv2d which acts like normal correlation.

    Inputs:
        h0 (array-like): low pass column filter bank
        h1 (array-like): high pass column filter bank
        device: which device to put the tensors on to

    Returns:
        (h0, h1)
    &#34;&#34;&#34;
    h0 = np.array(h0[::-1]).ravel()
    h1 = np.array(h1[::-1]).ravel()
    t = torch.get_default_dtype()
    h0 = torch.tensor(h0, device=device, dtype=t).reshape((1, 1, -1))
    h1 = torch.tensor(h1, device=device, dtype=t).reshape((1, 1, -1))
    return h0, h1</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.afb1d"><code class="name flex">
<span>def <span class="ident">afb1d</span></span>(<span>x, h0, h1, mode='zero', dim=-1)</span>
</code></dt>
<dd>
<section class="desc"><p>1D analysis filter bank (along one dimension only) of an image</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tensor</code></dt>
<dd>4D input with the last two dimensions the spatial input</dd>
<dt><strong><code>h0</code></strong> :&ensp;<code>tensor</code></dt>
<dd>4D input for the lowpass filter. Should have shape (1, 1,
h, 1) or (1, 1, 1, w)</dd>
<dt><strong><code>h1</code></strong> :&ensp;<code>tensor</code></dt>
<dd>4D input for the highpass filter. Should have shape (1, 1,
h, 1) or (1, 1, 1, w)</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>padding method</dd>
</dl>
<p>dim (int) - dimension of filtering. d=2 is for a vertical filter (called
column filtering but filters across the rows). d=3 is for a
horizontal filter, (called row filtering but filters across the
columns).</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>lohi</code></strong></dt>
<dd>lowpass and highpass subbands concatenated along the channel
dimension</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def afb1d(x, h0, h1, mode=&#39;zero&#39;, dim=-1):
    &#34;&#34;&#34; 1D analysis filter bank (along one dimension only) of an image

    Inputs:
        x (tensor): 4D input with the last two dimensions the spatial input
        h0 (tensor): 4D input for the lowpass filter. Should have shape (1, 1,
            h, 1) or (1, 1, 1, w)
        h1 (tensor): 4D input for the highpass filter. Should have shape (1, 1,
            h, 1) or (1, 1, 1, w)
        mode (str): padding method
        dim (int) - dimension of filtering. d=2 is for a vertical filter (called
            column filtering but filters across the rows). d=3 is for a
            horizontal filter, (called row filtering but filters across the
            columns).

    Returns:
        lohi: lowpass and highpass subbands concatenated along the channel
            dimension
    &#34;&#34;&#34;
    C = x.shape[1]
    # Convert the dim to positive
    d = dim % 4
    s = (2, 1) if d == 2 else (1, 2)
    N = x.shape[d]
    # If h0, h1 are not tensors, make them. If they are, then assume that they
    # are in the right order
    if not isinstance(h0, torch.Tensor):
        h0 = torch.tensor(np.copy(np.array(h0).ravel()[::-1]),
                          dtype=torch.float, device=x.device)
    if not isinstance(h1, torch.Tensor):
        h1 = torch.tensor(np.copy(np.array(h1).ravel()[::-1]),
                          dtype=torch.float, device=x.device)
    L = h0.numel()
    L2 = L // 2
    shape = [1, 1, 1, 1]
    shape[d] = L
    # If h aren&#39;t in the right shape, make them so
    if h0.shape != tuple(shape):
        h0 = h0.reshape(*shape)
    if h1.shape != tuple(shape):
        h1 = h1.reshape(*shape)
    h = torch.cat([h0, h1] * C, dim=0)

    if mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        if x.shape[dim] % 2 == 1:
            if d == 2:
                x = torch.cat((x, x[:, :, -1:]), dim=2)
            else:
                x = torch.cat((x, x[:, :, :, -1:]), dim=3)
            N += 1
        x = roll(x, -L2, dim=d)
        pad = (L - 1, 0) if d == 2 else (0, L - 1)
        lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)
        N2 = N // 2
        if d == 2:
            lohi[:, :, :L2] = lohi[:, :, :L2] + lohi[:, :, N2:N2 + L2]
            lohi = lohi[:, :, :N2]
        else:
            lohi[:, :, :, :L2] = lohi[:, :, :, :L2] + lohi[:, :, :, N2:N2 + L2]
            lohi = lohi[:, :, :, :N2]
    else:
        # Calculate the pad size
        outsize = pywt.dwt_coeff_len(N, L, mode=mode)
        p = 2 * (outsize - 1) - N + L
        if mode == &#39;zero&#39;:
            # Sadly, pytorch only allows for same padding before and after, if
            # we need to do more padding after for odd length signals, have to
            # prepad
            if p % 2 == 1:
                pad = (0, 0, 0, 1) if d == 2 else (0, 1, 0, 0)
                x = F.pad(x, pad)
            pad = (p // 2, 0) if d == 2 else (0, p // 2)
            # Calculate the high and lowpass
            lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)
        elif mode == &#39;symmetric&#39; or mode == &#39;reflect&#39; or mode == &#39;periodic&#39;:
            pad = (0, 0, p // 2, (p + 1) // 2) if d == 2 else (p // 2, (p + 1) // 2, 0, 0)
            x = mypad(x, pad=pad, mode=mode)
            lohi = F.conv2d(x, h, stride=s, groups=C)
        else:
            raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))

    return lohi</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.int_to_mode"><code class="name flex">
<span>def <span class="ident">int_to_mode</span></span>(<span>mode)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def int_to_mode(mode):
    if mode == 0:
        return &#39;zero&#39;
    elif mode == 1:
        return &#39;symmetric&#39;
    elif mode == 2:
        return &#39;periodization&#39;
    elif mode == 3:
        return &#39;constant&#39;
    elif mode == 4:
        return &#39;reflect&#39;
    elif mode == 5:
        return &#39;replicate&#39;
    elif mode == 6:
        return &#39;periodic&#39;
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.mode_to_int"><code class="name flex">
<span>def <span class="ident">mode_to_int</span></span>(<span>mode)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode_to_int(mode):
    if mode == &#39;zero&#39;:
        return 0
    elif mode == &#39;symmetric&#39;:
        return 1
    elif mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        return 2
    elif mode == &#39;constant&#39;:
        return 3
    elif mode == &#39;reflect&#39;:
        return 4
    elif mode == &#39;replicate&#39;:
        return 5
    elif mode == &#39;periodic&#39;:
        return 6
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.mypad"><code class="name flex">
<span>def <span class="ident">mypad</span></span>(<span>x, pad, mode='constant', value=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Function to do numpy like padding on tensors. Only works for 2-D
padding.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>tensor</code></dt>
<dd>tensor to pad</dd>
<dt><strong><code>pad</code></strong> :&ensp;<code>tuple</code></dt>
<dd>tuple of (left, right, top, bottom) pad sizes</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>'symmetric', 'wrap', 'constant, 'reflect', 'replicate', or
'zero'. The padding technique.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mypad(x, pad, mode=&#39;constant&#39;, value=0):
    &#34;&#34;&#34; Function to do numpy like padding on tensors. Only works for 2-D
    padding.

    Inputs:
        x (tensor): tensor to pad
        pad (tuple): tuple of (left, right, top, bottom) pad sizes
        mode (str): &#39;symmetric&#39;, &#39;wrap&#39;, &#39;constant, &#39;reflect&#39;, &#39;replicate&#39;, or
            &#39;zero&#39;. The padding technique.
    &#34;&#34;&#34;
    if mode == &#39;symmetric&#39;:
        # Vertical only
        if pad[0] == 0 and pad[1] == 0:
            m1, m2 = pad[2], pad[3]
            l = x.shape[-2]
            xe = reflect(np.arange(-m1, l + m2, dtype=&#39;int32&#39;), -0.5, l - 0.5)
            return x[:, :, xe]
        # horizontal only
        elif pad[2] == 0 and pad[3] == 0:
            m1, m2 = pad[0], pad[1]
            l = x.shape[-1]
            xe = reflect(np.arange(-m1, l + m2, dtype=&#39;int32&#39;), -0.5, l - 0.5)
            return x[:, :, :, xe]
        # Both
        else:
            m1, m2 = pad[0], pad[1]
            l1 = x.shape[-1]
            xe_row = reflect(np.arange(-m1, l1 + m2, dtype=&#39;int32&#39;), -0.5, l1 - 0.5)
            m1, m2 = pad[2], pad[3]
            l2 = x.shape[-2]
            xe_col = reflect(np.arange(-m1, l2 + m2, dtype=&#39;int32&#39;), -0.5, l2 - 0.5)
            i = np.outer(xe_col, np.ones(xe_row.shape[0]))
            j = np.outer(np.ones(xe_col.shape[0]), xe_row)
            return x[:, :, i, j]
    elif mode == &#39;periodic&#39;:
        # Vertical only
        if pad[0] == 0 and pad[1] == 0:
            xe = np.arange(x.shape[-2])
            xe = np.pad(xe, (pad[2], pad[3]), mode=&#39;wrap&#39;)
            return x[:, :, xe]
        # Horizontal only
        elif pad[2] == 0 and pad[3] == 0:
            xe = np.arange(x.shape[-1])
            xe = np.pad(xe, (pad[0], pad[1]), mode=&#39;wrap&#39;)
            return x[:, :, :, xe]
        # Both
        else:
            xe_col = np.arange(x.shape[-2])
            xe_col = np.pad(xe_col, (pad[2], pad[3]), mode=&#39;wrap&#39;)
            xe_row = np.arange(x.shape[-1])
            xe_row = np.pad(xe_row, (pad[0], pad[1]), mode=&#39;wrap&#39;)
            i = np.outer(xe_col, np.ones(xe_row.shape[0]))
            j = np.outer(np.ones(xe_col.shape[0]), xe_row)
            return x[:, :, i, j]

    elif mode == &#39;constant&#39; or mode == &#39;reflect&#39; or mode == &#39;replicate&#39;:
        return F.pad(x, pad, mode, value)
    elif mode == &#39;zero&#39;:
        return F.pad(x, pad)
    else:
        raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.prep_filt_afb1d"><code class="name flex">
<span>def <span class="ident">prep_filt_afb1d</span></span>(<span>h0, h1, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Prepares the filters to be of the right form for the afb2d function.
In
particular, makes the tensors the right shape. It takes mirror images of
them as as afb2d uses conv2d which acts like normal correlation.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt>h0 (array-like): low pass column filter bank</dt>
<dt>h1 (array-like): high pass column filter bank</dt>
<dt><strong><code>device</code></strong></dt>
<dd>which device to put the tensors on to</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(h0, h1)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_filt_afb1d(h0, h1, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the afb2d function.  In
    particular, makes the tensors the right shape. It takes mirror images of
    them as as afb2d uses conv2d which acts like normal correlation.

    Inputs:
        h0 (array-like): low pass column filter bank
        h1 (array-like): high pass column filter bank
        device: which device to put the tensors on to

    Returns:
        (h0, h1)
    &#34;&#34;&#34;
    h0 = np.array(h0[::-1]).ravel()
    h1 = np.array(h1[::-1]).ravel()
    t = torch.get_default_dtype()
    h0 = torch.tensor(h0, device=device, dtype=t).reshape((1, 1, -1))
    h1 = torch.tensor(h1, device=device, dtype=t).reshape((1, 1, -1))
    return h0, h1</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.prep_filt_afb2d"><code class="name flex">
<span>def <span class="ident">prep_filt_afb2d</span></span>(<span>h0_col, h1_col, h0_row=None, h1_row=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Prepares the filters to be of the right form for the afb2d function.
In
particular, makes the tensors the right shape. It takes mirror images of
them as as afb2d uses conv2d which acts like normal correlation.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt>h0_col (array-like): low pass column filter bank</dt>
<dt>h1_col (array-like): high pass column filter bank</dt>
<dt>h0_row (array-like): low pass row filter bank. If none, will assume the</dt>
<dt>same as column filter</dt>
<dt>h1_row (array-like): high pass row filter bank. If none, will assume the</dt>
<dt>same as column filter</dt>
<dt><strong><code>device</code></strong></dt>
<dd>which device to put the tensors on to</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(h0_col, h1_col, h0_row, h1_row)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_filt_afb2d(h0_col, h1_col, h0_row=None, h1_row=None, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the afb2d function.  In
    particular, makes the tensors the right shape. It takes mirror images of
    them as as afb2d uses conv2d which acts like normal correlation.

    Inputs:
        h0_col (array-like): low pass column filter bank
        h1_col (array-like): high pass column filter bank
        h0_row (array-like): low pass row filter bank. If none, will assume the
            same as column filter
        h1_row (array-like): high pass row filter bank. If none, will assume the
            same as column filter
        device: which device to put the tensors on to

    Returns:
        (h0_col, h1_col, h0_row, h1_row)
    &#34;&#34;&#34;
    h0_col, h1_col = prep_filt_afb1d(h0_col, h1_col, device)
    if h0_row is None:
        h0_row, h1_row = h0_col, h1_col
    else:
        h0_row, h1_row = prep_filt_afb1d(h0_row, h1_row, device)

    h0_col = h0_col.reshape((1, 1, -1, 1))
    h1_col = h1_col.reshape((1, 1, -1, 1))
    h0_row = h0_row.reshape((1, 1, 1, -1))
    h1_row = h1_row.reshape((1, 1, 1, -1))
    return h0_col, h1_col, h0_row, h1_row</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.prep_filt_sfb1d"><code class="name flex">
<span>def <span class="ident">prep_filt_sfb1d</span></span>(<span>g0, g1, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Prepares the filters to be of the right form for the sfb1d function. In
particular, makes the tensors the right shape. It does not mirror image them
as as sfb2d uses conv2d_transpose which acts like normal convolution.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt>g0 (array-like): low pass filter bank</dt>
<dt>g1 (array-like): high pass filter bank</dt>
<dt><strong><code>device</code></strong></dt>
<dd>which device to put the tensors on to</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(g0, g1)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_filt_sfb1d(g0, g1, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the sfb1d function. In
    particular, makes the tensors the right shape. It does not mirror image them
    as as sfb2d uses conv2d_transpose which acts like normal convolution.

    Inputs:
        g0 (array-like): low pass filter bank
        g1 (array-like): high pass filter bank
        device: which device to put the tensors on to

    Returns:
        (g0, g1)
    &#34;&#34;&#34;
    g0 = np.array(g0).ravel()
    g1 = np.array(g1).ravel()
    t = torch.get_default_dtype()
    g0 = torch.tensor(g0, device=device, dtype=t).reshape((1, 1, -1))
    g1 = torch.tensor(g1, device=device, dtype=t).reshape((1, 1, -1))

    return g0, g1</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.prep_filt_sfb2d"><code class="name flex">
<span>def <span class="ident">prep_filt_sfb2d</span></span>(<span>g0_col, g1_col, g0_row=None, g1_row=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Prepares the filters to be of the right form for the sfb2d function.
In
particular, makes the tensors the right shape. It does not mirror image them
as as sfb2d uses conv2d_transpose which acts like normal convolution.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt>g0_col (array-like): low pass column filter bank</dt>
<dt>g1_col (array-like): high pass column filter bank</dt>
<dt>g0_row (array-like): low pass row filter bank. If none, will assume the</dt>
<dt>same as column filter</dt>
<dt>g1_row (array-like): high pass row filter bank. If none, will assume the</dt>
<dt>same as column filter</dt>
<dt><strong><code>device</code></strong></dt>
<dd>which device to put the tensors on to</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(g0_col, g1_col, g0_row, g1_row)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prep_filt_sfb2d(g0_col, g1_col, g0_row=None, g1_row=None, device=None):
    &#34;&#34;&#34;
    Prepares the filters to be of the right form for the sfb2d function.  In
    particular, makes the tensors the right shape. It does not mirror image them
    as as sfb2d uses conv2d_transpose which acts like normal convolution.

    Inputs:
        g0_col (array-like): low pass column filter bank
        g1_col (array-like): high pass column filter bank
        g0_row (array-like): low pass row filter bank. If none, will assume the
            same as column filter
        g1_row (array-like): high pass row filter bank. If none, will assume the
            same as column filter
        device: which device to put the tensors on to

    Returns:
        (g0_col, g1_col, g0_row, g1_row)
    &#34;&#34;&#34;
    g0_col, g1_col = prep_filt_sfb1d(g0_col, g1_col, device)
    if g0_row is None:
        g0_row, g1_row = g0_col, g1_col
    else:
        g0_row, g1_row = prep_filt_sfb1d(g0_row, g1_row, device)

    g0_col = g0_col.reshape((1, 1, -1, 1))
    g1_col = g1_col.reshape((1, 1, -1, 1))
    g0_row = g0_row.reshape((1, 1, 1, -1))
    g1_row = g1_row.reshape((1, 1, 1, -1))

    return g0_col, g1_col, g0_row, g1_row</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.roll"><code class="name flex">
<span>def <span class="ident">roll</span></span>(<span>x, n, dim, make_even=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def roll(x, n, dim, make_even=False):
    if n &lt; 0:
        n = x.shape[dim] + n

    if make_even and x.shape[dim] % 2 == 1:
        end = 1
    else:
        end = 0

    if dim == 0:
        return torch.cat((x[-n:], x[:-n + end]), dim=0)
    elif dim == 1:
        return torch.cat((x[:, -n:], x[:, :-n + end]), dim=1)
    elif dim == 2 or dim == -2:
        return torch.cat((x[:, :, -n:], x[:, :, :-n + end]), dim=2)
    elif dim == 3 or dim == -1:
        return torch.cat((x[:, :, :, -n:], x[:, :, :, :-n + end]), dim=3)</code></pre>
</details>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.sfb1d"><code class="name flex">
<span>def <span class="ident">sfb1d</span></span>(<span>lo, hi, g0, g1, mode='zero', dim=-1)</span>
</code></dt>
<dd>
<section class="desc"><p>1D synthesis filter bank of an image tensor</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sfb1d(lo, hi, g0, g1, mode=&#39;zero&#39;, dim=-1):
    &#34;&#34;&#34; 1D synthesis filter bank of an image tensor
    &#34;&#34;&#34;
    C = lo.shape[1]
    d = dim % 4
    # If g0, g1 are not tensors, make them. If they are, then assume that they
    # are in the right order
    if not isinstance(g0, torch.Tensor):
        g0 = torch.tensor(np.copy(np.array(g0).ravel()),
                          dtype=torch.float, device=lo.device)
    if not isinstance(g1, torch.Tensor):
        g1 = torch.tensor(np.copy(np.array(g1).ravel()),
                          dtype=torch.float, device=lo.device)
    L = g0.numel()
    shape = [1, 1, 1, 1]
    shape[d] = L
    N = 2 * lo.shape[d]
    # If g aren&#39;t in the right shape, make them so
    if g0.shape != tuple(shape):
        g0 = g0.reshape(*shape)
    if g1.shape != tuple(shape):
        g1 = g1.reshape(*shape)

    s = (2, 1) if d == 2 else (1, 2)
    g0 = torch.cat([g0] * C, dim=0)
    g1 = torch.cat([g1] * C, dim=0)
    if mode == &#39;per&#39; or mode == &#39;periodization&#39;:
        y = F.conv_transpose2d(lo, g0, stride=s, groups=C) + \
            F.conv_transpose2d(hi, g1, stride=s, groups=C)
        if d == 2:
            y[:, :, :L - 2] = y[:, :, :L - 2] + y[:, :, N:N + L - 2]
            y = y[:, :, :N]
        else:
            y[:, :, :, :L - 2] = y[:, :, :, :L - 2] + y[:, :, :, N:N + L - 2]
            y = y[:, :, :, :N]
        y = roll(y, 1 - L // 2, dim=dim)
    else:
        if mode == &#39;zero&#39; or mode == &#39;symmetric&#39; or mode == &#39;reflect&#39; or \
                mode == &#39;periodic&#39;:
            pad = (L - 2, 0) if d == 2 else (0, L - 2)
            y = F.conv_transpose2d(lo, g0, stride=s, padding=pad, groups=C) + \
                F.conv_transpose2d(hi, g1, stride=s, padding=pad, groups=C)
        else:
            raise ValueError(&#34;Unkown pad type: {}&#34;.format(mode))

    return y</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.AFB1D"><code class="flex name class">
<span>class <span class="ident">AFB1D</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Does a single level 1d wavelet decomposition of an input.</p>
<p>Needs to have the tensors in the right form. Because this function defines
its own backward pass, saves on memory by not having to save the input
tensors.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input to decompose</dd>
<dt><strong><code>h0</code></strong></dt>
<dd>lowpass</dd>
<dt><strong><code>h1</code></strong></dt>
<dd>highpass</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code></dt>
<dd>use mode_to_int to get the int code here</dd>
</dl>
<p>We encode the mode as an integer rather than a string as gradcheck causes an
error when a string is provided.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>x0</code></strong></dt>
<dd>Tensor of shape (N, C, L') - lowpass</dd>
<dt><strong><code>x1</code></strong></dt>
<dd>Tensor of shape (N, C, L') - highpass</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AFB1D(Function):
    &#34;&#34;&#34; Does a single level 1d wavelet decomposition of an input.

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0: lowpass
        h1: highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        x0: Tensor of shape (N, C, L&#39;) - lowpass
        x1: Tensor of shape (N, C, L&#39;) - highpass
    &#34;&#34;&#34;

    @staticmethod
    def forward(x, h0, h1, mode):
        mode = int_to_mode(mode)

        # Make inputs 4d
        x = x[:, :, None, :]
        h0 = h0[:, :, None, :]
        h1 = h1[:, :, None, :]

        lohi = afb1d(x, h0, h1, mode=mode, dim=3)
        x0 = lohi[:, ::2, 0].contiguous()
        x1 = lohi[:, 1::2, 0].contiguous()
        return x0, x1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.AFB1D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>x, h0, h1, mode)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(x, h0, h1, mode):
    mode = int_to_mode(mode)

    # Make inputs 4d
    x = x[:, :, None, :]
    h0 = h0[:, :, None, :]
    h1 = h1[:, :, None, :]

    lohi = afb1d(x, h0, h1, mode=mode, dim=3)
    x0 = lohi[:, ::2, 0].contiguous()
    x1 = lohi[:, 1::2, 0].contiguous()
    return x0, x1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.AFB2D"><code class="flex name class">
<span>class <span class="ident">AFB2D</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Does a single level 2d wavelet decomposition of an input. Does separate
row and column filtering by two calls to
:py:func:<code>pytorch_wavelets.dwt.lowlevel.afb1d</code></p>
<p>Needs to have the tensors in the right form. Because this function defines
its own backward pass, saves on memory by not having to save the input
tensors.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input to decompose</dd>
<dt><strong><code>h0_row</code></strong></dt>
<dd>row lowpass</dd>
<dt><strong><code>h1_row</code></strong></dt>
<dd>row highpass</dd>
<dt><strong><code>h0_col</code></strong></dt>
<dd>col lowpass</dd>
<dt><strong><code>h1_col</code></strong></dt>
<dd>col highpass</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code></dt>
<dd>use mode_to_int to get the int code here</dd>
</dl>
<p>We encode the mode as an integer rather than a string as gradcheck causes an
error when a string is provided.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>Tensor of shape (N, C*4, H, W)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AFB2D(Function):
    &#34;&#34;&#34; Does a single level 2d wavelet decomposition of an input. Does separate
    row and column filtering by two calls to
    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0_row: row lowpass
        h1_row: row highpass
        h0_col: col lowpass
        h1_col: col highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*4, H, W)
    &#34;&#34;&#34;

    @staticmethod
    def forward(x, h0_row, h1_row, h0_col, h1_col, mode):
        mode = int_to_mode(mode)
        lohi = afb1d(x, h0_row, h1_row, mode=mode, dim=3)
        y = afb1d(lohi, h0_col, h1_col, mode=mode, dim=2)
        s = y.shape
        y = y.reshape(s[0], -1, 4, s[-2], s[-1])
        low = y[:, :, 0].contiguous()
        highs = y[:, :, 1:].contiguous()
        return low, highs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.AFB2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>x, h0_row, h1_row, h0_col, h1_col, mode)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(x, h0_row, h1_row, h0_col, h1_col, mode):
    mode = int_to_mode(mode)
    lohi = afb1d(x, h0_row, h1_row, mode=mode, dim=3)
    y = afb1d(lohi, h0_col, h1_col, mode=mode, dim=2)
    s = y.shape
    y = y.reshape(s[0], -1, 4, s[-2], s[-1])
    low = y[:, :, 0].contiguous()
    highs = y[:, :, 1:].contiguous()
    return low, highs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.SFB1D"><code class="flex name class">
<span>class <span class="ident">SFB1D</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Does a single level 1d wavelet decomposition of an input.</p>
<p>Needs to have the tensors in the right form. Because this function defines
its own backward pass, saves on memory by not having to save the input
tensors.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>low</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Lowpass to reconstruct of shape (N, C, L)</dd>
<dt><strong><code>high</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Highpass to reconstruct of shape (N, C, L)</dd>
<dt><strong><code>g0</code></strong></dt>
<dd>lowpass</dd>
<dt><strong><code>g1</code></strong></dt>
<dd>highpass</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code></dt>
<dd>use mode_to_int to get the int code here</dd>
</dl>
<p>We encode the mode as an integer rather than a string as gradcheck causes an
error when a string is provided.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>Tensor of shape (N, C*2, L')</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SFB1D(Function):
    &#34;&#34;&#34; Does a single level 1d wavelet decomposition of an input.

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        low (torch.Tensor): Lowpass to reconstruct of shape (N, C, L)
        high (torch.Tensor): Highpass to reconstruct of shape (N, C, L)
        g0: lowpass
        g1: highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*2, L&#39;)
    &#34;&#34;&#34;

    @staticmethod
    def forward(low, high, g0, g1, mode):
        mode = int_to_mode(mode)
        # Make into a 2d tensor with 1 row
        low = low[:, :, None, :]
        high = high[:, :, None, :]
        g0 = g0[:, :, None, :]
        g1 = g1[:, :, None, :]

        return sfb1d(low, high, g0, g1, mode=mode, dim=3)[:, :, 0]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.SFB1D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>low, high, g0, g1, mode)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(low, high, g0, g1, mode):
    mode = int_to_mode(mode)
    # Make into a 2d tensor with 1 row
    low = low[:, :, None, :]
    high = high[:, :, None, :]
    g0 = g0[:, :, None, :]
    g1 = g1[:, :, None, :]

    return sfb1d(low, high, g0, g1, mode=mode, dim=3)[:, :, 0]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="awd.adaptive_wavelets.lowlevel.SFB2D"><code class="flex name class">
<span>class <span class="ident">SFB2D</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Does a single level 2d wavelet decomposition of an input. Does separate
row and column filtering by two calls to
:py:func:<code>pytorch_wavelets.dwt.lowlevel.afb1d</code></p>
<p>Needs to have the tensors in the right form. Because this function defines
its own backward pass, saves on memory by not having to save the input
tensors.</p>
<h2 id="inputs">Inputs</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input to decompose</dd>
<dt><strong><code>h0_row</code></strong></dt>
<dd>row lowpass</dd>
<dt><strong><code>h1_row</code></strong></dt>
<dd>row highpass</dd>
<dt><strong><code>h0_col</code></strong></dt>
<dd>col lowpass</dd>
<dt><strong><code>h1_col</code></strong></dt>
<dd>col highpass</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>int</code></dt>
<dd>use mode_to_int to get the int code here</dd>
</dl>
<p>We encode the mode as an integer rather than a string as gradcheck causes an
error when a string is provided.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>Tensor of shape (N, C*4, H, W)</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SFB2D(Function):
    &#34;&#34;&#34; Does a single level 2d wavelet decomposition of an input. Does separate
    row and column filtering by two calls to
    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`

    Needs to have the tensors in the right form. Because this function defines
    its own backward pass, saves on memory by not having to save the input
    tensors.

    Inputs:
        x (torch.Tensor): Input to decompose
        h0_row: row lowpass
        h1_row: row highpass
        h0_col: col lowpass
        h1_col: col highpass
        mode (int): use mode_to_int to get the int code here

    We encode the mode as an integer rather than a string as gradcheck causes an
    error when a string is provided.

    Returns:
        y: Tensor of shape (N, C*4, H, W)
    &#34;&#34;&#34;

    @staticmethod
    def forward(low, highs, g0_row, g1_row, g0_col, g1_col, mode):
        mode = int_to_mode(mode)

        lh, hl, hh = torch.unbind(highs, dim=2)
        lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)
        hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)
        y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)
        return y</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="awd.adaptive_wavelets.lowlevel.SFB2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>low, highs, g0_row, g1_row, g0_col, g1_col, mode)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(low, highs, g0_row, g1_row, g0_col, g1_col, mode):
    mode = int_to_mode(mode)

    lh, hl, hh = torch.unbind(highs, dim=2)
    lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)
    hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)
    y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)
    return y</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="awd.adaptive_wavelets" href="index.html">awd.adaptive_wavelets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="awd.adaptive_wavelets.lowlevel.afb1d" href="#awd.adaptive_wavelets.lowlevel.afb1d">afb1d</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.int_to_mode" href="#awd.adaptive_wavelets.lowlevel.int_to_mode">int_to_mode</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.mode_to_int" href="#awd.adaptive_wavelets.lowlevel.mode_to_int">mode_to_int</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.mypad" href="#awd.adaptive_wavelets.lowlevel.mypad">mypad</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.prep_filt_afb1d" href="#awd.adaptive_wavelets.lowlevel.prep_filt_afb1d">prep_filt_afb1d</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.prep_filt_afb2d" href="#awd.adaptive_wavelets.lowlevel.prep_filt_afb2d">prep_filt_afb2d</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.prep_filt_sfb1d" href="#awd.adaptive_wavelets.lowlevel.prep_filt_sfb1d">prep_filt_sfb1d</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.prep_filt_sfb2d" href="#awd.adaptive_wavelets.lowlevel.prep_filt_sfb2d">prep_filt_sfb2d</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.roll" href="#awd.adaptive_wavelets.lowlevel.roll">roll</a></code></li>
<li><code><a title="awd.adaptive_wavelets.lowlevel.sfb1d" href="#awd.adaptive_wavelets.lowlevel.sfb1d">sfb1d</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="awd.adaptive_wavelets.lowlevel.AFB1D" href="#awd.adaptive_wavelets.lowlevel.AFB1D">AFB1D</a></code></h4>
<ul class="">
<li><code><a title="awd.adaptive_wavelets.lowlevel.AFB1D.forward" href="#awd.adaptive_wavelets.lowlevel.AFB1D.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="awd.adaptive_wavelets.lowlevel.AFB2D" href="#awd.adaptive_wavelets.lowlevel.AFB2D">AFB2D</a></code></h4>
<ul class="">
<li><code><a title="awd.adaptive_wavelets.lowlevel.AFB2D.forward" href="#awd.adaptive_wavelets.lowlevel.AFB2D.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="awd.adaptive_wavelets.lowlevel.SFB1D" href="#awd.adaptive_wavelets.lowlevel.SFB1D">SFB1D</a></code></h4>
<ul class="">
<li><code><a title="awd.adaptive_wavelets.lowlevel.SFB1D.forward" href="#awd.adaptive_wavelets.lowlevel.SFB1D.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="awd.adaptive_wavelets.lowlevel.SFB2D" href="#awd.adaptive_wavelets.lowlevel.SFB2D">SFB2D</a></code></h4>
<ul class="">
<li><code><a title="awd.adaptive_wavelets.lowlevel.SFB2D.forward" href="#awd.adaptive_wavelets.lowlevel.SFB2D.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>