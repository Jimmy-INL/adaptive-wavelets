<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>adaptive_wavelets.dsets.images.dset API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>adaptive_wavelets.dsets.images.dset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import subprocess
import os
import abc
import hashlib
import zipfile
import glob
import logging
import tarfile
from skimage.io import imread
from PIL import Image
from tqdm import tqdm
import numpy as np

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, datasets

DIR = os.path.abspath(os.path.dirname(__file__))
COLOUR_BLACK = 0
COLOUR_WHITE = 1
DATASETS_DICT = {&#34;mnist&#34;: &#34;MNIST&#34;,
                 &#34;fashion&#34;: &#34;FashionMNIST&#34;,
                 &#34;dsprites&#34;: &#34;DSprites&#34;,
                 &#34;celeba&#34;: &#34;CelebA&#34;,
                 &#34;chairs&#34;: &#34;Chairs&#34;}
DATASETS = list(DATASETS_DICT.keys())


def get_dataset(dataset):
    &#34;&#34;&#34;Return the correct dataset.&#34;&#34;&#34;
    dataset = dataset.lower()
    try:
        # eval because stores name as string in order to put it at top of file
        return eval(DATASETS_DICT[dataset])
    except KeyError:
        raise ValueError(&#34;Unkown dataset: {}&#34;.format(dataset))


def get_img_size(dataset):
    &#34;&#34;&#34;Return the correct image size.&#34;&#34;&#34;
    return get_dataset(dataset).img_size


def get_background(dataset):
    &#34;&#34;&#34;Return the image background color.&#34;&#34;&#34;
    return get_dataset(dataset).background_color


def get_dataloaders(dataset, root=None, shuffle=True, pin_memory=True,
                    batch_size=128, logger=logging.getLogger(__name__), **kwargs):
    &#34;&#34;&#34;A generic data loader

    Parameters
    ----------
    dataset : {&#34;mnist&#34;, &#34;fashion&#34;, &#34;dsprites&#34;, &#34;celeba&#34;, &#34;chairs&#34;}
        Name of the dataset to load

    root : str
        Path to the dataset root. If `None` uses the default one.

    kwargs :
        Additional arguments to `DataLoader`. Default values are modified.
    &#34;&#34;&#34;
    pin_memory = pin_memory and torch.cuda.is_available  # only pin if GPU available
    Dataset = get_dataset(dataset)
    dataset = Dataset(logger=logger) if root is None else Dataset(root=root, logger=logger)
    return DataLoader(dataset,
                      batch_size=batch_size,
                      shuffle=shuffle,
                      pin_memory=pin_memory,
                      **kwargs)


class DisentangledDataset(Dataset, abc.ABC):
    &#34;&#34;&#34;Base Class for disentangled VAE datasets.

    Parameters
    ----------
    root : string
        Root directory of dataset.

    transforms_list : list
        List of `torch.vision.transforms` to apply to the data when loading it.
    &#34;&#34;&#34;

    def __init__(self, root, transforms_list=[], logger=logging.getLogger(__name__)):
        self.root = root
        self.train_data = os.path.join(root, type(self).files[&#34;train&#34;])
        self.transforms = transforms.Compose(transforms_list)
        self.logger = logger

        if not os.path.isdir(root):
            self.logger.info(&#34;Downloading {} ...&#34;.format(str(type(self))))
            self.download()
            self.logger.info(&#34;Finished Downloading.&#34;)

    def __len__(self):
        return len(self.imgs)

    @abc.abstractmethod
    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`.

        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.
        &#34;&#34;&#34;
        pass

    @abc.abstractmethod
    def download(self):
        &#34;&#34;&#34;Download the dataset. &#34;&#34;&#34;
        pass


class DSprites(DisentangledDataset):
    &#34;&#34;&#34;DSprites Dataset from [1].

    Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6
    disentangled latent factors. This dataset uses 6 latents, controlling the color,
    shape, scale, rotation and position of a sprite. All possible variations of
    the latents are present. Ordering along dimension 1 is fixed and can be mapped
    back to the exact latent values that generated that image. Pixel outputs are
    different. No noise added.

    Notes
    -----
    - Link : https://github.com/deepmind/dsprites-dataset/
    - hard coded metadata because issue with python 3 loading of python 2

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,
        M., ... &amp; Lerchner, A. (2017). beta-vae: Learning basic visual concepts
        with a constrained variational framework. In International Conference
        on Learning Representations.

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz?raw=true&#34;}
    files = {&#34;train&#34;: &#34;dsprite_train.npz&#34;}
    lat_names = (&#39;shape&#39;, &#39;scale&#39;, &#39;orientation&#39;, &#39;posX&#39;, &#39;posY&#39;)
    lat_sizes = np.array([3, 6, 40, 32, 32])
    img_size = (1, 64, 64)
    background_color = COLOUR_BLACK
    lat_values = {&#39;posX&#39;: np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,
                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,
                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,
                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,
                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,
                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,
                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,
                                    0.93548387, 0.96774194, 1.]),
                  &#39;posY&#39;: np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,
                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,
                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,
                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,
                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,
                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,
                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,
                                    0.93548387, 0.96774194, 1.]),
                  &#39;scale&#39;: np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.]),
                  &#39;orientation&#39;: np.array([0., 0.16110732, 0.32221463, 0.48332195,
                                           0.64442926, 0.80553658, 0.96664389, 1.12775121,
                                           1.28885852, 1.44996584, 1.61107316, 1.77218047,
                                           1.93328779, 2.0943951, 2.25550242, 2.41660973,
                                           2.57771705, 2.73882436, 2.89993168, 3.061039,
                                           3.22214631, 3.38325363, 3.54436094, 3.70546826,
                                           3.86657557, 4.02768289, 4.1887902, 4.34989752,
                                           4.51100484, 4.67211215, 4.83321947, 4.99432678,
                                           5.1554341, 5.31654141, 5.47764873, 5.63875604,
                                           5.79986336, 5.96097068, 6.12207799, 6.28318531]),
                  &#39;shape&#39;: np.array([1., 2., 3.]),
                  &#39;color&#39;: np.array([1.])}

    def __init__(self, root=os.path.join(DIR, &#39;data/dsprites/&#39;), **kwargs):
        super().__init__(root, [transforms.ToTensor()], **kwargs)

        dataset_zip = np.load(self.train_data)
        self.imgs = dataset_zip[&#39;imgs&#39;]
        self.lat_values = dataset_zip[&#39;latents_values&#39;]
        
        ########################
        ### subsample dataset ###
        x_pos = self.lat_values[:,4]
        y_pos = self.lat_values[:,5]
        x_cutvalues = [0, 0.23, 0.49, 0.75, 1.1]
        y_cutvalues = [0, 0.1, 0.23, 0.36, 0.49]
        # y_cutvalues = [0, 0.1, 0.23, 0.36, 0.49, 0.62, 0.75, 0.88, 1.1]
        idx = np.zeros(self.imgs.shape[0], dtype=bool)

        for i in range(len(x_cutvalues)-1):
            lb = x_cutvalues[i]
            ub = x_cutvalues[i+1]
            idx1 = np.logical_and(x_pos&gt;=lb, x_pos&lt;=ub)
            lb = y_cutvalues[i]
            ub = y_cutvalues[i+1]    
            idx2 = np.logical_and(y_pos&gt;=lb, y_pos&lt;=ub)
            idx = idx + np.logical_and(idx1, idx2)

        self.imgs = self.imgs[idx,:]
        self.lat_values = self.lat_values[idx,:]             
        ########################            
            

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, &#34;-L&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, self.train_data])

    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`
        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.

        lat_value : np.array
            Array of length 6, that gives the value of each factor of variation.
        &#34;&#34;&#34;
        # stored image have binary and shape (H x W) so multiply by 255 to get pixel
        # values + add dimension
        sample = np.expand_dims(self.imgs[idx] * 255, axis=-1)

        # ToTensor transforms numpy.ndarray (H x W x C) in the range
        # [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]
        sample = self.transforms(sample)

        lat_value = self.lat_values[idx]
        return sample, lat_value


class CelebA(DisentangledDataset):
    &#34;&#34;&#34;CelebA Dataset from [1].

    CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset
    with more than 200K celebrity images, each with 40 attribute annotations.
    The images in this dataset cover large pose variations and background clutter.
    CelebA has large diversities, large quantities, and rich annotations, including
    10,177 number of identities, and 202,599 number of face images.

    Notes
    -----
    - Link : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Liu, Z., Luo, P., Wang, X., &amp; Tang, X. (2015). Deep learning face
        attributes in the wild. In Proceedings of the IEEE international conference
        on computer vision (pp. 3730-3738).

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip&#34;}
    files = {&#34;train&#34;: &#34;img_align_celeba&#34;}
    img_size = (3, 64, 64)
    background_color = COLOUR_WHITE

    def __init__(self, root=os.path.join(DIR, &#39;../data/celeba&#39;), **kwargs):
        super().__init__(root, [transforms.ToTensor()], **kwargs)

        self.imgs = glob.glob(self.train_data + &#39;/*&#39;)

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        save_path = os.path.join(self.root, &#39;celeba.zip&#39;)
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, &#34;-L&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, save_path])

        hash_code = &#39;00d2c5bc6d35e252742224ab0c1e8fcb&#39;
        assert hashlib.md5(open(save_path, &#39;rb&#39;).read()).hexdigest() == hash_code, \
            &#39;{} file is corrupted.  Remove the file and try again.&#39;.format(save_path)

        with zipfile.ZipFile(save_path) as zf:
            self.logger.info(&#34;Extracting CelebA ...&#34;)
            zf.extractall(self.root)

        os.remove(save_path)

        self.logger.info(&#34;Resizing CelebA ...&#34;)
        preprocess(self.train_data, size=type(self).img_size[1:])

    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`

        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.

        placeholder :
            Placeholder value as their are no targets.
        &#34;&#34;&#34;
        img_path = self.imgs[idx]
        # img values already between 0 and 255
        img = imread(img_path)

        # put each pixel in [0.,1.] and reshape to (C x H x W)
        img = self.transforms(img)

        # no label so return 0 (note that can&#39;t return None because)
        # dataloaders requires so
        return img, 0


class Chairs(datasets.ImageFolder):
    &#34;&#34;&#34;Chairs Dataset from [1].

    Notes
    -----
    - Link : https://www.di.ens.fr/willow/research/seeing3Dchairs

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Aubry, M., Maturana, D., Efros, A. A., Russell, B. C., &amp; Sivic, J. (2014).
        Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset
        of cad models. In Proceedings of the IEEE conference on computer vision
        and pattern recognition (pp. 3762-3769).

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://www.di.ens.fr/willow/research/seeing3Dchairs/data/rendered_chairs.tar&#34;}
    files = {&#34;train&#34;: &#34;chairs_64&#34;}
    img_size = (1, 64, 64)
    background_color = COLOUR_WHITE

    def __init__(self, root=os.path.join(DIR, &#39;../data/chairs&#39;),
                 logger=logging.getLogger(__name__)):
        self.root = root
        self.train_data = os.path.join(root, type(self).files[&#34;train&#34;])
        self.transforms = transforms.Compose([transforms.Grayscale(),
                                              transforms.ToTensor()])
        self.logger = logger

        if not os.path.isdir(root):
            self.logger.info(&#34;Downloading {} ...&#34;.format(str(type(self))))
            self.download()
            self.logger.info(&#34;Finished Downloading.&#34;)

        super().__init__(self.train_data, transform=self.transforms)

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        save_path = os.path.join(self.root, &#39;chairs.tar&#39;)
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, save_path])

        self.logger.info(&#34;Extracting Chairs ...&#34;)
        tar = tarfile.open(save_path)
        tar.extractall(self.root)
        tar.close()
        os.rename(os.path.join(self.root, &#39;rendered_chairs&#39;), self.train_data)

        os.remove(save_path)

        self.logger.info(&#34;Preprocessing Chairs ...&#34;)
        preprocess(os.path.join(self.train_data, &#39;*/*&#39;),  # root/*/*/*.png structure
                   size=type(self).img_size[1:],
                   center_crop=(400, 400))


class MNIST(datasets.MNIST):
    &#34;&#34;&#34;Mnist wrapper. Docs: `datasets.MNIST.`&#34;&#34;&#34;
    img_size = (1, 32, 32)
    background_color = COLOUR_BLACK

    def __init__(self, root=os.path.join(DIR, &#39;../data/mnist&#39;), **kwargs):
        super().__init__(root,
                         train=True,
                         download=True,
                         transform=transforms.Compose([
                             transforms.Resize(32),
                             transforms.ToTensor()
                         ]))


class FashionMNIST(datasets.FashionMNIST):
    &#34;&#34;&#34;Fashion Mnist wrapper. Docs: `datasets.FashionMNIST.`&#34;&#34;&#34;
    img_size = (1, 32, 32)

    def __init__(self, root=os.path.join(DIR, &#39;../data/fashionMnist&#39;), **kwargs):
        super().__init__(root,
                         train=True,
                         download=True,
                         transform=transforms.Compose([
                             transforms.Resize(32),
                             transforms.ToTensor()
                         ]))


# HELPERS
def preprocess(root, size=(64, 64), img_format=&#39;JPEG&#39;, center_crop=None):
    &#34;&#34;&#34;Preprocess a folder of images.

    Parameters
    ----------
    root : string
        Root directory of all images.

    size : tuple of int
        Size (width, height) to rescale the images. If `None` don&#39;t rescale.

    img_format : string
        Format to save the image in. Possible formats:
        https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.

    center_crop : tuple of int
        Size (width, height) to center-crop the images. If `None` don&#39;t center-crop.
    &#34;&#34;&#34;
    imgs = []
    for ext in [&#34;.png&#34;, &#34;.jpg&#34;, &#34;.jpeg&#34;]:
        imgs += glob.glob(os.path.join(root, &#39;*&#39; + ext))

    for img_path in tqdm(imgs):
        img = Image.open(img_path)
        width, height = img.size

        if size is not None and width != size[1] or height != size[0]:
            img = img.resize(size, Image.ANTIALIAS)

        if center_crop is not None:
            new_width, new_height = center_crop
            left = (width - new_width) // 2
            top = (height - new_height) // 2
            right = (width + new_width) // 2
            bottom = (height + new_height) // 2

            img.crop((left, top, right, bottom))

        img.save(img_path, img_format)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.get_background"><code class="name flex">
<span>def <span class="ident">get_background</span></span>(<span>dataset)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the image background color.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_background(dataset):
    &#34;&#34;&#34;Return the image background color.&#34;&#34;&#34;
    return get_dataset(dataset).background_color</code></pre>
</details>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.get_dataloaders"><code class="name flex">
<span>def <span class="ident">get_dataloaders</span></span>(<span>dataset, root=None, shuffle=True, pin_memory=True, batch_size=128, logger=&lt;Logger adaptive_wavelets.dsets.images.dset (WARNING)&gt;, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>A generic data loader</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;{<code>"mnist"</code>, <code>"fashion"</code>, <code>"dsprites"</code>, <code>"celeba"</code>, <code>"chairs"</code>}</dt>
<dd>Name of the dataset to load</dd>
<dt><strong><code>root</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the dataset root. If <code>None</code> uses the default one.</dd>
</dl>
<p>kwargs :
Additional arguments to <code>DataLoader</code>. Default values are modified.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataloaders(dataset, root=None, shuffle=True, pin_memory=True,
                    batch_size=128, logger=logging.getLogger(__name__), **kwargs):
    &#34;&#34;&#34;A generic data loader

    Parameters
    ----------
    dataset : {&#34;mnist&#34;, &#34;fashion&#34;, &#34;dsprites&#34;, &#34;celeba&#34;, &#34;chairs&#34;}
        Name of the dataset to load

    root : str
        Path to the dataset root. If `None` uses the default one.

    kwargs :
        Additional arguments to `DataLoader`. Default values are modified.
    &#34;&#34;&#34;
    pin_memory = pin_memory and torch.cuda.is_available  # only pin if GPU available
    Dataset = get_dataset(dataset)
    dataset = Dataset(logger=logger) if root is None else Dataset(root=root, logger=logger)
    return DataLoader(dataset,
                      batch_size=batch_size,
                      shuffle=shuffle,
                      pin_memory=pin_memory,
                      **kwargs)</code></pre>
</details>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.get_dataset"><code class="name flex">
<span>def <span class="ident">get_dataset</span></span>(<span>dataset)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the correct dataset.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset(dataset):
    &#34;&#34;&#34;Return the correct dataset.&#34;&#34;&#34;
    dataset = dataset.lower()
    try:
        # eval because stores name as string in order to put it at top of file
        return eval(DATASETS_DICT[dataset])
    except KeyError:
        raise ValueError(&#34;Unkown dataset: {}&#34;.format(dataset))</code></pre>
</details>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.get_img_size"><code class="name flex">
<span>def <span class="ident">get_img_size</span></span>(<span>dataset)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the correct image size.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_img_size(dataset):
    &#34;&#34;&#34;Return the correct image size.&#34;&#34;&#34;
    return get_dataset(dataset).img_size</code></pre>
</details>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>root, size=(64, 64), img_format='JPEG', center_crop=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Preprocess a folder of images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>string</code></dt>
<dd>Root directory of all images.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Size (width, height) to rescale the images. If <code>None</code> don't rescale.</dd>
<dt><strong><code>img_format</code></strong> :&ensp;<code>string</code></dt>
<dd>Format to save the image in. Possible formats:
<a href="https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.">https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.</a></dd>
<dt><strong><code>center_crop</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Size (width, height) to center-crop the images. If <code>None</code> don't center-crop.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(root, size=(64, 64), img_format=&#39;JPEG&#39;, center_crop=None):
    &#34;&#34;&#34;Preprocess a folder of images.

    Parameters
    ----------
    root : string
        Root directory of all images.

    size : tuple of int
        Size (width, height) to rescale the images. If `None` don&#39;t rescale.

    img_format : string
        Format to save the image in. Possible formats:
        https://pillow.readthedocs.io/en/3.1.x/handbook/image-file-formats.html.

    center_crop : tuple of int
        Size (width, height) to center-crop the images. If `None` don&#39;t center-crop.
    &#34;&#34;&#34;
    imgs = []
    for ext in [&#34;.png&#34;, &#34;.jpg&#34;, &#34;.jpeg&#34;]:
        imgs += glob.glob(os.path.join(root, &#39;*&#39; + ext))

    for img_path in tqdm(imgs):
        img = Image.open(img_path)
        width, height = img.size

        if size is not None and width != size[1] or height != size[0]:
            img = img.resize(size, Image.ANTIALIAS)

        if center_crop is not None:
            new_width, new_height = center_crop
            left = (width - new_width) // 2
            top = (height - new_height) // 2
            right = (width + new_width) // 2
            bottom = (height + new_height) // 2

            img.crop((left, top, right, bottom))

        img.save(img_path, img_format)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.CelebA"><code class="flex name class">
<span>class <span class="ident">CelebA</span></span>
<span>(</span><span>root='/accounts/projects/vision/chandan/adaptive-wavelets/adaptive_wavelets/dsets/images/../data/celeba', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>CelebA Dataset from [1].</p>
<p>CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset
with more than 200K celebrity images, each with 40 attribute annotations.
The images in this dataset cover large pose variations and background clutter.
CelebA has large diversities, large quantities, and rich annotations, including
10,177 number of identities, and 202,599 number of face images.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Link : <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>string</code></dt>
<dd>Root directory of dataset.</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] Liu, Z., Luo, P., Wang, X., &amp; Tang, X. (2015). Deep learning face
attributes in the wild. In Proceedings of the IEEE international conference
on computer vision (pp. 3730-3738).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CelebA(DisentangledDataset):
    &#34;&#34;&#34;CelebA Dataset from [1].

    CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset
    with more than 200K celebrity images, each with 40 attribute annotations.
    The images in this dataset cover large pose variations and background clutter.
    CelebA has large diversities, large quantities, and rich annotations, including
    10,177 number of identities, and 202,599 number of face images.

    Notes
    -----
    - Link : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Liu, Z., Luo, P., Wang, X., &amp; Tang, X. (2015). Deep learning face
        attributes in the wild. In Proceedings of the IEEE international conference
        on computer vision (pp. 3730-3738).

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip&#34;}
    files = {&#34;train&#34;: &#34;img_align_celeba&#34;}
    img_size = (3, 64, 64)
    background_color = COLOUR_WHITE

    def __init__(self, root=os.path.join(DIR, &#39;../data/celeba&#39;), **kwargs):
        super().__init__(root, [transforms.ToTensor()], **kwargs)

        self.imgs = glob.glob(self.train_data + &#39;/*&#39;)

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        save_path = os.path.join(self.root, &#39;celeba.zip&#39;)
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, &#34;-L&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, save_path])

        hash_code = &#39;00d2c5bc6d35e252742224ab0c1e8fcb&#39;
        assert hashlib.md5(open(save_path, &#39;rb&#39;).read()).hexdigest() == hash_code, \
            &#39;{} file is corrupted.  Remove the file and try again.&#39;.format(save_path)

        with zipfile.ZipFile(save_path) as zf:
            self.logger.info(&#34;Extracting CelebA ...&#34;)
            zf.extractall(self.root)

        os.remove(save_path)

        self.logger.info(&#34;Resizing CelebA ...&#34;)
        preprocess(self.train_data, size=type(self).img_size[1:])

    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`

        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.

        placeholder :
            Placeholder value as their are no targets.
        &#34;&#34;&#34;
        img_path = self.imgs[idx]
        # img values already between 0 and 255
        img = imread(img_path)

        # put each pixel in [0.,1.] and reshape to (C x H x W)
        img = self.transforms(img)

        # no label so return 0 (note that can&#39;t return None because)
        # dataloaders requires so
        return img, 0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset">DisentangledDataset</a></li>
<li>torch.utils.data.dataset.Dataset</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.CelebA.background_color"><code class="name">var <span class="ident">background_color</span></code></dt>
<dd>
<section class="desc"><p>int([x]) -&gt; integer
int(x, base=10) -&gt; integer</p>
<p>Convert a number or string to an integer, or return 0 if no arguments
are given.
If x is a number, return x.<strong>int</strong>().
For floating point
numbers, this truncates towards zero.</p>
<p>If x is not a number or if base is given, then x must be a string,
bytes, or bytearray instance representing an integer literal in the
given base.
The literal can be preceded by '+' or '-' and be surrounded
by whitespace.
The base defaults to 10.
Valid bases are 0 and 2-36.
Base 0 means to interpret the base from the string as an integer literal.</p>
<pre><code>&gt;&gt;&gt; int('0b100', base=0)
4
</code></pre></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.CelebA.files"><code class="name">var <span class="ident">files</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.CelebA.img_size"><code class="name">var <span class="ident">img_size</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.CelebA.urls"><code class="name">var <span class="ident">urls</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset">DisentangledDataset</a></b></code>:
<ul class="hlist">
<li><code><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset.download" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset.download">download</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs"><code class="flex name class">
<span>class <span class="ident">Chairs</span></span>
<span>(</span><span>root='/accounts/projects/vision/chandan/adaptive-wavelets/adaptive_wavelets/dsets/images/../data/chairs', logger=&lt;Logger adaptive_wavelets.dsets.images.dset (WARNING)&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Chairs Dataset from [1].</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Link : <a href="https://www.di.ens.fr/willow/research/seeing3Dchairs">https://www.di.ens.fr/willow/research/seeing3Dchairs</a></li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>string</code></dt>
<dd>Root directory of dataset.</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] Aubry, M., Maturana, D., Efros, A. A., Russell, B. C., &amp; Sivic, J. (2014).
Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset
of cad models. In Proceedings of the IEEE conference on computer vision
and pattern recognition (pp. 3762-3769).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Chairs(datasets.ImageFolder):
    &#34;&#34;&#34;Chairs Dataset from [1].

    Notes
    -----
    - Link : https://www.di.ens.fr/willow/research/seeing3Dchairs

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Aubry, M., Maturana, D., Efros, A. A., Russell, B. C., &amp; Sivic, J. (2014).
        Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset
        of cad models. In Proceedings of the IEEE conference on computer vision
        and pattern recognition (pp. 3762-3769).

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://www.di.ens.fr/willow/research/seeing3Dchairs/data/rendered_chairs.tar&#34;}
    files = {&#34;train&#34;: &#34;chairs_64&#34;}
    img_size = (1, 64, 64)
    background_color = COLOUR_WHITE

    def __init__(self, root=os.path.join(DIR, &#39;../data/chairs&#39;),
                 logger=logging.getLogger(__name__)):
        self.root = root
        self.train_data = os.path.join(root, type(self).files[&#34;train&#34;])
        self.transforms = transforms.Compose([transforms.Grayscale(),
                                              transforms.ToTensor()])
        self.logger = logger

        if not os.path.isdir(root):
            self.logger.info(&#34;Downloading {} ...&#34;.format(str(type(self))))
            self.download()
            self.logger.info(&#34;Finished Downloading.&#34;)

        super().__init__(self.train_data, transform=self.transforms)

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        save_path = os.path.join(self.root, &#39;chairs.tar&#39;)
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, save_path])

        self.logger.info(&#34;Extracting Chairs ...&#34;)
        tar = tarfile.open(save_path)
        tar.extractall(self.root)
        tar.close()
        os.rename(os.path.join(self.root, &#39;rendered_chairs&#39;), self.train_data)

        os.remove(save_path)

        self.logger.info(&#34;Preprocessing Chairs ...&#34;)
        preprocess(os.path.join(self.train_data, &#39;*/*&#39;),  # root/*/*/*.png structure
                   size=type(self).img_size[1:],
                   center_crop=(400, 400))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torchvision.datasets.folder.ImageFolder</li>
<li>torchvision.datasets.folder.DatasetFolder</li>
<li>torchvision.datasets.vision.VisionDataset</li>
<li>torch.utils.data.dataset.Dataset</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs.background_color"><code class="name">var <span class="ident">background_color</span></code></dt>
<dd>
<section class="desc"><p>int([x]) -&gt; integer
int(x, base=10) -&gt; integer</p>
<p>Convert a number or string to an integer, or return 0 if no arguments
are given.
If x is a number, return x.<strong>int</strong>().
For floating point
numbers, this truncates towards zero.</p>
<p>If x is not a number or if base is given, then x must be a string,
bytes, or bytearray instance representing an integer literal in the
given base.
The literal can be preceded by '+' or '-' and be surrounded
by whitespace.
The base defaults to 10.
Valid bases are 0 and 2-36.
Base 0 means to interpret the base from the string as an integer literal.</p>
<pre><code>&gt;&gt;&gt; int('0b100', base=0)
4
</code></pre></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs.files"><code class="name">var <span class="ident">files</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs.img_size"><code class="name">var <span class="ident">img_size</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs.urls"><code class="name">var <span class="ident">urls</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.Chairs.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Download the dataset.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download(self):
    &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
    save_path = os.path.join(self.root, &#39;chairs.tar&#39;)
    os.makedirs(self.root)
    subprocess.check_call([&#34;curl&#34;, type(self).urls[&#34;train&#34;],
                           &#34;--output&#34;, save_path])

    self.logger.info(&#34;Extracting Chairs ...&#34;)
    tar = tarfile.open(save_path)
    tar.extractall(self.root)
    tar.close()
    os.rename(os.path.join(self.root, &#39;rendered_chairs&#39;), self.train_data)

    os.remove(save_path)

    self.logger.info(&#34;Preprocessing Chairs ...&#34;)
    preprocess(os.path.join(self.train_data, &#39;*/*&#39;),  # root/*/*/*.png structure
               size=type(self).img_size[1:],
               center_crop=(400, 400))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites"><code class="flex name class">
<span>class <span class="ident">DSprites</span></span>
<span>(</span><span>root='/accounts/projects/vision/chandan/adaptive-wavelets/adaptive_wavelets/dsets/images/data/dsprites/', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>DSprites Dataset from [1].</p>
<p>Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6
disentangled latent factors. This dataset uses 6 latents, controlling the color,
shape, scale, rotation and position of a sprite. All possible variations of
the latents are present. Ordering along dimension 1 is fixed and can be mapped
back to the exact latent values that generated that image. Pixel outputs are
different. No noise added.</p>
<h2 id="notes">Notes</h2>
<ul>
<li>Link : <a href="https://github.com/deepmind/dsprites-dataset/">https://github.com/deepmind/dsprites-dataset/</a></li>
<li>hard coded metadata because issue with python 3 loading of python 2</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>string</code></dt>
<dd>Root directory of dataset.</dd>
</dl>
<h2 id="references">References</h2>
<p>[1] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,
M., &hellip; &amp; Lerchner, A. (2017). beta-vae: Learning basic visual concepts
with a constrained variational framework. In International Conference
on Learning Representations.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DSprites(DisentangledDataset):
    &#34;&#34;&#34;DSprites Dataset from [1].

    Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6
    disentangled latent factors. This dataset uses 6 latents, controlling the color,
    shape, scale, rotation and position of a sprite. All possible variations of
    the latents are present. Ordering along dimension 1 is fixed and can be mapped
    back to the exact latent values that generated that image. Pixel outputs are
    different. No noise added.

    Notes
    -----
    - Link : https://github.com/deepmind/dsprites-dataset/
    - hard coded metadata because issue with python 3 loading of python 2

    Parameters
    ----------
    root : string
        Root directory of dataset.

    References
    ----------
    [1] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick,
        M., ... &amp; Lerchner, A. (2017). beta-vae: Learning basic visual concepts
        with a constrained variational framework. In International Conference
        on Learning Representations.

    &#34;&#34;&#34;
    urls = {&#34;train&#34;: &#34;https://github.com/deepmind/dsprites-dataset/blob/master/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz?raw=true&#34;}
    files = {&#34;train&#34;: &#34;dsprite_train.npz&#34;}
    lat_names = (&#39;shape&#39;, &#39;scale&#39;, &#39;orientation&#39;, &#39;posX&#39;, &#39;posY&#39;)
    lat_sizes = np.array([3, 6, 40, 32, 32])
    img_size = (1, 64, 64)
    background_color = COLOUR_BLACK
    lat_values = {&#39;posX&#39;: np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,
                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,
                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,
                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,
                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,
                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,
                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,
                                    0.93548387, 0.96774194, 1.]),
                  &#39;posY&#39;: np.array([0., 0.03225806, 0.06451613, 0.09677419, 0.12903226,
                                    0.16129032, 0.19354839, 0.22580645, 0.25806452,
                                    0.29032258, 0.32258065, 0.35483871, 0.38709677,
                                    0.41935484, 0.4516129, 0.48387097, 0.51612903,
                                    0.5483871, 0.58064516, 0.61290323, 0.64516129,
                                    0.67741935, 0.70967742, 0.74193548, 0.77419355,
                                    0.80645161, 0.83870968, 0.87096774, 0.90322581,
                                    0.93548387, 0.96774194, 1.]),
                  &#39;scale&#39;: np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.]),
                  &#39;orientation&#39;: np.array([0., 0.16110732, 0.32221463, 0.48332195,
                                           0.64442926, 0.80553658, 0.96664389, 1.12775121,
                                           1.28885852, 1.44996584, 1.61107316, 1.77218047,
                                           1.93328779, 2.0943951, 2.25550242, 2.41660973,
                                           2.57771705, 2.73882436, 2.89993168, 3.061039,
                                           3.22214631, 3.38325363, 3.54436094, 3.70546826,
                                           3.86657557, 4.02768289, 4.1887902, 4.34989752,
                                           4.51100484, 4.67211215, 4.83321947, 4.99432678,
                                           5.1554341, 5.31654141, 5.47764873, 5.63875604,
                                           5.79986336, 5.96097068, 6.12207799, 6.28318531]),
                  &#39;shape&#39;: np.array([1., 2., 3.]),
                  &#39;color&#39;: np.array([1.])}

    def __init__(self, root=os.path.join(DIR, &#39;data/dsprites/&#39;), **kwargs):
        super().__init__(root, [transforms.ToTensor()], **kwargs)

        dataset_zip = np.load(self.train_data)
        self.imgs = dataset_zip[&#39;imgs&#39;]
        self.lat_values = dataset_zip[&#39;latents_values&#39;]
        
        ########################
        ### subsample dataset ###
        x_pos = self.lat_values[:,4]
        y_pos = self.lat_values[:,5]
        x_cutvalues = [0, 0.23, 0.49, 0.75, 1.1]
        y_cutvalues = [0, 0.1, 0.23, 0.36, 0.49]
        # y_cutvalues = [0, 0.1, 0.23, 0.36, 0.49, 0.62, 0.75, 0.88, 1.1]
        idx = np.zeros(self.imgs.shape[0], dtype=bool)

        for i in range(len(x_cutvalues)-1):
            lb = x_cutvalues[i]
            ub = x_cutvalues[i+1]
            idx1 = np.logical_and(x_pos&gt;=lb, x_pos&lt;=ub)
            lb = y_cutvalues[i]
            ub = y_cutvalues[i+1]    
            idx2 = np.logical_and(y_pos&gt;=lb, y_pos&lt;=ub)
            idx = idx + np.logical_and(idx1, idx2)

        self.imgs = self.imgs[idx,:]
        self.lat_values = self.lat_values[idx,:]             
        ########################            
            

    def download(self):
        &#34;&#34;&#34;Download the dataset.&#34;&#34;&#34;
        os.makedirs(self.root)
        subprocess.check_call([&#34;curl&#34;, &#34;-L&#34;, type(self).urls[&#34;train&#34;],
                               &#34;--output&#34;, self.train_data])

    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`
        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.

        lat_value : np.array
            Array of length 6, that gives the value of each factor of variation.
        &#34;&#34;&#34;
        # stored image have binary and shape (H x W) so multiply by 255 to get pixel
        # values + add dimension
        sample = np.expand_dims(self.imgs[idx] * 255, axis=-1)

        # ToTensor transforms numpy.ndarray (H x W x C) in the range
        # [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]
        sample = self.transforms(sample)

        lat_value = self.lat_values[idx]
        return sample, lat_value</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset">DisentangledDataset</a></li>
<li>torch.utils.data.dataset.Dataset</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.background_color"><code class="name">var <span class="ident">background_color</span></code></dt>
<dd>
<section class="desc"><p>int([x]) -&gt; integer
int(x, base=10) -&gt; integer</p>
<p>Convert a number or string to an integer, or return 0 if no arguments
are given.
If x is a number, return x.<strong>int</strong>().
For floating point
numbers, this truncates towards zero.</p>
<p>If x is not a number or if base is given, then x must be a string,
bytes, or bytearray instance representing an integer literal in the
given base.
The literal can be preceded by '+' or '-' and be surrounded
by whitespace.
The base defaults to 10.
Valid bases are 0 and 2-36.
Base 0 means to interpret the base from the string as an integer literal.</p>
<pre><code>&gt;&gt;&gt; int('0b100', base=0)
4
</code></pre></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.files"><code class="name">var <span class="ident">files</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.img_size"><code class="name">var <span class="ident">img_size</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.lat_names"><code class="name">var <span class="ident">lat_names</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.lat_sizes"><code class="name">var <span class="ident">lat_sizes</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.lat_values"><code class="name">var <span class="ident">lat_values</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DSprites.urls"><code class="name">var <span class="ident">urls</span></code></dt>
<dd>
<section class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></section>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset">DisentangledDataset</a></b></code>:
<ul class="hlist">
<li><code><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset.download" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset.download">download</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.DisentangledDataset"><code class="flex name class">
<span>class <span class="ident">DisentangledDataset</span></span>
<span>(</span><span>root, transforms_list=[], logger=&lt;Logger adaptive_wavelets.dsets.images.dset (WARNING)&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Base Class for disentangled VAE datasets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>string</code></dt>
<dd>Root directory of dataset.</dd>
<dt><strong><code>transforms_list</code></strong> :&ensp;<code>list</code></dt>
<dd>List of <code>torch.vision.transforms</code> to apply to the data when loading it.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DisentangledDataset(Dataset, abc.ABC):
    &#34;&#34;&#34;Base Class for disentangled VAE datasets.

    Parameters
    ----------
    root : string
        Root directory of dataset.

    transforms_list : list
        List of `torch.vision.transforms` to apply to the data when loading it.
    &#34;&#34;&#34;

    def __init__(self, root, transforms_list=[], logger=logging.getLogger(__name__)):
        self.root = root
        self.train_data = os.path.join(root, type(self).files[&#34;train&#34;])
        self.transforms = transforms.Compose(transforms_list)
        self.logger = logger

        if not os.path.isdir(root):
            self.logger.info(&#34;Downloading {} ...&#34;.format(str(type(self))))
            self.download()
            self.logger.info(&#34;Finished Downloading.&#34;)

    def __len__(self):
        return len(self.imgs)

    @abc.abstractmethod
    def __getitem__(self, idx):
        &#34;&#34;&#34;Get the image of `idx`.

        Return
        ------
        sample : torch.Tensor
            Tensor in [0.,1.] of shape `img_size`.
        &#34;&#34;&#34;
        pass

    @abc.abstractmethod
    def download(self):
        &#34;&#34;&#34;Download the dataset. &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="adaptive_wavelets.dsets.images.dset.DSprites" href="#adaptive_wavelets.dsets.images.dset.DSprites">DSprites</a></li>
<li><a title="adaptive_wavelets.dsets.images.dset.CelebA" href="#adaptive_wavelets.dsets.images.dset.CelebA">CelebA</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.DisentangledDataset.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Download the dataset.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def download(self):
    &#34;&#34;&#34;Download the dataset. &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.FashionMNIST"><code class="flex name class">
<span>class <span class="ident">FashionMNIST</span></span>
<span>(</span><span>root='/accounts/projects/vision/chandan/adaptive-wavelets/adaptive_wavelets/dsets/images/../data/fashionMnist', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Fashion Mnist wrapper. Docs: <code>datasets.FashionMNIST.</code></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FashionMNIST(datasets.FashionMNIST):
    &#34;&#34;&#34;Fashion Mnist wrapper. Docs: `datasets.FashionMNIST.`&#34;&#34;&#34;
    img_size = (1, 32, 32)

    def __init__(self, root=os.path.join(DIR, &#39;../data/fashionMnist&#39;), **kwargs):
        super().__init__(root,
                         train=True,
                         download=True,
                         transform=transforms.Compose([
                             transforms.Resize(32),
                             transforms.ToTensor()
                         ]))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torchvision.datasets.mnist.FashionMNIST</li>
<li>torchvision.datasets.mnist.MNIST</li>
<li>torchvision.datasets.vision.VisionDataset</li>
<li>torch.utils.data.dataset.Dataset</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.FashionMNIST.img_size"><code class="name">var <span class="ident">img_size</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
</dl>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.MNIST"><code class="flex name class">
<span>class <span class="ident">MNIST</span></span>
<span>(</span><span>root='/accounts/projects/vision/chandan/adaptive-wavelets/adaptive_wavelets/dsets/images/../data/mnist', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Mnist wrapper. Docs: <code>datasets.MNIST.</code></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MNIST(datasets.MNIST):
    &#34;&#34;&#34;Mnist wrapper. Docs: `datasets.MNIST.`&#34;&#34;&#34;
    img_size = (1, 32, 32)
    background_color = COLOUR_BLACK

    def __init__(self, root=os.path.join(DIR, &#39;../data/mnist&#39;), **kwargs):
        super().__init__(root,
                         train=True,
                         download=True,
                         transform=transforms.Compose([
                             transforms.Resize(32),
                             transforms.ToTensor()
                         ]))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torchvision.datasets.mnist.MNIST</li>
<li>torchvision.datasets.vision.VisionDataset</li>
<li>torch.utils.data.dataset.Dataset</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="adaptive_wavelets.dsets.images.dset.MNIST.background_color"><code class="name">var <span class="ident">background_color</span></code></dt>
<dd>
<section class="desc"><p>int([x]) -&gt; integer
int(x, base=10) -&gt; integer</p>
<p>Convert a number or string to an integer, or return 0 if no arguments
are given.
If x is a number, return x.<strong>int</strong>().
For floating point
numbers, this truncates towards zero.</p>
<p>If x is not a number or if base is given, then x must be a string,
bytes, or bytearray instance representing an integer literal in the
given base.
The literal can be preceded by '+' or '-' and be surrounded
by whitespace.
The base defaults to 10.
Valid bases are 0 and 2-36.
Base 0 means to interpret the base from the string as an integer literal.</p>
<pre><code>&gt;&gt;&gt; int('0b100', base=0)
4
</code></pre></section>
</dd>
<dt id="adaptive_wavelets.dsets.images.dset.MNIST.img_size"><code class="name">var <span class="ident">img_size</span></code></dt>
<dd>
<section class="desc"><p>Built-in immutable sequence.</p>
<p>If no argument is given, the constructor returns an empty tuple.
If iterable is specified the tuple is initialized from iterable's items.</p>
<p>If the argument is a tuple, the return value is the same object.</p></section>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="adaptive_wavelets.dsets.images" href="index.html">adaptive_wavelets.dsets.images</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.get_background" href="#adaptive_wavelets.dsets.images.dset.get_background">get_background</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.get_dataloaders" href="#adaptive_wavelets.dsets.images.dset.get_dataloaders">get_dataloaders</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.get_dataset" href="#adaptive_wavelets.dsets.images.dset.get_dataset">get_dataset</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.get_img_size" href="#adaptive_wavelets.dsets.images.dset.get_img_size">get_img_size</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.preprocess" href="#adaptive_wavelets.dsets.images.dset.preprocess">preprocess</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.CelebA" href="#adaptive_wavelets.dsets.images.dset.CelebA">CelebA</a></code></h4>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.CelebA.background_color" href="#adaptive_wavelets.dsets.images.dset.CelebA.background_color">background_color</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.CelebA.files" href="#adaptive_wavelets.dsets.images.dset.CelebA.files">files</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.CelebA.img_size" href="#adaptive_wavelets.dsets.images.dset.CelebA.img_size">img_size</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.CelebA.urls" href="#adaptive_wavelets.dsets.images.dset.CelebA.urls">urls</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.Chairs" href="#adaptive_wavelets.dsets.images.dset.Chairs">Chairs</a></code></h4>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.Chairs.background_color" href="#adaptive_wavelets.dsets.images.dset.Chairs.background_color">background_color</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.Chairs.download" href="#adaptive_wavelets.dsets.images.dset.Chairs.download">download</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.Chairs.files" href="#adaptive_wavelets.dsets.images.dset.Chairs.files">files</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.Chairs.img_size" href="#adaptive_wavelets.dsets.images.dset.Chairs.img_size">img_size</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.Chairs.urls" href="#adaptive_wavelets.dsets.images.dset.Chairs.urls">urls</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.DSprites" href="#adaptive_wavelets.dsets.images.dset.DSprites">DSprites</a></code></h4>
<ul class="two-column">
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.background_color" href="#adaptive_wavelets.dsets.images.dset.DSprites.background_color">background_color</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.files" href="#adaptive_wavelets.dsets.images.dset.DSprites.files">files</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.img_size" href="#adaptive_wavelets.dsets.images.dset.DSprites.img_size">img_size</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.lat_names" href="#adaptive_wavelets.dsets.images.dset.DSprites.lat_names">lat_names</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.lat_sizes" href="#adaptive_wavelets.dsets.images.dset.DSprites.lat_sizes">lat_sizes</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.lat_values" href="#adaptive_wavelets.dsets.images.dset.DSprites.lat_values">lat_values</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.DSprites.urls" href="#adaptive_wavelets.dsets.images.dset.DSprites.urls">urls</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset">DisentangledDataset</a></code></h4>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.DisentangledDataset.download" href="#adaptive_wavelets.dsets.images.dset.DisentangledDataset.download">download</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.FashionMNIST" href="#adaptive_wavelets.dsets.images.dset.FashionMNIST">FashionMNIST</a></code></h4>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.FashionMNIST.img_size" href="#adaptive_wavelets.dsets.images.dset.FashionMNIST.img_size">img_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="adaptive_wavelets.dsets.images.dset.MNIST" href="#adaptive_wavelets.dsets.images.dset.MNIST">MNIST</a></code></h4>
<ul class="">
<li><code><a title="adaptive_wavelets.dsets.images.dset.MNIST.background_color" href="#adaptive_wavelets.dsets.images.dset.MNIST.background_color">background_color</a></code></li>
<li><code><a title="adaptive_wavelets.dsets.images.dset.MNIST.img_size" href="#adaptive_wavelets.dsets.images.dset.MNIST.img_size">img_size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>