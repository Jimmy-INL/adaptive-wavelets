<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>src.vae.loss_hessian API documentation</title>
<meta name="description" content="Official PyTorch implementation of the Hessian Penalty regularization term from https://arxiv.org/pdf/2008.10599.pdf
Author: Bill Peebles
TensorFlow â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.vae.loss_hessian</code></h1>
</header>
<section id="section-intro">
<p>Official PyTorch implementation of the Hessian Penalty regularization term from <a href="https://arxiv.org/pdf/2008.10599.pdf">https://arxiv.org/pdf/2008.10599.pdf</a>
Author: Bill Peebles
TensorFlow Implementation (GPU + Multi-Layer): hessian_penalty_tf.py
Simple Pure NumPy Implementation: hessian_penalty_np.py</p>
<p>Simple use case where you want to apply the Hessian Penalty to the output of net w.r.t. net_input:</p>
<pre><code>&gt;&gt;&gt; from hessian_penalty_pytorch import hessian_penalty
&gt;&gt;&gt; net = MyNeuralNet()
&gt;&gt;&gt; net_input = sample_input()
&gt;&gt;&gt; loss = hessian_penalty(net, z=net_input)  # Compute hessian penalty of net's output w.r.t. net_input
&gt;&gt;&gt; loss.backward()  # Compute gradients w.r.t. net's parameters
</code></pre>
<p>If your network takes multiple inputs, simply supply them to hessian_penalty as you do in the net's forward pass. In the
following example, we assume BigGAN.forward takes a second input argument "y". Note that we always take the Hessian
Penalty w.r.t. the z argument supplied to hessian_penalty:</p>
<pre><code>&gt;&gt;&gt; from hessian_penalty_pytorch import hessian_penalty
&gt;&gt;&gt; net = BigGAN()
&gt;&gt;&gt; z_input = sample_z_vector()
&gt;&gt;&gt; class_label = sample_class_label()
&gt;&gt;&gt; loss = hessian_penalty(net, z=net_input, y=class_label)
&gt;&gt;&gt; loss.backward()
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Official PyTorch implementation of the Hessian Penalty regularization term from https://arxiv.org/pdf/2008.10599.pdf
Author: Bill Peebles
TensorFlow Implementation (GPU + Multi-Layer): hessian_penalty_tf.py
Simple Pure NumPy Implementation: hessian_penalty_np.py

Simple use case where you want to apply the Hessian Penalty to the output of net w.r.t. net_input:
&gt;&gt;&gt; from hessian_penalty_pytorch import hessian_penalty
&gt;&gt;&gt; net = MyNeuralNet()
&gt;&gt;&gt; net_input = sample_input()
&gt;&gt;&gt; loss = hessian_penalty(net, z=net_input)  # Compute hessian penalty of net&#39;s output w.r.t. net_input
&gt;&gt;&gt; loss.backward()  # Compute gradients w.r.t. net&#39;s parameters

If your network takes multiple inputs, simply supply them to hessian_penalty as you do in the net&#39;s forward pass. In the
following example, we assume BigGAN.forward takes a second input argument &#34;y&#34;. Note that we always take the Hessian
Penalty w.r.t. the z argument supplied to hessian_penalty:
&gt;&gt;&gt; from hessian_penalty_pytorch import hessian_penalty
&gt;&gt;&gt; net = BigGAN()
&gt;&gt;&gt; z_input = sample_z_vector()
&gt;&gt;&gt; class_label = sample_class_label()
&gt;&gt;&gt; loss = hessian_penalty(net, z=net_input, y=class_label)
&gt;&gt;&gt; loss.backward()
&#34;&#34;&#34;

import torch


def hessian_penalty(G, z, k=2, epsilon=0.1, reduction=torch.max, return_separately=False, G_z=None, **G_kwargs):
    &#34;&#34;&#34;
    Official PyTorch Hessian Penalty implementation.

    Note: If you want to regularize multiple network activations simultaneously, you need to
    make sure the function G you pass to hessian_penalty returns a list of those activations when it&#39;s called with
    G(z, **G_kwargs). Otherwise, if G returns a tensor the Hessian Penalty will only be computed for the final
    output of G.

    :param G: Function that maps input z to either a tensor or a list of tensors (activations)
    :param z: Input to G that the Hessian Penalty will be computed with respect to
    :param k: Number of Hessian directions to sample (must be &gt;= 2)
    :param epsilon: Amount to blur G before estimating Hessian (must be &gt; 0)
    :param reduction: Many-to-one function to reduce each pixel/neuron&#39;s individual hessian penalty into a final loss
    :param return_separately: If False, hessian penalties for each activation output by G are automatically summed into
                              a final loss. If True, the hessian penalties for each layer will be returned in a list
                              instead. If G outputs a single tensor, setting this to True will produce a length-1
                              list.
    :param G_z: [Optional small speed-up] If you have already computed G(z, **G_kwargs) for the current training
                iteration, then you can provide it here to reduce the number of forward passes of this method by 1
    :param G_kwargs: Additional inputs to G besides the z vector. For example, in BigGAN you
                     would pass the class label into this function via y=&lt;class_label_tensor&gt;

    :return: A differentiable scalar (the hessian penalty), or a list of hessian penalties if return_separately is True
    &#34;&#34;&#34;
    if G_z is None:
        G_z = G(z, **G_kwargs)
    rademacher_size = torch.Size((k, *z.size()))  # (k, N, z.size())
    xs = epsilon * rademacher(rademacher_size, device=z.device)
    second_orders = []
    for x in xs:  # Iterate over each (N, z.size()) tensor in xs
        central_second_order = multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs)
        second_orders.append(central_second_order)  # Appends a tensor with shape equal to G(z).size()
    loss = multi_stack_var_and_reduce(second_orders, reduction, return_separately)  # (k, G(z).size()) --&gt; scalar
    return loss


def rademacher(shape, device=&#39;cpu&#39;):
    &#34;&#34;&#34;Creates a random tensor of size [shape] under the Rademacher distribution (P(x=1) == P(x=-1) == 0.5)&#34;&#34;&#34;
    x = torch.empty(shape, device=device)
    x.random_(0, 2)  # Creates random tensor of 0s and 1s
    x[x == 0] = -1  # Turn the 0s into -1s
    return x


def multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs):
    &#34;&#34;&#34;Estimates the second directional derivative of G w.r.t. its input at z in the direction x&#34;&#34;&#34;
    G_to_x = G(z + x, **G_kwargs)
    G_from_x = G(z - x, **G_kwargs)

    G_to_x = listify(G_to_x)
    G_from_x = listify(G_from_x)
    G_z = listify(G_z)

    eps_sqr = epsilon ** 2
    sdd = [(G2x - 2 * G_z_base + Gfx) / eps_sqr for G2x, G_z_base, Gfx in zip(G_to_x, G_z, G_from_x)]
    return sdd


def stack_var_and_reduce(list_of_activations, reduction=torch.max):
    &#34;&#34;&#34;Equation (5) from the paper.&#34;&#34;&#34;
    second_orders = torch.stack(list_of_activations)  # (k, N, C, H, W)
    var_tensor = torch.var(second_orders, dim=0, unbiased=True)  # (N, C, H, W)
    penalty = reduction(var_tensor)  # (1,) (scalar)
    return penalty


def multi_stack_var_and_reduce(sdds, reduction=torch.max, return_separately=False):
    &#34;&#34;&#34;Iterate over all activations to be regularized, then apply Equation (5) to each.&#34;&#34;&#34;
    sum_of_penalties = 0 if not return_separately else []
    for activ_n in zip(*sdds):
        penalty = stack_var_and_reduce(activ_n, reduction)
        sum_of_penalties += penalty if not return_separately else [penalty]
    return sum_of_penalties


def listify(x):
    &#34;&#34;&#34;If x is already a list, do nothing. Otherwise, wrap x in a list.&#34;&#34;&#34;
    if isinstance(x, list):
        return x
    else:
        return [x]


def _test_hessian_penalty():
    &#34;&#34;&#34;
    A simple multi-layer test to verify the implementation.
    Function: G(z) = [z_0 * z_1, z_0**2 * z_1]
    Ground Truth Hessian Penalty: [4, 16 * z_0**2]
    &#34;&#34;&#34;
    batch_size = 10
    nz = 2
    z = torch.randn(batch_size, nz)
    def reduction(x): return x.abs().mean()
    def G(z): return [z[:, 0] * z[:, 1], (z[:, 0] ** 2) * z[:, 1]]
    ground_truth = [4, reduction(16 * z[:, 0] ** 2).item()]
    # In this simple example, we use k=100 to reduce variance, but when applied to neural networks
    # you will probably want to use a small k (e.g., k=2) due to memory considerations.
    predicted = hessian_penalty(G, z, G_z=None, k=100, reduction=reduction, return_separately=True)
    predicted = [p.item() for p in predicted]
    print(&#39;Ground Truth: %s&#39; % ground_truth)
    print(&#39;Approximation: %s&#39; % predicted)  # This should be close to ground_truth, but not exactly correct
    print(&#39;Difference: %s&#39; % [str(100 * abs(p - gt) / gt) + &#39;%&#39; for p, gt in zip(predicted, ground_truth)])


if __name__ == &#39;__main__&#39;:
    _test_hessian_penalty()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.vae.loss_hessian.hessian_penalty"><code class="name flex">
<span>def <span class="ident">hessian_penalty</span></span>(<span>G, z, k=2, epsilon=0.1, reduction=&lt;built-in method max of type object&gt;, return_separately=False, G_z=None, **G_kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Official PyTorch Hessian Penalty implementation.</p>
<p>Note: If you want to regularize multiple network activations simultaneously, you need to
make sure the function G you pass to hessian_penalty returns a list of those activations when it's called with
G(z, **G_kwargs). Otherwise, if G returns a tensor the Hessian Penalty will only be computed for the final
output of G.</p>
<p>:param G: Function that maps input z to either a tensor or a list of tensors (activations)
:param z: Input to G that the Hessian Penalty will be computed with respect to
:param k: Number of Hessian directions to sample (must be &gt;= 2)
:param epsilon: Amount to blur G before estimating Hessian (must be &gt; 0)
:param reduction: Many-to-one function to reduce each pixel/neuron's individual hessian penalty into a final loss
:param return_separately: If False, hessian penalties for each activation output by G are automatically summed into
a final loss. If True, the hessian penalties for each layer will be returned in a list
instead. If G outputs a single tensor, setting this to True will produce a length-1
list.
:param G_z: [Optional small speed-up] If you have already computed G(z, **G_kwargs) for the current training
iteration, then you can provide it here to reduce the number of forward passes of this method by 1
:param G_kwargs: Additional inputs to G besides the z vector. For example, in BigGAN you
would pass the class label into this function via y=<class_label_tensor></p>
<p>:return: A differentiable scalar (the hessian penalty), or a list of hessian penalties if return_separately is True</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hessian_penalty(G, z, k=2, epsilon=0.1, reduction=torch.max, return_separately=False, G_z=None, **G_kwargs):
    &#34;&#34;&#34;
    Official PyTorch Hessian Penalty implementation.

    Note: If you want to regularize multiple network activations simultaneously, you need to
    make sure the function G you pass to hessian_penalty returns a list of those activations when it&#39;s called with
    G(z, **G_kwargs). Otherwise, if G returns a tensor the Hessian Penalty will only be computed for the final
    output of G.

    :param G: Function that maps input z to either a tensor or a list of tensors (activations)
    :param z: Input to G that the Hessian Penalty will be computed with respect to
    :param k: Number of Hessian directions to sample (must be &gt;= 2)
    :param epsilon: Amount to blur G before estimating Hessian (must be &gt; 0)
    :param reduction: Many-to-one function to reduce each pixel/neuron&#39;s individual hessian penalty into a final loss
    :param return_separately: If False, hessian penalties for each activation output by G are automatically summed into
                              a final loss. If True, the hessian penalties for each layer will be returned in a list
                              instead. If G outputs a single tensor, setting this to True will produce a length-1
                              list.
    :param G_z: [Optional small speed-up] If you have already computed G(z, **G_kwargs) for the current training
                iteration, then you can provide it here to reduce the number of forward passes of this method by 1
    :param G_kwargs: Additional inputs to G besides the z vector. For example, in BigGAN you
                     would pass the class label into this function via y=&lt;class_label_tensor&gt;

    :return: A differentiable scalar (the hessian penalty), or a list of hessian penalties if return_separately is True
    &#34;&#34;&#34;
    if G_z is None:
        G_z = G(z, **G_kwargs)
    rademacher_size = torch.Size((k, *z.size()))  # (k, N, z.size())
    xs = epsilon * rademacher(rademacher_size, device=z.device)
    second_orders = []
    for x in xs:  # Iterate over each (N, z.size()) tensor in xs
        central_second_order = multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs)
        second_orders.append(central_second_order)  # Appends a tensor with shape equal to G(z).size()
    loss = multi_stack_var_and_reduce(second_orders, reduction, return_separately)  # (k, G(z).size()) --&gt; scalar
    return loss</code></pre>
</details>
</dd>
<dt id="src.vae.loss_hessian.listify"><code class="name flex">
<span>def <span class="ident">listify</span></span>(<span>x)</span>
</code></dt>
<dd>
<section class="desc"><p>If x is already a list, do nothing. Otherwise, wrap x in a list.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def listify(x):
    &#34;&#34;&#34;If x is already a list, do nothing. Otherwise, wrap x in a list.&#34;&#34;&#34;
    if isinstance(x, list):
        return x
    else:
        return [x]</code></pre>
</details>
</dd>
<dt id="src.vae.loss_hessian.multi_layer_second_directional_derivative"><code class="name flex">
<span>def <span class="ident">multi_layer_second_directional_derivative</span></span>(<span>G, z, x, G_z, epsilon, **G_kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Estimates the second directional derivative of G w.r.t. its input at z in the direction x</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multi_layer_second_directional_derivative(G, z, x, G_z, epsilon, **G_kwargs):
    &#34;&#34;&#34;Estimates the second directional derivative of G w.r.t. its input at z in the direction x&#34;&#34;&#34;
    G_to_x = G(z + x, **G_kwargs)
    G_from_x = G(z - x, **G_kwargs)

    G_to_x = listify(G_to_x)
    G_from_x = listify(G_from_x)
    G_z = listify(G_z)

    eps_sqr = epsilon ** 2
    sdd = [(G2x - 2 * G_z_base + Gfx) / eps_sqr for G2x, G_z_base, Gfx in zip(G_to_x, G_z, G_from_x)]
    return sdd</code></pre>
</details>
</dd>
<dt id="src.vae.loss_hessian.multi_stack_var_and_reduce"><code class="name flex">
<span>def <span class="ident">multi_stack_var_and_reduce</span></span>(<span>sdds, reduction=&lt;built-in method max of type object&gt;, return_separately=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Iterate over all activations to be regularized, then apply Equation (5) to each.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multi_stack_var_and_reduce(sdds, reduction=torch.max, return_separately=False):
    &#34;&#34;&#34;Iterate over all activations to be regularized, then apply Equation (5) to each.&#34;&#34;&#34;
    sum_of_penalties = 0 if not return_separately else []
    for activ_n in zip(*sdds):
        penalty = stack_var_and_reduce(activ_n, reduction)
        sum_of_penalties += penalty if not return_separately else [penalty]
    return sum_of_penalties</code></pre>
</details>
</dd>
<dt id="src.vae.loss_hessian.rademacher"><code class="name flex">
<span>def <span class="ident">rademacher</span></span>(<span>shape, device='cpu')</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a random tensor of size [shape] under the Rademacher distribution (P(x=1) == P(x=-1) == 0.5)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rademacher(shape, device=&#39;cpu&#39;):
    &#34;&#34;&#34;Creates a random tensor of size [shape] under the Rademacher distribution (P(x=1) == P(x=-1) == 0.5)&#34;&#34;&#34;
    x = torch.empty(shape, device=device)
    x.random_(0, 2)  # Creates random tensor of 0s and 1s
    x[x == 0] = -1  # Turn the 0s into -1s
    return x</code></pre>
</details>
</dd>
<dt id="src.vae.loss_hessian.stack_var_and_reduce"><code class="name flex">
<span>def <span class="ident">stack_var_and_reduce</span></span>(<span>list_of_activations, reduction=&lt;built-in method max of type object&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Equation (5) from the paper.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stack_var_and_reduce(list_of_activations, reduction=torch.max):
    &#34;&#34;&#34;Equation (5) from the paper.&#34;&#34;&#34;
    second_orders = torch.stack(list_of_activations)  # (k, N, C, H, W)
    var_tensor = torch.var(second_orders, dim=0, unbiased=True)  # (N, C, H, W)
    penalty = reduction(var_tensor)  # (1,) (scalar)
    return penalty</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.vae" href="index.html">src.vae</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.vae.loss_hessian.hessian_penalty" href="#src.vae.loss_hessian.hessian_penalty">hessian_penalty</a></code></li>
<li><code><a title="src.vae.loss_hessian.listify" href="#src.vae.loss_hessian.listify">listify</a></code></li>
<li><code><a title="src.vae.loss_hessian.multi_layer_second_directional_derivative" href="#src.vae.loss_hessian.multi_layer_second_directional_derivative">multi_layer_second_directional_derivative</a></code></li>
<li><code><a title="src.vae.loss_hessian.multi_stack_var_and_reduce" href="#src.vae.loss_hessian.multi_stack_var_and_reduce">multi_stack_var_and_reduce</a></code></li>
<li><code><a title="src.vae.loss_hessian.rademacher" href="#src.vae.loss_hessian.rademacher">rademacher</a></code></li>
<li><code><a title="src.vae.loss_hessian.stack_var_and_reduce" href="#src.vae.loss_hessian.stack_var_and_reduce">stack_var_and_reduce</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>