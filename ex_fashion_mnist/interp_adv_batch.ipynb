{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "from model_fashion_mnist import Net, Net2c, FashionCNN\n",
    "from visualize import *\n",
    "import dset_fashion_mnist as dset\n",
    "import foolbox\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: -47.7807, Accuracy: 9032/10000 (90.32%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set args\n",
    "args = dset.get_args()\n",
    "args.cuda = \"Trie\"\n",
    "args.test_batch_size = 50\n",
    "\n",
    "# load fashion-mnist dataset\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.test_batch_size, device)\n",
    "\n",
    "# model\n",
    "model = FashionCNN().to(device)\n",
    "model.load_state_dict(torch.load('./fashion-mnist.model.pth', map_location=device))\n",
    "model = model.eval().to(device)\n",
    "dset.test(model, test_loader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.0\n",
      "0.0e+00, 2.5e-02, inf\n",
      "2 of 50 attacks failed\n",
      "4 of 50 inputs misclassified without perturbation\n"
     ]
    }
   ],
   "source": [
    "# foolbox model\n",
    "preprocessing = dict(mean=[0,], std=[1,], axis=-3)\n",
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1), num_classes=10, preprocessing=preprocessing)\n",
    "\n",
    "# get a batch of images and labels and print the accuracy\n",
    "images, labels = iter(test_loader).next()\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()\n",
    "print(np.mean(fmodel.forward(images).argmax(axis=-1) == labels))\n",
    "\n",
    "# apply the attack\n",
    "attack = foolbox.attacks.FGSM(fmodel)\n",
    "im_adversarials = attack(images, labels)\n",
    "# if the i'th image is misclassfied without a perturbation, then adversarials[i] will be the same as images[i]\n",
    "# if the attack fails to find an adversarial for the i'th image, then adversarials[i] will all be np.nan\n",
    "# Foolbox guarantees that all returned adversarials are in fact in adversarials\n",
    "print(np.mean(fmodel.forward(im_adversarials).argmax(axis=-1) == labels))\n",
    "\n",
    "# # You can always get the actual adversarial class that was observed for that sample by Foolbox by\n",
    "# # passing `unpack=False` to get the actual `Adversarial` objects:\n",
    "attack = foolbox.attacks.FGSM(fmodel, distance=foolbox.distances.Linf)\n",
    "adversarials = attack(images, labels, unpack=False)\n",
    "\n",
    "adversarial_classes = np.asarray([a.adversarial_class for a in adversarials])\n",
    "# print(labels)\n",
    "# print(adversarial_classes)\n",
    "# print(np.mean(adversarial_classes == labels))  # will always be 0.0\n",
    "\n",
    "# The `Adversarial` objects also provide a `distance` attribute. Note that the distances\n",
    "# can be 0 (misclassified without perturbation) and inf (attack failed).\n",
    "distances = np.asarray([a.distance.value for a in adversarials])\n",
    "print(\"{:.1e}, {:.1e}, {:.1e}\".format(distances.min(), np.median(distances), distances.max()))\n",
    "print(\"{} of {} attacks failed\".format(sum(adv.distance.value == np.inf for adv in adversarials), len(adversarials)))\n",
    "print(\"{} of {} inputs misclassified without perturbation\".format(sum(adv.distance.value == 0 for adv in adversarials), len(adversarials)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True and perturbed labels:  Coat Pullover\n"
     ]
    }
   ],
   "source": [
    "# index with minimum dist to orig. image while perturbation suceed\n",
    "min_idx = np.where(distances == np.min(distances[np.nonzero(distances)]))[0][0]\n",
    "im = images[min_idx]\n",
    "im_a = im_adversarials[min_idx]\n",
    "viz_im_a(im.squeeze(), im_a.squeeze())\n",
    "print('True and perturbed labels: ', dset.output_label(labels[min_idx]), dset.output_label(adversarial_classes[min_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret adversarial images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "t = lambda x: torch.fft(torch.stack((x, torch.zeros_like(x)),dim=4), 2)\n",
    "transform_i = modularize(lambda x: torch.ifft(x, 2)[...,0])\n",
    "\n",
    "# prepend transformation\n",
    "model_t = TrimModel(model, transform_i)\n",
    "\n",
    "# interp methods\n",
    "attr_methods = ['IG', 'DeepLift', 'SHAP', 'CD', 'InputXGradient']\n",
    "\n",
    "# band center and width\n",
    "band_centers = list(np.arange(1, 40) * 0.025)\n",
    "band_width_lower = 0.025\n",
    "band_width_upper = 0.025\n",
    "\n",
    "# indexes\n",
    "idx_adv = np.logical_and(distances > 0, distances < np.inf)\n",
    "idx_fail = distances == np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### images that succeed adversarial attach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_scores = {\n",
    "    'IG': [],\n",
    "    'DeepLift': [],\n",
    "    'SHAP': [],\n",
    "#     'CD': [], no support on batch with different class labels\n",
    "    'InputXGradient': []\n",
    "}\n",
    "im = torch.from_numpy(images[idx_adv]).to(device)\n",
    "im_t = t(im)\n",
    "target = torch.from_numpy(labels[idx_adv]).to(device)\n",
    "\n",
    "# attr\n",
    "results = get_attributions(im_t, model_t, class_num=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr\n",
    "results = get_attributions(im_t, model_t, class_num=np.int(target))    \n",
    "for band_center in band_centers:\n",
    "    mask = freq_band(28, band_center, band_width_lower, band_width_upper)\n",
    "    for name in attr_methods:\n",
    "        im_attr = np.sum(fftshift(results[name]) * mask) \n",
    "        scores[name].append(np.array([im_attr])) \n",
    "im_scores['IG'].append(np.array(scores['IG']).flatten())\n",
    "im_scores['DeepLift'].append(np.array(scores['DeepLift']).flatten())\n",
    "im_scores['SHAP'].append(np.array(scores['SHAP']).flatten())\n",
    "im_scores['CD'].append(np.array(scores['CD']).flatten())\n",
    "im_scores['InputXGradient'].append(np.array(scores['InputXGradient']).flatten())\n",
    "print('\\riteration', i, end='')\n",
    "im_scores['IG'] = np.array(im_scores['IG'])\n",
    "im_scores['DeepLift'] = np.array(im_scores['DeepLift'])\n",
    "im_scores['SHAP'] = np.array(im_scores['SHAP'])\n",
    "im_scores['CD'] = np.array(im_scores['CD'])\n",
    "im_scores['InputXGradient'] = np.array(im_scores['InputXGradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_scores = {\n",
    "    'IG': [],\n",
    "    'DeepLift': [],\n",
    "    'SHAP': [],\n",
    "    'CD': [],\n",
    "    'InputXGradient': []\n",
    "}\n",
    "for i in range(np.sum(idx_adv)):\n",
    "    scores = {\n",
    "        'IG': [],\n",
    "        'DeepLift': [],\n",
    "        'SHAP': [],\n",
    "        'CD': [],\n",
    "        'InputXGradient': []\n",
    "    }    \n",
    "    im = images[idx_adv][i:i+1]\n",
    "    im = torch.Tensor(im).to(device)\n",
    "    im_t = t(im)\n",
    "    target = labels[idx_adv][i:i+1].item()\n",
    "    \n",
    "    # attr\n",
    "    results = get_attributions(im_t, model_t, class_num=np.int(target))    \n",
    "    for band_center in band_centers:\n",
    "        mask = freq_band(28, band_center, band_width_lower, band_width_upper)\n",
    "        for name in attr_methods:\n",
    "            im_attr = np.sum(fftshift(results[name]) * mask) \n",
    "            scores[name].append(np.array([im_attr])) \n",
    "    im_scores['IG'].append(np.array(scores['IG']).flatten())\n",
    "    im_scores['DeepLift'].append(np.array(scores['DeepLift']).flatten())\n",
    "    im_scores['SHAP'].append(np.array(scores['SHAP']).flatten())\n",
    "    im_scores['CD'].append(np.array(scores['CD']).flatten())\n",
    "    im_scores['InputXGradient'].append(np.array(scores['InputXGradient']).flatten())\n",
    "    print('\\riteration', i, end='')\n",
    "im_scores['IG'] = np.array(im_scores['IG'])\n",
    "im_scores['DeepLift'] = np.array(im_scores['DeepLift'])\n",
    "im_scores['SHAP'] = np.array(im_scores['SHAP'])\n",
    "im_scores['CD'] = np.array(im_scores['CD'])\n",
    "im_scores['InputXGradient'] = np.array(im_scores['InputXGradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adversarial images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_a_scores = {\n",
    "    'IG': [],\n",
    "    'DeepLift': [],\n",
    "    'SHAP': [],\n",
    "    'CD': [],\n",
    "    'InputXGradient': []\n",
    "}\n",
    "for i in range(np.sum(idx_adv)):\n",
    "    scores = {\n",
    "        'IG': [],\n",
    "        'DeepLift': [],\n",
    "        'SHAP': [],\n",
    "        'CD': [],\n",
    "        'InputXGradient': []\n",
    "    }    \n",
    "    im = im_adversarials[idx_adv][i:i+1]\n",
    "    im = torch.Tensor(im).to(device)\n",
    "    im_t = t(im)\n",
    "    target = adversarial_classes[idx_adv][i:i+1].item()\n",
    "    \n",
    "    # attr\n",
    "    results = get_attributions(im_t, model_t, class_num=np.int(target))    \n",
    "    for band_center in band_centers:\n",
    "        mask = freq_band(28, band_center, band_width_lower, band_width_upper)\n",
    "        for name in attr_methods:\n",
    "            im_attr = np.sum(fftshift(results[name]) * mask) \n",
    "            scores[name].append(np.array([im_attr])) \n",
    "    im_a_scores['IG'].append(np.array(scores['IG']).flatten())\n",
    "    im_a_scores['DeepLift'].append(np.array(scores['DeepLift']).flatten())\n",
    "    im_a_scores['SHAP'].append(np.array(scores['SHAP']).flatten())\n",
    "    im_a_scores['CD'].append(np.array(scores['CD']).flatten())\n",
    "    im_a_scores['InputXGradient'].append(np.array(scores['InputXGradient']).flatten())\n",
    "    print('\\riteration', i, end='')\n",
    "im_a_scores['IG'] = np.array(im_a_scores['IG'])\n",
    "im_a_scores['DeepLift'] = np.array(im_a_scores['DeepLift'])\n",
    "im_a_scores['SHAP'] = np.array(im_a_scores['SHAP'])\n",
    "im_a_scores['CD'] = np.array(im_a_scores['CD'])\n",
    "im_a_scores['InputXGradient'] = np.array(im_a_scores['InputXGradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### images that fail adversarial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f_scores = {\n",
    "    'IG': [],\n",
    "    'DeepLift': [],\n",
    "    'SHAP': [],\n",
    "    'CD': [],\n",
    "    'InputXGradient': []\n",
    "}\n",
    "for i in range(np.sum(idx_fail)):\n",
    "    scores = {\n",
    "        'IG': [],\n",
    "        'DeepLift': [],\n",
    "        'SHAP': [],\n",
    "        'CD': [],\n",
    "        'InputXGradient': []\n",
    "    }    \n",
    "    im = images[idx_fail][i:i+1]\n",
    "    im = torch.Tensor(im).to(device)\n",
    "    im_t = t(im)\n",
    "    target = labels[idx_fail][i:i+1].item()\n",
    "    \n",
    "    # attr\n",
    "    results = get_attributions(im_t, model_t, class_num=np.int(target))    \n",
    "    for band_center in band_centers:\n",
    "        mask = freq_band(28, band_center, band_width_lower, band_width_upper)\n",
    "        for name in attr_methods:\n",
    "            im_attr = np.sum(fftshift(results[name]) * mask) \n",
    "            scores[name].append(np.array([im_attr])) \n",
    "    im_f_scores['IG'].append(np.array(scores['IG']).flatten())\n",
    "    im_f_scores['DeepLift'].append(np.array(scores['DeepLift']).flatten())\n",
    "    im_f_scores['SHAP'].append(np.array(scores['SHAP']).flatten())\n",
    "    im_f_scores['CD'].append(np.array(scores['CD']).flatten())\n",
    "    im_f_scores['InputXGradient'].append(np.array(scores['InputXGradient']).flatten())\n",
    "    print('\\riteration', i, end='')\n",
    "im_f_scores['IG'] = np.array(im_f_scores['IG'])\n",
    "im_f_scores['DeepLift'] = np.array(im_f_scores['DeepLift'])\n",
    "im_f_scores['SHAP'] = np.array(im_f_scores['SHAP'])\n",
    "im_f_scores['CD'] = np.array(im_f_scores['CD'])\n",
    "im_f_scores['InputXGradient'] = np.array(im_f_scores['InputXGradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
