{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "# import acd\n",
    "from random import randint\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "sys.path.append('../../src/vae')\n",
    "sys.path.append('../../src/vae/models')\n",
    "sys.path.append('../../src/dsets/images')\n",
    "from dset import get_dataloaders\n",
    "from model import init_specific_model\n",
    "from losses import get_loss_f\n",
    "from training import Trainer\n",
    "\n",
    "sys.path.append('../../lib/trim')\n",
    "# trim modules\n",
    "from trim import DecoderEncoder\n",
    "\n",
    "sys.path.append('../../lib/disentangling-vae')\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = main.parse_arguments()\n",
    "args.dataset = \"dsprites\"\n",
    "args.model_type = \"Burgess\"\n",
    "args.latent_dim = 10\n",
    "args.img_size = (1, 64, 64)\n",
    "args.rec_dist = \"bernoulli\"\n",
    "args.reg_anneal = 0\n",
    "args.beta = 0\n",
    "args.lamPT = 1\n",
    "args.lamNN = 0.1\n",
    "args.lamH = 0\n",
    "args.lamSP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p:\n",
    "    '''Parameters for Gaussian mixture simulation\n",
    "    '''\n",
    "    # parameters for generating data\n",
    "    seed = 13\n",
    "    dataset = \"dsprites\"\n",
    "    \n",
    "    # parameters for model architecture\n",
    "    model_type = \"Burgess\"\n",
    "    latent_dim = 10 \n",
    "    img_size = (1, 64, 64)\n",
    "    \n",
    "    # parameters for training\n",
    "    train_batch_size = 64\n",
    "    test_batch_size = 100\n",
    "    lr = 1e-4\n",
    "    rec_dist = \"bernoulli\"\n",
    "    reg_anneal = 0\n",
    "    num_epochs = 100\n",
    "    \n",
    "    # hyperparameters for loss\n",
    "    beta = 0.0\n",
    "    lamPT = 0.0\n",
    "    lamNN = 0.0\n",
    "    lamH = 0.0\n",
    "    lamSP = 0.0\n",
    "    \n",
    "    # parameters for exp\n",
    "    warm_start = None # which parameter to warm start with respect to\n",
    "    seq_init = 1      # value of warm_start parameter to start with respect to\n",
    "    \n",
    "    # SAVE MODEL\n",
    "    out_dir = \"/home/ubuntu/local-vae/notebooks/ex_dsprites/results\" # wooseok's setup\n",
    "#     out_dir = '/scratch/users/vision/chandan/local-vae' # chandan's setup\n",
    "    dirname = \"vary\"\n",
    "    pid = ''.join([\"%s\" % randint(0, 9) for num in range(0, 10)])\n",
    "\n",
    "    def _str(self):\n",
    "        vals = vars(p)\n",
    "        return 'beta=' + str(vals['beta']) + '_lamPT=' + str(vals['lamPT']) + '_lamNN=' + str(vals['lamNN']) + '_lamSP=' + str(vals['lamSP']) \\\n",
    "                + '_seed=' + str(vals['seed']) + '_pid=' + vals['pid']\n",
    "    \n",
    "    def _dict(self):\n",
    "        return {attr: val for (attr, val) in vars(self).items()\n",
    "                 if not attr.startswith('_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arg in vars(args):\n",
    "    setattr(p, arg, getattr(args, arg))\n",
    "    \n",
    "# create dir\n",
    "out_dir = opj(p.out_dir, p.dirname)\n",
    "os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "# seed\n",
    "random.seed(p.seed)\n",
    "np.random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "# get dataloaders\n",
    "train_loader = get_dataloaders(p.dataset,\n",
    "                               batch_size=p.train_batch_size,\n",
    "                               logger=None)\n",
    "\n",
    "# prepare model\n",
    "model = init_specific_model(model_type=p.model_type, \n",
    "                            img_size=p.img_size,\n",
    "                            latent_dim=p.latent_dim,\n",
    "                            hidden_dim=None).to(device)\n",
    "\n",
    "# train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=p.lr)\n",
    "loss_f = get_loss_f(decoder=model.decoder, **vars(p))\n",
    "trainer = Trainer(model, optimizer, loss_f, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average train loss: 345.9500\n",
      "====> Epoch: 1 Average train loss: 159.0704\n",
      "====> Epoch: 2 Average train loss: 140.5256\n",
      "====> Epoch: 3 Average train loss: 133.0484\n",
      "====> Epoch: 4 Average train loss: 130.1644\n",
      "====> Epoch: 5 Average train loss: 128.5116\n",
      "====> Epoch: 6 Average train loss: 127.3136\n",
      "====> Epoch: 7 Average train loss: 126.2052\n",
      "====> Epoch: 8 Average train loss: 125.6266\n",
      "====> Epoch: 9 Average train loss: 125.1243\n",
      "====> Epoch: 10 Average train loss: 124.6516\n",
      "====> Epoch: 11 Average train loss: 124.4359\n",
      "====> Epoch: 12 Average train loss: 123.9639\n",
      "====> Epoch: 13 Average train loss: 123.5313\n",
      "====> Epoch: 14 Average train loss: 123.3407\n",
      "====> Epoch: 15 Average train loss: 123.2088\n",
      "====> Epoch: 16 Average train loss: 122.8320\n",
      "====> Epoch: 17 Average train loss: 122.6288\n",
      "====> Epoch: 18 Average train loss: 122.3611\n",
      "====> Epoch: 19 Average train loss: 122.2580\n",
      "====> Epoch: 20 Average train loss: 121.9465\n",
      "====> Epoch: 21 Average train loss: 121.7966\n",
      "====> Epoch: 22 Average train loss: 121.7933\n",
      "====> Epoch: 23 Average train loss: 121.6422\n",
      "====> Epoch: 24 Average train loss: 121.2667\n",
      "====> Epoch: 25 Average train loss: 121.2186\n",
      "====> Epoch: 26 Average train loss: 120.9762\n",
      "====> Epoch: 27 Average train loss: 121.0526\n",
      "====> Epoch: 28 Average train loss: 120.6903\n",
      "====> Epoch: 29 Average train loss: 120.5622\n",
      "====> Epoch: 30 Average train loss: 120.3973\n",
      "====> Epoch: 31 Average train loss: 120.4219\n",
      "====> Epoch: 32 Average train loss: 120.1322\n",
      "====> Epoch: 33 Average train loss: 119.9542\n",
      "====> Epoch: 34 Average train loss: 119.7577\n",
      "====> Epoch: 35 Average train loss: 119.8213\n",
      "====> Epoch: 36 Average train loss: 119.4057\n",
      "====> Epoch: 37 Average train loss: 118.6717\n",
      "====> Epoch: 38 Average train loss: 113.9309\n",
      "====> Epoch: 39 Average train loss: 110.4262\n",
      "====> Epoch: 40 Average train loss: 108.7121\n",
      "====> Epoch: 41 Average train loss: 108.0993\n",
      "====> Epoch: 42 Average train loss: 107.1341\n",
      "====> Epoch: 43 Average train loss: 106.3915\n",
      "====> Epoch: 44 Average train loss: 105.9807\n",
      "====> Epoch: 45 Average train loss: 105.4571\n",
      "====> Epoch: 46 Average train loss: 104.7487\n",
      "====> Epoch: 47 Average train loss: 104.0264\n",
      "====> Epoch: 48 Average train loss: 103.1271\n",
      "====> Epoch: 49 Average train loss: 102.3402\n",
      "====> Epoch: 50 Average train loss: 101.7360\n",
      "====> Epoch: 51 Average train loss: 101.7423\n",
      "====> Epoch: 52 Average train loss: 101.1836\n",
      "====> Epoch: 53 Average train loss: 100.8593\n",
      "====> Epoch: 54 Average train loss: 100.1916\n",
      "====> Epoch: 55 Average train loss: 99.8914\n",
      "====> Epoch: 56 Average train loss: 101.8510\n",
      "====> Epoch: 57 Average train loss: 104.5738\n",
      "====> Epoch: 58 Average train loss: 103.7824\n",
      "====> Epoch: 59 Average train loss: 103.1446\n",
      "====> Epoch: 60 Average train loss: 102.3009\n",
      "====> Epoch: 61 Average train loss: 100.9242\n",
      "====> Epoch: 62 Average train loss: 100.5134\n",
      "====> Epoch: 63 Average train loss: 99.1832\n",
      "====> Epoch: 64 Average train loss: 98.7407\n",
      "====> Epoch: 65 Average train loss: 97.7358\n",
      "====> Epoch: 66 Average train loss: 97.2605\n",
      "====> Epoch: 67 Average train loss: 95.9406\n",
      "====> Epoch: 68 Average train loss: 95.5656\n",
      "====> Epoch: 69 Average train loss: 94.6020\n",
      "====> Epoch: 70 Average train loss: 94.0775\n",
      "====> Epoch: 71 Average train loss: 93.7109\n",
      "====> Epoch: 72 Average train loss: 93.2877\n",
      "====> Epoch: 73 Average train loss: 93.7306\n",
      "====> Epoch: 74 Average train loss: 91.8007\n",
      "====> Epoch: 75 Average train loss: 91.7896\n",
      "====> Epoch: 76 Average train loss: 90.6826\n",
      "====> Epoch: 77 Average train loss: 90.4395\n",
      "====> Epoch: 78 Average train loss: 95.2430\n",
      "====> Epoch: 79 Average train loss: 93.7500\n",
      "====> Epoch: 81 Average train loss: 90.4351\n",
      "====> Epoch: 82 Average train loss: 89.8805\n",
      "====> Epoch: 83 Average train loss: 89.1040\n",
      "====> Epoch: 84 Average train loss: 88.4270\n",
      "====> Epoch: 85 Average train loss: 88.4739\n",
      "====> Epoch: 86 Average train loss: 87.7646\n",
      "====> Epoch: 87 Average train loss: 86.7460\n",
      "====> Epoch: 88 Average train loss: 86.3448\n",
      "====> Epoch: 89 Average train loss: 85.8884\n",
      "====> Epoch: 90 Average train loss: 85.2641\n",
      "====> Epoch: 91 Average train loss: 84.5701\n",
      "====> Epoch: 92 Average train loss: 84.4089\n",
      "====> Epoch: 93 Average train loss: 83.7876\n",
      "====> Epoch: 94 Average train loss: 82.8949\n",
      "====> Epoch: 95 Average train loss: 82.6372\n",
      "====> Epoch: 96 Average train loss: 81.8778\n",
      "====> Epoch: 97 Average train loss: 82.7194\n",
      "====> Epoch: 98 Average train loss: 80.9430\n",
      "====> Epoch: 99 Average train loss: 80.5074\n"
     ]
    }
   ],
   "source": [
    "trainer(train_loader, epochs=p.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
