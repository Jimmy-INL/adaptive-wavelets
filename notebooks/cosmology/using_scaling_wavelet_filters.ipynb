{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "from ex_cosmology import p\n",
    "from dset import get_dataloader, load_pretrained_model\n",
    "from dset import get_validation\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "from losses import get_loss_f\n",
    "from train import Trainer\n",
    "from evaluate import Validator\n",
    "from transform2d import DWT2d\n",
    "from utils import get_2dfilts, get_wavefun, low_to_high\n",
    "from wave_attributions import Attributer\n",
    "from visualize import cshow, plot_1dfilts, plot_2dfilts, plot_2dreconstruct, plot_wavefun\n",
    "\n",
    "# peakcounting\n",
    "from peak_counting import PeakCount, ModelPred, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader and model\n",
    "train_loader, test_loader = get_dataloader(p.data_path, \n",
    "                                           img_size=p.img_size[2],\n",
    "                                           split_train_test=True,\n",
    "                                           batch_size=p.batch_size) \n",
    "\n",
    "model = load_pretrained_model(model_name='resnet18', device=device, data_path=p.model_path)    \n",
    "\n",
    "# used for cross-validation\n",
    "val_loader = get_validation(p.data_path, \n",
    "                            img_size=p.img_size[2],\n",
    "                            batch_size=p.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelet params\n",
    "waves = [\"db5\"]\n",
    "mode = \"zero\"\n",
    "J = 4\n",
    "\n",
    "# result path\n",
    "path = opj(os.getcwd(), \"results\")\n",
    "dirs = [wave + \"_saliency_warmstart_seed=1\" for wave in waves]\n",
    "\n",
    "results = []\n",
    "models = []\n",
    "for i in range(len(dirs)):\n",
    "    # load results\n",
    "    out_dir = opj(path, dirs[i])\n",
    "    fnames = sorted(os.listdir(out_dir))\n",
    "    \n",
    "    results_list = []\n",
    "    models_list = []\n",
    "    for fname in fnames:\n",
    "        if fname[-3:] == 'pkl':\n",
    "            results_list.append(pkl.load(open(opj(out_dir, fname), 'rb')))\n",
    "        if fname[-3:] == 'pth':\n",
    "            wt = DWT2d(wave=waves[i], mode=mode, J=J, init_factor=1, noise_factor=0.0).to(device)\n",
    "            wt.load_state_dict(torch.load(opj(out_dir, fname)))       \n",
    "            models_list.append(wt)\n",
    "    results.append(pd.DataFrame(results_list))\n",
    "    models.append(models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indexes\n",
    "res = results[0]\n",
    "mos = models[0]\n",
    "lamL1wave = np.array(res['lamL1wave'])\n",
    "lamL1attr = np.array(res['lamL1attr'])\n",
    "lamL1wave_grid = np.unique(lamL1wave)\n",
    "lamL1attr_grid = np.unique(lamL1attr)\n",
    "R = len(lamL1wave_grid)\n",
    "C = len(lamL1attr_grid)\n",
    "\n",
    "# original wavelet\n",
    "wt_o = DWT2d(wave='db5', mode=mode, J=J).to(device)\n",
    "\n",
    "# collect results\n",
    "dic = {'psi':{},\n",
    "       'wt': {},\n",
    "       'lamL1wave': {},\n",
    "       'lamL1attr': {},\n",
    "       'index': {}}\n",
    "\n",
    "for r in range(R):\n",
    "    for c in range(C):\n",
    "        if lamL1attr_grid[c] <= 0.1:\n",
    "            loc = (lamL1wave == lamL1wave_grid[r]) & (lamL1attr == lamL1attr_grid[c])\n",
    "            if loc.sum() == 1: \n",
    "                loc = np.argwhere(loc).flatten()[0]\n",
    "                dic['index'][(r,c)] = loc\n",
    "                wt = mos[loc]\n",
    "                _, psi, x = get_wavefun(wt)\n",
    "\n",
    "                dic['wt'][(r,c)] = wt\n",
    "                dic['psi'][(r,c)] = psi     \n",
    "                dic['lamL1wave'][(r,c)] = lamL1wave_grid[r]\n",
    "                dic['lamL1attr'][(r,c)] = lamL1attr_grid[c]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(h, g, centering=True):\n",
    "    \"\"\"Given 1-d filters h, g, extract 3x3 LH,HL,HH filters with largest variation\n",
    "    \"\"\"\n",
    "    hc = h - h.mean()\n",
    "    var = []\n",
    "    for left in range(len(h)-3):\n",
    "        v = torch.sum((hc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    h_small = h[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    gc = g - g.mean()\n",
    "    var = []\n",
    "    for left in range(len(g)-3):\n",
    "        v = torch.sum((gc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    g_small = g[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    ll = h_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    lh = h_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    hl = g_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    hh = g_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    \n",
    "    if centering:\n",
    "        lh -= lh.mean()\n",
    "        hl -= hl.mean()\n",
    "        hh -= hh.mean()\n",
    "    \n",
    "    return [ll, lh, hl, hh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern_list = []\n",
    "for wt in [wt_o] + list(dic['wt'].values()):\n",
    "    filt = get_2dfilts(wt)\n",
    "    h = filt[0][0]\n",
    "    g = filt[0][1]\n",
    "    kern_list.append(extract_patches(h, g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peak counting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001685223019855431\n"
     ]
    }
   ],
   "source": [
    "filt = get_2dfilts(wt_o)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010868861277496487\n"
     ]
    }
   ],
   "source": [
    "wt = dic['wt'][(2,9)]\n",
    "filt = get_2dfilts(wt)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(h, g, centering=True):\n",
    "    \"\"\"Given 1-d filters h, g, extract 3x3 LH,HL,HH filters with largest variation\n",
    "    \"\"\"\n",
    "    hc = h - h.mean()\n",
    "    var = []\n",
    "    for left in range(len(h)-3):\n",
    "        v = torch.sum((hc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    h_small = h[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    gc = g - g.mean()\n",
    "    var = []\n",
    "    for left in range(len(g)-3):\n",
    "        v = torch.sum((gc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    g_small = g[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    ll = h_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    lh = h_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    hl = g_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    hh = g_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    \n",
    "    if centering:\n",
    "        lh -= lh.mean()\n",
    "        hl -= hl.mean()\n",
    "        hh -= hh.mean()\n",
    "    \n",
    "    return [ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007149659175576834\n"
     ]
    }
   ],
   "source": [
    "filt = get_2dfilts(wt_o)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006922352098295143\n"
     ]
    }
   ],
   "source": [
    "wt = dic['wt'][(2,9)]\n",
    "filt = get_2dfilts(wt)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(h, g, centering=True):\n",
    "    \"\"\"Given 1-d filters h, g, extract 3x3 LH,HL,HH filters with largest variation\n",
    "    \"\"\"\n",
    "    hc = h - h.mean()\n",
    "    var = []\n",
    "    for left in range(len(h)-3):\n",
    "        v = torch.sum((hc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    h_small = h[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    gc = g - g.mean()\n",
    "    var = []\n",
    "    for left in range(len(g)-3):\n",
    "        v = torch.sum((gc[left:left+3])**2)\n",
    "        var.append(v)\n",
    "    var = np.array(var)\n",
    "    g_small = g[np.argmax(var):np.argmax(var)+3]\n",
    "    \n",
    "    ll = h_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    lh = h_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    hl = g_small.unsqueeze(0)*h_small.unsqueeze(1)\n",
    "    hh = g_small.unsqueeze(0)*g_small.unsqueeze(1)\n",
    "    \n",
    "    if centering:\n",
    "        lh -= lh.mean()\n",
    "        hl -= hl.mean()\n",
    "        hh -= hh.mean()\n",
    "    \n",
    "    return [lh, hl, hh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002594819570291622\n"
     ]
    }
   ],
   "source": [
    "filt = get_2dfilts(wt_o)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015476402937567265\n"
     ]
    }
   ],
   "source": [
    "wt = dic['wt'][(2,9)]\n",
    "filt = get_2dfilts(wt)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,0.02,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(val_loader)\n",
    "print(rmse(y_params, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
