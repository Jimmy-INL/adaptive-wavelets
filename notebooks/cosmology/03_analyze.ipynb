{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "from ex_cosmology import p\n",
    "from dset import get_dataloader, load_pretrained_model\n",
    "from dset import get_validation\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "from losses import get_loss_f\n",
    "from train import Trainer\n",
    "from evaluate import Validator\n",
    "from transform2d import DWT2d\n",
    "from utils import get_2dfilts, get_wavefun\n",
    "from wave_attributions import Attributer\n",
    "from visualize import cshow, plot_1dfilts, plot_2dfilts, plot_2dreconstruct, plot_wavefun\n",
    "\n",
    "# evaluation\n",
    "from eval_cosmology import load_results, rmse_bootstrap, extract_patches\n",
    "from peak_counting import PeakCount, rmse\n",
    "\n",
    "# TRIM module\n",
    "sys.path.append('../../lib/trim')\n",
    "from trim import TrimModel\n",
    "from utils import tuple_to_tensor, tensor_to_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader and model\n",
    "train_loader, val_loader = get_dataloader(p.data_path, \n",
    "                                          img_size=p.img_size[2],\n",
    "                                          split_train_test=True,\n",
    "                                          batch_size=p.batch_size) \n",
    "\n",
    "model = load_pretrained_model(model_name='resnet18', device=device, data_path=p.model_path)   \n",
    "\n",
    "# validation dataset\n",
    "test_loader = get_validation(p.data_path, \n",
    "                             img_size=p.img_size[2],\n",
    "                             batch_size=p.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "        \"db5_saliency_warmstart_seed=1\"\n",
    "        ]\n",
    "dics, _, _ = load_results(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select optimal bin using heldout dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration bd=5/5 kern=34/340.010762824622920775\n"
     ]
    }
   ],
   "source": [
    "# DB5\n",
    "wt_o = DWT2d(wave='db5', mode='zero', J=4, \n",
    "             init_factor=1, noise_factor=0, const_factor=0)\n",
    "\n",
    "# extract kernels\n",
    "kern_list = []\n",
    "for wt in [wt_o] + list(dics[0]['wt'].values()):\n",
    "    filt = get_2dfilts(wt)\n",
    "    h = filt[0][0]\n",
    "    g = filt[0][1]\n",
    "    kern_list.append(extract_patches(h, g))\n",
    "    \n",
    "bds = np.linspace(0.015,0.035,5)\n",
    "scores = np.zeros((len(bds), len(kern_list)))  \n",
    "\n",
    "for i,b in enumerate(bds):\n",
    "    for j,kernels in enumerate(kern_list):\n",
    "        pcw = PeakCount(peak_counting_method='custom', \n",
    "                        bins=np.linspace(0,b,23),\n",
    "                        kernels=kernels)\n",
    "        pcw.fit(train_loader)\n",
    "        y_preds, y_params = pcw.predict(val_loader)\n",
    "        scores[i,j] = rmse(y_params, y_preds)\n",
    "        pkl.dump(scores, open('results/scores.pkl', 'wb'))  \n",
    "        print(\n",
    "            \"\\riteration bd={}/{} kern={}/{}\".format(\n",
    "                i + 1, len(bds), j + 1, len(kern_list)\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )        \n",
    "        \n",
    "print('\\n', np.min(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 0.03684 gamma: 0.01129\n",
      "AWD:  0.010625418474537752 0.00033850487763549216\n",
      "DB5:  0.015692681086327664 0.00048067312692403594\n"
     ]
    }
   ],
   "source": [
    "# load optimal wavelet for prediction on heldout dataset\n",
    "# scores = pkl.load(open('results/scores.pkl', 'rb'))\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "bd_opt = bds[row]\n",
    "idx1, idx2 = list(dics[0]['wt'].keys())[col-1]\n",
    "wt = dics[0]['wt'][(idx1, idx2)]   \n",
    "lamL1wave = dics[0]['lamL1wave'][(idx1, idx2)]\n",
    "lamL1attr = dics[0]['lamL1attr'][(idx1, idx2)]\n",
    "print('lambda: {} gamma: {}'.format(lamL1wave, lamL1attr))\n",
    "\n",
    "# AWD prediction performance\n",
    "filt = get_2dfilts(wt)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,bd_opt,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(test_loader)\n",
    "acc, std = rmse_bootstrap(y_preds, y_params)\n",
    "print(\"AWD: \", acc, std)\n",
    "\n",
    "# original wavelet prediction performance\n",
    "filt = get_2dfilts(wt_o)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,bds[np.argmin(scores[:,0])],23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(test_loader)\n",
    "acc, std = rmse_bootstrap(y_preds, y_params)\n",
    "print(\"DB5: \", acc, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6069843769073486 0.6201186180114746\n"
     ]
    }
   ],
   "source": [
    "# define trim model\n",
    "mt = TrimModel(model, wt.inverse, use_residuals=True)    \n",
    "mt_o = TrimModel(model, wt_o.inverse, use_residuals=True)  \n",
    "attributer = Attributer(mt, attr_methods='Saliency', device='cuda')\n",
    "attributer_o = Attributer(mt_o, attr_methods='Saliency', device='cuda')\n",
    "\n",
    "# compute compression rate and representations\n",
    "attrs = {'AWD': torch.tensor([]),\n",
    "         'DB5': torch.tensor([])}\n",
    "reps = {'AWD': torch.tensor([]),\n",
    "        'DB5': torch.tensor([])}\n",
    "wt, wt_o = wt.to(device), wt_o.to(device)\n",
    "for data, _ in test_loader:\n",
    "    data = data.to(device)\n",
    "    i = 0\n",
    "    for w in [wt, wt_o]:\n",
    "        if i == 0:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['AWD'] = torch.cat((reps['AWD'], y.detach().cpu()), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['AWD'] = torch.cat((attrs['AWD'], z.detach().cpu()), dim=0)\n",
    "        else:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer_o(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['DB5'] = torch.cat((reps['DB5'], y.detach().cpu()), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['DB5'] = torch.cat((attrs['DB5'], z.detach().cpu()), dim=0)\n",
    "        i += 1\n",
    "reps['AWD'] = reps['AWD'].reshape(-1)\n",
    "reps['DB5'] = reps['DB5'].reshape(-1) \n",
    "attrs['AWD'] = attrs['AWD'].reshape(-1)\n",
    "attrs['DB5'] = attrs['DB5'].reshape(-1) \n",
    "\n",
    "thresh1 = 1e-3\n",
    "thresh2 = 1e-3\n",
    "c_rate_AWD = 1.0*((abs(reps['AWD']) > thresh1) & (abs(attrs['AWD']) > thresh2)).sum() / reps['AWD'].shape[0]\n",
    "c_rate_DB5 = 1.0*((abs(reps['DB5']) > thresh1) & (abs(attrs['DB5']) > thresh2)).sum() / reps['DB5'].shape[0]\n",
    "print(c_rate_AWD.item(), c_rate_DB5.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# viz importance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(attributions: tuple, num):\n",
    "    \"\"\" \n",
    "    Threshold attributions retaining those with top absolute attributions \n",
    "    \"\"\"    \n",
    "    batch_size = attributions[0].shape[0]\n",
    "    J = len(attributions)\n",
    "    b = torch.tensor([]).to(device)\n",
    "    list_of_size = [0]    \n",
    "    for j in range(J):\n",
    "        a = abs(attributions[j]).reshape(batch_size, -1)\n",
    "        b = torch.cat((b,a), dim=1)\n",
    "        list_of_size.append(list_of_size[-1] + a.shape[1])\n",
    "    sort_indexes = torch.argsort(b, dim=1, descending=True)      \n",
    "    \n",
    "    m = torch.zeros_like(sort_indexes)\n",
    "    for i in range(batch_size):\n",
    "        m[i][sort_indexes[i,:num]] = 1\n",
    "\n",
    "    list_of_masks = []\n",
    "    for j in range(J):\n",
    "        n0 = list_of_size[j]\n",
    "        n1 = list_of_size[j+1]\n",
    "        list_of_masks.append(m[:,n0:n1].reshape(attributions[j].shape))\n",
    "\n",
    "    return list_of_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim model\n",
    "mt = TrimModel(model, wt.inverse, use_residuals=True) \n",
    "attributer = Attributer(mt, attr_methods='IntegratedGradient', device=device)\n",
    "\n",
    "# data\n",
    "torch.manual_seed(2) \n",
    "data = iter(train_loader).next()[0].to(device)\n",
    "num = 600\n",
    "\n",
    "# plot\n",
    "num_rows = 5\n",
    "num_cols = 2\n",
    "titsize = 6\n",
    "\n",
    "fig = plt.figure(dpi=300, figsize=(2,4))\n",
    "\n",
    "for j in range(5):\n",
    "    # AWD\n",
    "    data_t = wt(data[j:j+1])\n",
    "    attributions = attributer(data_t, target=1, additional_forward_args=deepcopy(data)) \n",
    "    mask = create_mask(attributions, num)\n",
    "    m = ()\n",
    "    for i,a in enumerate(data_t):\n",
    "        m += (a * mask[i],)\n",
    "    data_orig = data[j:j+1]\n",
    "    data_mask = wt.inverse(m)\n",
    "\n",
    "    im1 = data_orig.detach().squeeze().cpu().numpy()\n",
    "    im2 = data_mask.detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    plt.subplot(num_rows, num_cols, j*num_cols + 1)\n",
    "    plt.imshow(im1, cmap='magma', vmax=0.15, vmin=-0.05)\n",
    "    plt.axis('off')\n",
    "    if j == 0:\n",
    "        plt.title('Original image', fontsize=titsize)\n",
    "        \n",
    "    plt.subplot(num_rows, num_cols, j*num_cols + 2)\n",
    "    plt.imshow(im2, cmap='magma', vmax=0.15, vmin=-0.05)\n",
    "    plt.axis('off') \n",
    "    if j == 0:\n",
    "        plt.title('Activation map', fontsize=titsize)    \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=-0.2, hspace=0.05)\n",
    "\n",
    "# plt.savefig('figures/cosmo_act_map.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
