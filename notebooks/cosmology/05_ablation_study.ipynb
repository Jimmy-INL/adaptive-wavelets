{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import sys\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "\n",
    "from ex_cosmology import p\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "sys.path.append('../..')\n",
    "from src.mdata.cosmology import get_dataloader, load_pretrained_model\n",
    "from src.mdata.cosmology import get_validation\n",
    "from src.adaptive_wavelets.utils import tuple_to_tensor, tensor_to_tuple\n",
    "from src import adaptive_wavelets\n",
    "from src.trim import TrimModel\n",
    "\n",
    "# evaluation\n",
    "from eval_cosmology import load_results, rmse_bootstrap, extract_patches\n",
    "from peak_counting import PeakCount, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloader and model\n",
    "train_loader, val_loader = get_dataloader(p.data_path, \n",
    "                                          img_size=p.img_size[2],\n",
    "                                          split_train_test=True,\n",
    "                                          batch_size=p.batch_size) \n",
    "\n",
    "model = load_pretrained_model(model_name='resnet18', device=device, data_path=p.model_path)   \n",
    "\n",
    "# validation dataset\n",
    "test_loader = get_validation(p.data_path, \n",
    "                             img_size=p.img_size[2],\n",
    "                             batch_size=p.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "        \"db5_saliency_warmstart_seed=1_new\"\n",
    "        ]\n",
    "dics, _, _ = load_results(dirs, include_interp_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select optimal bin using heldout dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration bd=4/5 kern=2/4"
     ]
    }
   ],
   "source": [
    "# DB5\n",
    "wt_o = adaptive_wavelets.DWT2d(wave='db5', mode='zero', J=4, \n",
    "                               init_factor=1, noise_factor=0, const_factor=0)\n",
    "\n",
    "# extract kernels\n",
    "kern_list = []\n",
    "for wt in [wt_o] + list(dics[0]['wt'].values()):\n",
    "    filt = adaptive_wavelets.get_2dfilts(wt)\n",
    "    h = filt[0][0]\n",
    "    g = filt[0][1]\n",
    "    kern_list.append(extract_patches(h, g))\n",
    "    \n",
    "bds = np.linspace(0.015,0.035,5)\n",
    "scores = np.zeros((len(bds), len(kern_list)))  \n",
    "\n",
    "for i,b in enumerate(bds):\n",
    "    for j,kernels in enumerate(kern_list):\n",
    "        pcw = PeakCount(peak_counting_method='custom', \n",
    "                        bins=np.linspace(0,b,23),\n",
    "                        kernels=kernels)\n",
    "        pcw.fit(train_loader)\n",
    "        y_preds, y_params = pcw.predict(val_loader)\n",
    "        scores[i,j] = rmse(y_params, y_preds)\n",
    "        pkl.dump(scores, open('results/scores_ablation.pkl', 'wb'))  \n",
    "        print(\n",
    "            \"\\riteration bd={}/{} kern={}/{}\".format(\n",
    "                i + 1, len(bds), j + 1, len(kern_list)\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )        \n",
    "        \n",
    "print('\\n', np.min(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal wavelet for prediction on heldout dataset\n",
    "# scores = pkl.load(open('results/scores.pkl', 'rb'))\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "bd_opt = bds[row]\n",
    "idx1, idx2 = list(dics[0]['wt'].keys())[col-1]\n",
    "wt = dics[0]['wt'][(idx1, idx2)]   \n",
    "lamL1wave = dics[0]['lamL1wave'][(idx1, idx2)]\n",
    "lamL1attr = dics[0]['lamL1attr'][(idx1, idx2)]\n",
    "print('lambda: {} gamma: {}'.format(lamL1wave, lamL1attr))\n",
    "\n",
    "# AWD prediction performance\n",
    "filt = adaptive_wavelets.get_2dfilts(wt)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,bd_opt,23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(test_loader)\n",
    "acc, std = rmse_bootstrap(y_preds, y_params)\n",
    "print(\"AWD: \", acc, std)\n",
    "\n",
    "# original wavelet prediction performance\n",
    "filt = adaptive_wavelets.get_2dfilts(wt_o)\n",
    "h = filt[0][0]\n",
    "g = filt[0][1]\n",
    "kernels = extract_patches(h, g)\n",
    "pcw = PeakCount(peak_counting_method='custom', \n",
    "                bins=np.linspace(0,bds[np.argmin(scores[:,0])],23),\n",
    "                kernels=kernels)\n",
    "pcw.fit(train_loader)\n",
    "y_preds, y_params = pcw.predict(test_loader)\n",
    "acc, std = rmse_bootstrap(y_preds, y_params)\n",
    "print(\"DB5: \", acc, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trim model\n",
    "mt = TrimModel(model, wt.inverse, use_residuals=True)    \n",
    "mt_o = TrimModel(model, wt_o.inverse, use_residuals=True)  \n",
    "attributer = adaptive_wavelets.Attributer(mt, attr_methods='Saliency', device='cuda')\n",
    "attributer_o = adaptive_wavelets.Attributer(mt_o, attr_methods='Saliency', device='cuda')\n",
    "\n",
    "# compute compression rate and representations\n",
    "attrs = {'AWD': torch.tensor([]),\n",
    "         'DB5': torch.tensor([])}\n",
    "reps = {'AWD': torch.tensor([]),\n",
    "        'DB5': torch.tensor([])}\n",
    "wt, wt_o = wt.to(device), wt_o.to(device)\n",
    "for data, _ in test_loader:\n",
    "    data = data.to(device)\n",
    "    i = 0\n",
    "    for w in [wt, wt_o]:\n",
    "        if i == 0:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['AWD'] = torch.cat((reps['AWD'], y.detach().cpu()), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['AWD'] = torch.cat((attrs['AWD'], z.detach().cpu()), dim=0)\n",
    "        else:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer_o(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['DB5'] = torch.cat((reps['DB5'], y.detach().cpu()), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['DB5'] = torch.cat((attrs['DB5'], z.detach().cpu()), dim=0)\n",
    "        i += 1\n",
    "reps['AWD'] = reps['AWD'].reshape(-1)\n",
    "reps['DB5'] = reps['DB5'].reshape(-1) \n",
    "attrs['AWD'] = attrs['AWD'].reshape(-1)\n",
    "attrs['DB5'] = attrs['DB5'].reshape(-1) \n",
    "\n",
    "thresh1 = 1e-3\n",
    "thresh2 = 1e-3\n",
    "c_rate_AWD = 1.0*((abs(reps['AWD']) > thresh1) & (abs(attrs['AWD']) > thresh2)).sum() / reps['AWD'].shape[0]\n",
    "c_rate_DB5 = 1.0*((abs(reps['DB5']) > thresh1) & (abs(attrs['DB5']) > thresh2)).sum() / reps['DB5'].shape[0]\n",
    "print(c_rate_AWD.item(), c_rate_DB5.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
