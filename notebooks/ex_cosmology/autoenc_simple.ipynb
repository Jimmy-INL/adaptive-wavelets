{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "from torch import nn\n",
    "from models import AutoEncoder, AutoEncoderSimple, load_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import load_model\n",
    "sys.path.append('../../src')\n",
    "sys.path.append('../../src/vae')\n",
    "sys.path.append('../../src/vae/models')\n",
    "sys.path.append('../../src/dsets/cosmology')\n",
    "from dset import get_dataloader\n",
    "from model import init_specific_model\n",
    "from losses import get_loss_f, _reconstruction_loss\n",
    "from training import Trainer\n",
    "from viz import viz_im_r, cshow, viz_filters\n",
    "from sim_cosmology import p\n",
    "\n",
    "sys.path.append('../../lib/trim')\n",
    "# trim modules\n",
    "from trim import DecoderEncoder, TrimModel\n",
    "from captum.attr import *\n",
    "\n",
    "# wavelet\n",
    "from pytorch_wavelets import DTCWTForward, DTCWTInverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE on cosmology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "out_dir = opj(p.out_dir, p.dirname)\n",
    "os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "# seed\n",
    "random.seed(p.seed)\n",
    "np.random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "# get dataloaders\n",
    "train_loader = get_dataloader(p.data_path, \n",
    "                              img_size=256,\n",
    "                              batch_size=p.train_batch_size)\n",
    "im = iter(train_loader).next()[0][0:1]\n",
    "\n",
    "# load model\n",
    "model = load_model(model_name='resnet18', device=device, data_path=p.data_path)\n",
    "model = model.eval()\n",
    "# freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask_WaveCoeffs(nn.Module):\n",
    "    def __init__(self, img_size=256, J=5, device='cuda'):\n",
    "        super(Mask_WaveCoeffs, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num = J+1\n",
    "\n",
    "        # initialize masks \n",
    "        x = torch.randn(1,1,img_size,img_size).to(device)\n",
    "        xfm = DTCWTForward(J=J, biort='near_sym_b', qshift='qshift_b').to(device)\n",
    "        Yl, Yh = xfm(x)   \n",
    "        \n",
    "        self.mask = nn.ParameterList([nn.Parameter(torch.ones_like(Yl))])\n",
    "        for i in range(J):\n",
    "            self.mask.append(nn.Parameter(torch.ones_like(Yh[i])))\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output_list = []\n",
    "        for i in range(self.num):\n",
    "            output_list.append(torch.mul(x[i], self.mask[i]))\n",
    "        return tuple(output_list)\n",
    "    \n",
    "    \n",
    "    def projection(self):\n",
    "        for i in range(self.num):\n",
    "            self.mask[i].data = torch.clamp(self.mask[i].data, 0, 1)\n",
    "\n",
    "class InputXGradient(nn.Module):\n",
    "    def __init__(self, mt, target=1, device='cuda'): \n",
    "        super(InputXGradient, self).__init__()\n",
    "        self.mt = mt.to(device)\n",
    "        self.target = target\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        n = len(x)\n",
    "        for i in range(n):\n",
    "            x[i].retain_grad()\n",
    "        output = self.mt(x)[0][self.target]\n",
    "        output.backward(retain_graph=True)\n",
    "        \n",
    "        # input * gradient\n",
    "        scores = []\n",
    "        for i in range(n):\n",
    "            scores.append(torch.mul(x[i], x[i].grad))\n",
    "        return tuple(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "torch.manual_seed(p.seed)\n",
    "im = iter(train_loader).next()[0][0:1].to(device)\n",
    "im.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelet transform\n",
    "xfm = Wavelet_Transform(J=5)\n",
    "ifm = Wavelet_iTransform(J=5)\n",
    "\n",
    "# prepend transformation onto network\n",
    "model_t = TrimModel(model, ifm)\n",
    "\n",
    "# interp score\n",
    "attributer = InputXGradient(model_t, target=1)\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "num_lamb = 20\n",
    "lamb_l1 = np.geomspace(0.00001, 0.001, num_lamb)\n",
    "Losses = []\n",
    "masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderSimple(nn.Module):\n",
    "    def __init__(self, img_size=(1,256,256), hid_channels=2):\n",
    "        \"\"\"\n",
    "        Class which defines model and forward pass. Consists of one layer of Conv and ConvTranspose.\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_size : tuple of (C,H,W)\n",
    "        \n",
    "        hid_channels : int\n",
    "            number of hidden channels\n",
    "        \"\"\"\n",
    "        super(AutoEncoderSimple, self).__init__()\n",
    "        \n",
    "        # Layer parameters\n",
    "        kernel_size = 4\n",
    "        n_chan = img_size[0]\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Convolutional layers\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "        self.conv1 = nn.Conv2d(n_chan, hid_channels, kernel_size, **cnn_kwargs)       \n",
    "        self.bn1 = nn.BatchNorm2d(hid_channels)\n",
    "\n",
    "        # Transpose Convolutional layers\n",
    "        self.convT1 = nn.ConvTranspose2d(hid_channels, n_chan, kernel_size, **cnn_kwargs)\n",
    "        self.bn2 = nn.BatchNorm2d(n_chan)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Convolutional layers with ReLu activations\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.bn2(self.convT1(x))\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x            \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
    "        \"\"\"\n",
    "        latent_sample = self.encoder(x)\n",
    "        reconstruct = self.decoder(latent_sample)\n",
    "        \n",
    "        return reconstruct, latent_sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir\n",
    "out_dir = opj(p.out_dir, p.dirname)\n",
    "os.makedirs(out_dir, exist_ok=True)  \n",
    "\n",
    "# seed\n",
    "random.seed(p.seed)\n",
    "np.random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "# get dataloaders\n",
    "train_loader = get_dataloader(p.data_path, \n",
    "                              batch_size=p.train_batch_size)\n",
    "im = iter(train_loader).next()[0].to(device)\n",
    "\n",
    "# prepare model\n",
    "model = AutoEncoderSimple(img_size=p.img_size, hid_channels=2).to(device)\n",
    "\n",
    "# train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=p.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "# Lists to keep track of progress\n",
    "losses = []\n",
    "num_epochs = 100\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader, 0):\n",
    "        data = data.to(device)\n",
    "        recon_data, latent_sample = model(data)\n",
    "        # loss\n",
    "        loss = _reconstruction_loss(data, recon_data, distribution=\"gaussian\", storer=None)\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # Update step\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data.item()\n",
    "\n",
    "        # Output training stats\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data.item()), end='')\n",
    "        \n",
    "\n",
    "    # Save Losses for plotting later\n",
    "    losses.append(epoch_loss/(batch_idx + 1))\n",
    "\n",
    "# SAVE MODEL\n",
    "# torch.save(model.state_dict(), 'results/autoenc_6layer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = iter(train_loader).next()[0].to(device)\n",
    "recon_data, latent_sample = model(im)\n",
    "viz_im_r(im[0,0], recon_data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss versus training iterations\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.plot(np.log(losses), label=\"epoch-loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_filters(model.conv1.weight[:,0,...], n_row=1, n_col=2, resize_fac=1, normalize=True, title='Conv Filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_filters(model.encoder(im)[0], n_row=1, n_col=2, normalize=True, resize_fac=1, title='Representation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
