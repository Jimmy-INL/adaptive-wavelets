{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "opj = os.path.join\n",
    "import pickle as pkl\n",
    "\n",
    "from ex_biology import p\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "from awd import awd\n",
    "from awd.mdata.biology import get_dataloader, load_pretrained_model\n",
    "from awd.awd.utils import get_wavefun, get_1dfilts\n",
    "from awd.awd.visualize import plot_1dfilts, plot_wavefun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.wave = 'db5'\n",
    "p.J = 4\n",
    "p.mode = 'zero'\n",
    "p.init_factor = 0\n",
    "p.noise_factor = 0.2\n",
    "p.const_factor = 0\n",
    "p.num_epochs = 600\n",
    "p.attr_methods = 'Saliency'\n",
    "\n",
    "lamWaveloss = 1\n",
    "p.lamlSum = lamWaveloss\n",
    "p.lamhSum = lamWaveloss\n",
    "p.lamL2sum = lamWaveloss\n",
    "p.lamCMF = lamWaveloss\n",
    "p.lamConv = lamWaveloss\n",
    "p.lamHighfreq = lamWaveloss\n",
    "p.lamL1wave = 0.0001\n",
    "p.lamL1attr = 0.5\n",
    "p.target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and model\n",
    "train_loader, test_loader = get_dataloader(p.data_path,\n",
    "                                           batch_size=p.batch_size,\n",
    "                                           is_continuous=p.is_continuous)\n",
    "\n",
    "model = load_pretrained_model(p.model_path, device=device)\n",
    "\n",
    "# prepare model\n",
    "random.seed(p.seed)\n",
    "np.random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "wt = awd.DWT1d(wave=p.wave, mode=p.mode, J=p.J,\n",
    "               init_factor=p.init_factor,\n",
    "               noise_factor=p.noise_factor,\n",
    "               const_factor=p.const_factor).to(device)\n",
    "wt.train()\n",
    "\n",
    "# train\n",
    "params = list(wt.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=p.lr)\n",
    "loss_f = awd.get_loss_f(lamlSum=p.lamlSum, lamhSum=p.lamhSum, lamL2norm=p.lamL2norm, lamCMF=p.lamCMF,\n",
    "                        lamConv=p.lamConv, lamL1wave=p.lamL1wave, lamL1attr=p.lamL1attr, lamHighfreq=p.lamHighfreq)\n",
    "trainer = awd.Trainer(model, wt, optimizer, loss_f, target=p.target,\n",
    "                      use_residuals=True, attr_methods=p.attr_methods, device=device, n_print=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Train Epoch: 0 [1044/2936 (97%)]\tLoss: 20.265581\n",
      "====> Epoch: 0 Average train loss: 31.6549\n",
      "Train Epoch: 5 [1044/2936 (97%)]\tLoss: 11.318017\n",
      "====> Epoch: 5 Average train loss: 11.4980\n",
      "Train Epoch: 10 [1044/2936 (97%)]\tLoss: 8.355836\n",
      "====> Epoch: 10 Average train loss: 8.3002\n",
      "Train Epoch: 15 [1044/2936 (97%)]\tLoss: 5.792733\n",
      "====> Epoch: 15 Average train loss: 6.0572\n",
      "Train Epoch: 20 [1044/2936 (97%)]\tLoss: 4.719848\n",
      "====> Epoch: 20 Average train loss: 4.5766\n",
      "Train Epoch: 25 [1044/2936 (97%)]\tLoss: 2.844547\n",
      "====> Epoch: 25 Average train loss: 3.0800\n",
      "Train Epoch: 30 [1044/2936 (97%)]\tLoss: 1.471580\n",
      "====> Epoch: 30 Average train loss: 1.6291\n",
      "Train Epoch: 35 [1044/2936 (97%)]\tLoss: 0.943736\n",
      "====> Epoch: 35 Average train loss: 0.9711\n",
      "Train Epoch: 40 [1044/2936 (97%)]\tLoss: 0.723615\n",
      "====> Epoch: 40 Average train loss: 0.7325\n",
      "Train Epoch: 45 [1044/2936 (97%)]\tLoss: 0.601572\n",
      "====> Epoch: 45 Average train loss: 0.5858\n",
      "Train Epoch: 50 [1044/2936 (97%)]\tLoss: 0.437424\n",
      "====> Epoch: 50 Average train loss: 0.4777\n",
      "Train Epoch: 55 [1044/2936 (97%)]\tLoss: 0.440134\n",
      "====> Epoch: 55 Average train loss: 0.4031\n",
      "Train Epoch: 60 [1044/2936 (97%)]\tLoss: 0.349732\n",
      "====> Epoch: 60 Average train loss: 0.3506\n",
      "Train Epoch: 65 [1044/2936 (97%)]\tLoss: 0.361863\n",
      "====> Epoch: 65 Average train loss: 0.3199\n",
      "Train Epoch: 70 [1044/2936 (97%)]\tLoss: 0.269718\n",
      "====> Epoch: 70 Average train loss: 0.2998\n",
      "Train Epoch: 75 [1044/2936 (97%)]\tLoss: 0.320865\n",
      "====> Epoch: 75 Average train loss: 0.2909\n",
      "Train Epoch: 80 [1044/2936 (97%)]\tLoss: 0.306363\n",
      "====> Epoch: 80 Average train loss: 0.2854\n",
      "Train Epoch: 85 [1044/2936 (97%)]\tLoss: 0.262242\n",
      "====> Epoch: 85 Average train loss: 0.2819\n",
      "Train Epoch: 90 [1044/2936 (97%)]\tLoss: 0.268676\n",
      "====> Epoch: 90 Average train loss: 0.2810\n",
      "Train Epoch: 95 [1044/2936 (97%)]\tLoss: 0.284955\n",
      "====> Epoch: 95 Average train loss: 0.2808\n",
      "Train Epoch: 100 [1044/2936 (97%)]\tLoss: 0.266488\n",
      "====> Epoch: 100 Average train loss: 0.2803\n",
      "Train Epoch: 105 [1044/2936 (97%)]\tLoss: 0.281307\n",
      "====> Epoch: 105 Average train loss: 0.2804\n",
      "Train Epoch: 110 [1044/2936 (97%)]\tLoss: 0.296067\n",
      "====> Epoch: 110 Average train loss: 0.2807\n",
      "Train Epoch: 115 [1044/2936 (97%)]\tLoss: 0.262784\n",
      "====> Epoch: 115 Average train loss: 0.2799\n",
      "Train Epoch: 120 [1044/2936 (97%)]\tLoss: 0.270556\n",
      "====> Epoch: 120 Average train loss: 0.2801\n",
      "Train Epoch: 125 [1044/2936 (97%)]\tLoss: 0.271877\n",
      "====> Epoch: 125 Average train loss: 0.2799\n",
      "Train Epoch: 130 [1044/2936 (97%)]\tLoss: 0.298872\n",
      "====> Epoch: 130 Average train loss: 0.2808\n",
      "Train Epoch: 135 [1044/2936 (97%)]\tLoss: 0.265019\n",
      "====> Epoch: 135 Average train loss: 0.2796\n",
      "Train Epoch: 140 [1044/2936 (97%)]\tLoss: 0.299155\n",
      "====> Epoch: 140 Average train loss: 0.2805\n",
      "Train Epoch: 145 [1044/2936 (97%)]\tLoss: 0.282895\n",
      "====> Epoch: 145 Average train loss: 0.2798\n",
      "Train Epoch: 150 [1044/2936 (97%)]\tLoss: 0.258660\n",
      "====> Epoch: 150 Average train loss: 0.2793\n",
      "Train Epoch: 155 [1044/2936 (97%)]\tLoss: 0.268892\n",
      "====> Epoch: 155 Average train loss: 0.2794\n",
      "Train Epoch: 160 [1044/2936 (97%)]\tLoss: 0.295509\n",
      "====> Epoch: 160 Average train loss: 0.2799\n",
      "Train Epoch: 165 [1044/2936 (97%)]\tLoss: 0.285278\n",
      "====> Epoch: 165 Average train loss: 0.2797\n",
      "Train Epoch: 170 [1044/2936 (97%)]\tLoss: 0.259760\n",
      "====> Epoch: 170 Average train loss: 0.2789\n",
      "Train Epoch: 175 [1044/2936 (97%)]\tLoss: 0.345622\n",
      "====> Epoch: 175 Average train loss: 0.2809\n",
      "Train Epoch: 180 [1044/2936 (97%)]\tLoss: 0.279095\n",
      "====> Epoch: 180 Average train loss: 0.2789\n",
      "Train Epoch: 185 [1044/2936 (97%)]\tLoss: 0.256984\n",
      "====> Epoch: 185 Average train loss: 0.2781\n",
      "Train Epoch: 190 [1044/2936 (97%)]\tLoss: 0.278172\n",
      "====> Epoch: 190 Average train loss: 0.2786\n",
      "Train Epoch: 195 [1044/2936 (97%)]\tLoss: 0.306484\n",
      "====> Epoch: 195 Average train loss: 0.2788\n",
      "Train Epoch: 200 [1044/2936 (97%)]\tLoss: 0.306165\n",
      "====> Epoch: 200 Average train loss: 0.2795\n",
      "Train Epoch: 205 [1044/2936 (97%)]\tLoss: 0.273065\n",
      "====> Epoch: 205 Average train loss: 0.2783\n",
      "Train Epoch: 210 [1044/2936 (97%)]\tLoss: 0.261476\n",
      "====> Epoch: 210 Average train loss: 0.2780\n",
      "Train Epoch: 215 [1044/2936 (97%)]\tLoss: 0.309829\n",
      "====> Epoch: 215 Average train loss: 0.2784\n",
      "Train Epoch: 220 [1044/2936 (97%)]\tLoss: 0.279557\n",
      "====> Epoch: 220 Average train loss: 0.2777\n",
      "Train Epoch: 225 [1044/2936 (97%)]\tLoss: 0.273513\n",
      "====> Epoch: 225 Average train loss: 0.2778\n",
      "Train Epoch: 230 [1044/2936 (97%)]\tLoss: 0.295351\n",
      "====> Epoch: 230 Average train loss: 0.2778\n",
      "Train Epoch: 235 [1044/2936 (97%)]\tLoss: 0.274484\n",
      "====> Epoch: 235 Average train loss: 0.2772\n",
      "Train Epoch: 240 [1044/2936 (97%)]\tLoss: 0.254675\n",
      "====> Epoch: 240 Average train loss: 0.2766\n",
      "Train Epoch: 245 [1044/2936 (97%)]\tLoss: 0.253362\n",
      "====> Epoch: 245 Average train loss: 0.2764\n",
      "Train Epoch: 250 [1044/2936 (97%)]\tLoss: 0.261508\n",
      "====> Epoch: 250 Average train loss: 0.2768\n",
      "Train Epoch: 255 [1044/2936 (97%)]\tLoss: 0.269991\n",
      "====> Epoch: 255 Average train loss: 0.2765\n",
      "Train Epoch: 260 [1044/2936 (97%)]\tLoss: 0.355519\n",
      "====> Epoch: 260 Average train loss: 0.2783\n",
      "Train Epoch: 265 [1044/2936 (97%)]\tLoss: 0.271219\n",
      "====> Epoch: 265 Average train loss: 0.2762\n",
      "Train Epoch: 270 [1044/2936 (97%)]\tLoss: 0.295042\n",
      "====> Epoch: 270 Average train loss: 0.2763\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "trainer(train_loader, epochs=p.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(trainer.train_losses))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"log train loss\")\n",
    "plt.title('Log-train loss vs epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('calculating losses and metric...')\n",
    "model.train()  # cudnn RNN backward can only be called in training mode\n",
    "validator = awd.Validator(model, test_loader)\n",
    "rec_loss, lsum_loss, hsum_loss, L2norm_loss, CMF_loss, conv_loss, L1wave_loss, L1saliency_loss, L1inputxgrad_loss = validator(\n",
    "    wt, target=p.target)\n",
    "print(\"Recon={:.5f}\\n lsum={:.5f}\\n hsum={:.5f}\\n L2norm={:.5f}\\n CMF={:.5f}\\n conv={:.5f}\\n L1wave={:.5f}\\n Saliency={:.5f}\\n Inputxgrad={:.5f}\\n\".format(rec_loss,\n",
    "                                      lsum_loss,\n",
    "                                      hsum_loss,\n",
    "                                      L2norm_loss,\n",
    "                                      CMF_loss,\n",
    "                                      conv_loss,\n",
    "                                      L1wave_loss,\n",
    "                                      L1saliency_loss,\n",
    "                                      L1inputxgrad_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = get_1dfilts(wt)\n",
    "phi, psi, x = get_wavefun(wt)\n",
    "\n",
    "plot_1dfilts(filt, is_title=True, figsize=(2,2))\n",
    "plot_wavefun((phi, psi, x), is_title=True, figsize=(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
