{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "opj = os.path.join\n",
    "import pickle as pkl\n",
    "\n",
    "from ex_biology import p\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "from awd import awd\n",
    "from awd.mdata.biology import get_dataloader, load_pretrained_model\n",
    "from awd.utils import get_wavefun, get_1dfilts\n",
    "from awd.visualize import plot_1dfilts, plot_wavefun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.wave = 'db5'\n",
    "p.J = 4\n",
    "p.mode = 'zero'\n",
    "p.init_factor = 0\n",
    "p.noise_factor = 0.2\n",
    "p.const_factor = 0\n",
    "p.num_epochs = 600\n",
    "p.attr_methods = 'Saliency'\n",
    "\n",
    "lamWaveloss = 1\n",
    "p.lamlSum = lamWaveloss\n",
    "p.lamhSum = lamWaveloss\n",
    "p.lamL2sum = lamWaveloss\n",
    "p.lamCMF = lamWaveloss\n",
    "p.lamConv = lamWaveloss\n",
    "p.lamHighfreq = 0\n",
    "p.lamL1wave = 0.0001\n",
    "p.lamL1attr = 0.1\n",
    "p.target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and model\n",
    "train_loader, test_loader = get_dataloader(p.data_path,\n",
    "                                           batch_size=p.batch_size,\n",
    "                                           is_continuous=p.is_continuous)\n",
    "\n",
    "model = load_pretrained_model(p.model_path, device=device)\n",
    "\n",
    "# prepare model\n",
    "random.seed(p.seed)\n",
    "np.random.seed(p.seed)\n",
    "torch.manual_seed(p.seed)\n",
    "\n",
    "wt = awd.DWT1d(wave=p.wave, mode=p.mode, J=p.J,\n",
    "               init_factor=p.init_factor,\n",
    "               noise_factor=p.noise_factor,\n",
    "               const_factor=p.const_factor).to(device)\n",
    "wt.train()\n",
    "\n",
    "# train\n",
    "params = list(wt.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=p.lr)\n",
    "loss_f = awd.get_loss_f(lamlSum=p.lamlSum, lamhSum=p.lamhSum, lamL2norm=p.lamL2norm, lamCMF=p.lamCMF,\n",
    "                        lamConv=p.lamConv, lamL1wave=p.lamL1wave, lamL1attr=p.lamL1attr, lamHighfreq=p.lamHighfreq)\n",
    "trainer = awd.Trainer(model, wt, optimizer, loss_f, target=p.target,\n",
    "                      use_residuals=True, attr_methods=p.attr_methods, device=device, n_print=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "Train Epoch: 0 [1044/2936 (97%)]\tLoss: 20.178627\n",
      "====> Epoch: 0 Average train loss: 31.5841\n",
      "Train Epoch: 5 [1044/2936 (97%)]\tLoss: 11.205288\n",
      "====> Epoch: 5 Average train loss: 11.3705\n",
      "Train Epoch: 10 [1044/2936 (97%)]\tLoss: 8.204903\n",
      "====> Epoch: 10 Average train loss: 8.1395\n",
      "Train Epoch: 15 [1044/2936 (97%)]\tLoss: 5.621610\n",
      "====> Epoch: 15 Average train loss: 5.8784\n",
      "Train Epoch: 20 [1044/2936 (97%)]\tLoss: 4.512144\n",
      "====> Epoch: 20 Average train loss: 4.3882\n",
      "Train Epoch: 25 [1044/2936 (97%)]\tLoss: 2.637395\n",
      "====> Epoch: 25 Average train loss: 2.8719\n",
      "Train Epoch: 30 [1044/2936 (97%)]\tLoss: 1.262867\n",
      "====> Epoch: 30 Average train loss: 1.4109\n",
      "Train Epoch: 35 [1044/2936 (97%)]\tLoss: 0.717810\n",
      "====> Epoch: 35 Average train loss: 0.7517\n",
      "Train Epoch: 40 [1044/2936 (97%)]\tLoss: 0.495922\n",
      "====> Epoch: 40 Average train loss: 0.5110\n",
      "Train Epoch: 45 [1044/2936 (97%)]\tLoss: 0.356926\n",
      "====> Epoch: 45 Average train loss: 0.3636\n",
      "Train Epoch: 50 [1044/2936 (97%)]\tLoss: 0.242366\n",
      "====> Epoch: 50 Average train loss: 0.2575\n",
      "Train Epoch: 55 [1044/2936 (97%)]\tLoss: 0.185034\n",
      "====> Epoch: 55 Average train loss: 0.1825\n",
      "Train Epoch: 60 [1044/2936 (97%)]\tLoss: 0.127609\n",
      "====> Epoch: 60 Average train loss: 0.1310\n",
      "Train Epoch: 65 [1044/2936 (97%)]\tLoss: 0.104828\n",
      "====> Epoch: 65 Average train loss: 0.0983\n",
      "Train Epoch: 70 [1044/2936 (97%)]\tLoss: 0.071297\n",
      "====> Epoch: 70 Average train loss: 0.0783\n",
      "Train Epoch: 75 [1044/2936 (97%)]\tLoss: 0.073007\n",
      "====> Epoch: 75 Average train loss: 0.0675\n",
      "Train Epoch: 80 [1044/2936 (97%)]\tLoss: 0.065626\n",
      "====> Epoch: 80 Average train loss: 0.0617\n",
      "Train Epoch: 85 [1044/2936 (97%)]\tLoss: 0.054605\n",
      "====> Epoch: 85 Average train loss: 0.0587\n",
      "Train Epoch: 90 [1044/2936 (97%)]\tLoss: 0.054926\n",
      "====> Epoch: 90 Average train loss: 0.0575\n",
      "Train Epoch: 95 [1044/2936 (97%)]\tLoss: 0.057808\n",
      "====> Epoch: 95 Average train loss: 0.0570\n",
      "Train Epoch: 100 [1044/2936 (97%)]\tLoss: 0.054028\n",
      "====> Epoch: 100 Average train loss: 0.0567\n",
      "Train Epoch: 105 [1044/2936 (97%)]\tLoss: 0.056870\n",
      "====> Epoch: 105 Average train loss: 0.0567\n",
      "Train Epoch: 110 [1044/2936 (97%)]\tLoss: 0.059838\n",
      "====> Epoch: 110 Average train loss: 0.0568\n",
      "Train Epoch: 115 [1044/2936 (97%)]\tLoss: 0.053222\n",
      "====> Epoch: 115 Average train loss: 0.0566\n",
      "Train Epoch: 120 [1044/2936 (97%)]\tLoss: 0.054747\n",
      "====> Epoch: 120 Average train loss: 0.0566\n",
      "Train Epoch: 125 [1044/2936 (97%)]\tLoss: 0.055059\n",
      "====> Epoch: 125 Average train loss: 0.0566\n",
      "Train Epoch: 130 [1044/2936 (97%)]\tLoss: 0.060274\n",
      "====> Epoch: 130 Average train loss: 0.0567\n",
      "Train Epoch: 135 [1044/2936 (97%)]\tLoss: 0.053685\n",
      "====> Epoch: 135 Average train loss: 0.0566\n",
      "Train Epoch: 140 [1044/2936 (97%)]\tLoss: 0.060293\n",
      "====> Epoch: 140 Average train loss: 0.0567\n",
      "Train Epoch: 145 [1044/2936 (97%)]\tLoss: 0.057304\n",
      "====> Epoch: 145 Average train loss: 0.0567\n",
      "Train Epoch: 150 [1044/2936 (97%)]\tLoss: 0.052493\n",
      "====> Epoch: 150 Average train loss: 0.0566\n",
      "Train Epoch: 155 [1044/2936 (97%)]\tLoss: 0.054501\n",
      "====> Epoch: 155 Average train loss: 0.0566\n",
      "Train Epoch: 160 [1044/2936 (97%)]\tLoss: 0.059919\n",
      "====> Epoch: 160 Average train loss: 0.0567\n",
      "Train Epoch: 165 [1044/2936 (97%)]\tLoss: 0.057483\n",
      "====> Epoch: 165 Average train loss: 0.0566\n",
      "Train Epoch: 170 [1044/2936 (97%)]\tLoss: 0.052758\n",
      "====> Epoch: 170 Average train loss: 0.0565\n",
      "Train Epoch: 175 [1044/2936 (97%)]\tLoss: 0.069905\n",
      "====> Epoch: 175 Average train loss: 0.0569\n",
      "Train Epoch: 180 [1044/2936 (97%)]\tLoss: 0.056692\n",
      "====> Epoch: 180 Average train loss: 0.0566\n",
      "Train Epoch: 185 [1044/2936 (97%)]\tLoss: 0.052277\n",
      "====> Epoch: 185 Average train loss: 0.0565\n",
      "Train Epoch: 190 [1044/2936 (97%)]\tLoss: 0.056407\n",
      "====> Epoch: 190 Average train loss: 0.0566\n",
      "Train Epoch: 195 [1044/2936 (97%)]\tLoss: 0.062307\n",
      "====> Epoch: 195 Average train loss: 0.0568\n",
      "Train Epoch: 200 [1044/2936 (97%)]\tLoss: 0.062199\n",
      "====> Epoch: 200 Average train loss: 0.0567\n",
      "Train Epoch: 205 [1044/2936 (97%)]\tLoss: 0.055507\n",
      "====> Epoch: 205 Average train loss: 0.0565\n",
      "Train Epoch: 210 [1044/2936 (97%)]\tLoss: 0.053333\n",
      "====> Epoch: 210 Average train loss: 0.0565\n",
      "Train Epoch: 215 [1044/2936 (97%)]\tLoss: 0.063316\n",
      "====> Epoch: 215 Average train loss: 0.0567\n",
      "Train Epoch: 220 [1044/2936 (97%)]\tLoss: 0.057015\n",
      "====> Epoch: 220 Average train loss: 0.0566\n",
      "Train Epoch: 225 [1044/2936 (97%)]\tLoss: 0.055309\n",
      "====> Epoch: 225 Average train loss: 0.0565\n",
      "Train Epoch: 230 [1044/2936 (97%)]\tLoss: 0.060164\n",
      "====> Epoch: 230 Average train loss: 0.0566\n",
      "Train Epoch: 235 [1044/2936 (97%)]\tLoss: 0.055692\n",
      "====> Epoch: 235 Average train loss: 0.0565\n",
      "Train Epoch: 240 [1044/2936 (97%)]\tLoss: 0.052074\n",
      "====> Epoch: 240 Average train loss: 0.0564\n",
      "Train Epoch: 245 [1044/2936 (97%)]\tLoss: 0.051850\n",
      "====> Epoch: 245 Average train loss: 0.0564\n",
      "Train Epoch: 250 [1044/2936 (97%)]\tLoss: 0.053436\n",
      "====> Epoch: 250 Average train loss: 0.0564\n",
      "Train Epoch: 255 [1044/2936 (97%)]\tLoss: 0.055167\n",
      "====> Epoch: 255 Average train loss: 0.0564\n",
      "Train Epoch: 260 [1044/2936 (97%)]\tLoss: 0.072459\n",
      "====> Epoch: 260 Average train loss: 0.0568\n",
      "Train Epoch: 265 [1044/2936 (97%)]\tLoss: 0.055398\n",
      "====> Epoch: 265 Average train loss: 0.0564\n",
      "Train Epoch: 270 [1044/2936 (97%)]\tLoss: 0.060468\n",
      "====> Epoch: 270 Average train loss: 0.0565\n",
      "Train Epoch: 275 [1044/2936 (97%)]\tLoss: 0.051915\n",
      "====> Epoch: 275 Average train loss: 0.0563\n",
      "Train Epoch: 280 [1044/2936 (97%)]\tLoss: 0.065014\n",
      "====> Epoch: 280 Average train loss: 0.0566\n",
      "Train Epoch: 285 [1044/2936 (97%)]\tLoss: 0.057049\n",
      "====> Epoch: 285 Average train loss: 0.0563\n",
      "Train Epoch: 290 [1044/2936 (97%)]\tLoss: 0.063482\n",
      "====> Epoch: 290 Average train loss: 0.0565\n",
      "Train Epoch: 295 [1044/2936 (97%)]\tLoss: 0.063179\n",
      "====> Epoch: 295 Average train loss: 0.0564\n",
      "Train Epoch: 300 [1044/2936 (97%)]\tLoss: 0.054797\n",
      "====> Epoch: 300 Average train loss: 0.0562\n",
      "Train Epoch: 305 [1044/2936 (97%)]\tLoss: 0.052418\n",
      "====> Epoch: 305 Average train loss: 0.0565\n",
      "Train Epoch: 310 [1044/2936 (97%)]\tLoss: 0.062108\n",
      "====> Epoch: 310 Average train loss: 0.0563\n",
      "Train Epoch: 315 [1044/2936 (97%)]\tLoss: 0.056380\n",
      "====> Epoch: 315 Average train loss: 0.0563\n",
      "Train Epoch: 320 [1044/2936 (97%)]\tLoss: 0.059552\n",
      "====> Epoch: 320 Average train loss: 0.0562\n",
      "Train Epoch: 325 [1044/2936 (97%)]\tLoss: 0.054938\n",
      "====> Epoch: 325 Average train loss: 0.0561\n",
      "Train Epoch: 330 [1044/2936 (97%)]\tLoss: 0.056265\n",
      "====> Epoch: 330 Average train loss: 0.0562\n",
      "Train Epoch: 335 [1044/2936 (97%)]\tLoss: 0.050498\n",
      "====> Epoch: 335 Average train loss: 0.0561\n",
      "Train Epoch: 340 [1044/2936 (97%)]\tLoss: 0.058216\n",
      "====> Epoch: 340 Average train loss: 0.0564\n",
      "Train Epoch: 345 [1044/2936 (97%)]\tLoss: 0.054674\n",
      "====> Epoch: 345 Average train loss: 0.0561\n",
      "Train Epoch: 350 [1044/2936 (97%)]\tLoss: 0.047031\n",
      "====> Epoch: 350 Average train loss: 0.0560\n",
      "Train Epoch: 355 [1044/2936 (97%)]\tLoss: 0.049515\n",
      "====> Epoch: 355 Average train loss: 0.0559\n",
      "Train Epoch: 360 [1044/2936 (97%)]\tLoss: 0.051970\n",
      "====> Epoch: 360 Average train loss: 0.0561\n",
      "Train Epoch: 365 [1044/2936 (97%)]\tLoss: 0.059619\n",
      "====> Epoch: 365 Average train loss: 0.0561\n",
      "Train Epoch: 370 [1044/2936 (97%)]\tLoss: 0.052899\n",
      "====> Epoch: 370 Average train loss: 0.0560\n",
      "Train Epoch: 375 [1044/2936 (97%)]\tLoss: 0.056110\n",
      "====> Epoch: 375 Average train loss: 0.0560\n",
      "Train Epoch: 380 [1044/2936 (97%)]\tLoss: 0.058672\n",
      "====> Epoch: 380 Average train loss: 0.0562\n",
      "Train Epoch: 385 [1044/2936 (97%)]\tLoss: 0.052224\n",
      "====> Epoch: 385 Average train loss: 0.0559\n",
      "Train Epoch: 390 [1044/2936 (97%)]\tLoss: 0.068901\n",
      "====> Epoch: 390 Average train loss: 0.0562\n",
      "Train Epoch: 395 [1044/2936 (97%)]\tLoss: 0.048850\n",
      "====> Epoch: 395 Average train loss: 0.0558\n",
      "Train Epoch: 400 [1044/2936 (97%)]\tLoss: 0.053730\n",
      "====> Epoch: 400 Average train loss: 0.0560\n",
      "Train Epoch: 405 [1044/2936 (97%)]\tLoss: 0.053109\n",
      "====> Epoch: 405 Average train loss: 0.0559\n",
      "Train Epoch: 410 [1044/2936 (97%)]\tLoss: 0.060125\n",
      "====> Epoch: 410 Average train loss: 0.0560\n",
      "Train Epoch: 415 [1044/2936 (97%)]\tLoss: 0.062623\n",
      "====> Epoch: 415 Average train loss: 0.0560\n",
      "Train Epoch: 420 [1044/2936 (97%)]\tLoss: 0.059036\n",
      "====> Epoch: 420 Average train loss: 0.0560\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "trainer(train_loader, epochs=p.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(trainer.train_losses))\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"log train loss\")\n",
    "plt.title('Log-train loss vs epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('calculating losses and metric...')\n",
    "model.train()  # cudnn RNN backward can only be called in training mode\n",
    "validator = awd.Validator(model, test_loader)\n",
    "rec_loss, lsum_loss, hsum_loss, L2norm_loss, CMF_loss, conv_loss, L1wave_loss, L1saliency_loss, L1inputxgrad_loss = validator(\n",
    "    wt, target=p.target)\n",
    "print(\"Recon={:.5f}\\n lsum={:.5f}\\n hsum={:.5f}\\n L2norm={:.5f}\\n CMF={:.5f}\\n conv={:.5f}\\n L1wave={:.5f}\\n Saliency={:.5f}\\n Inputxgrad={:.5f}\\n\".format(rec_loss,\n",
    "                                      lsum_loss,\n",
    "                                      hsum_loss,\n",
    "                                      L2norm_loss,\n",
    "                                      CMF_loss,\n",
    "                                      conv_loss,\n",
    "                                      L1wave_loss,\n",
    "                                      L1saliency_loss,\n",
    "                                      L1inputxgrad_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = get_1dfilts(wt)\n",
    "phi, psi, x = get_wavefun(wt)\n",
    "\n",
    "plot_1dfilts(filt, is_title=True, figsize=(2,2))\n",
    "plot_wavefun((phi, psi, x), is_title=True, figsize=(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}