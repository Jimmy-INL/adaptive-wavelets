{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "from ex_biology import p\n",
    "from dset import get_dataloader, load_pretrained_model\n",
    "\n",
    "# adaptive-wavelets modules\n",
    "from losses import get_loss_f\n",
    "from train import Trainer\n",
    "from evaluate import Validator\n",
    "from transform1d import DWT1d\n",
    "from utils import get_1dfilts, get_wavefun\n",
    "from wave_attributions import Attributer\n",
    "from visualize import cshow, plot_1dfilts, plot_1dreconstruct, plot_wavefun\n",
    "\n",
    "# evaluation\n",
    "from matplotlib import gridspec\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from feature_transform import max_transformer\n",
    "sys.path.append('../../lib/trim')\n",
    "from trim import TrimModel\n",
    "from utils import tuple_to_tensor, tensor_to_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"db5_saliency_warmstart_seed=1\"]\n",
    "results = []\n",
    "models = []\n",
    "for i in range(len(dirs)):\n",
    "    # load results\n",
    "    out_dir = opj(\"/home/ubuntu/adaptive-wavelets/notebooks/biology/results\", dirs[i])\n",
    "    fnames = sorted(os.listdir(out_dir))\n",
    "    \n",
    "    results_list = []\n",
    "    models_list = []\n",
    "    for fname in fnames:\n",
    "        if fname[-3:] == 'pkl':\n",
    "            results_list.append(pkl.load(open(opj(out_dir, fname), 'rb')))\n",
    "        if fname[-3:] == 'pth':\n",
    "            wt = DWT1d(wave=\"db5\", mode='zero', J=4, init_factor=1, noise_factor=0.0).to(device)\n",
    "            wt.load_state_dict(torch.load(opj(out_dir, fname)))\n",
    "            models_list.append(wt)\n",
    "    results.append(pd.DataFrame(results_list))\n",
    "    models.append(models_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dics = []\n",
    "for i in range(len(dirs)):\n",
    "    # define indexes\n",
    "    res = results[i]\n",
    "    mos = models[i]\n",
    "    lamL1wave = np.array(res['lamL1wave'])\n",
    "    lamL1attr = np.array(res['lamL1attr'])\n",
    "    lamL1wave_grid = np.unique(lamL1wave)\n",
    "    lamL1attr_grid = np.unique(lamL1attr)\n",
    "    R = len(lamL1wave_grid)\n",
    "    C = len(lamL1attr_grid)\n",
    "\n",
    "    # collect results\n",
    "    dic = {'psi':{},\n",
    "           'wt': {},\n",
    "           'x': {},\n",
    "           'lamL1wave': {},\n",
    "           'lamL1attr': {},\n",
    "           'index': {}}\n",
    "\n",
    "    for r in range(R):\n",
    "        for c in range(C):\n",
    "            loc = (lamL1wave == lamL1wave_grid[r]) & (lamL1attr == lamL1attr_grid[c])\n",
    "            if loc.sum() == 1: \n",
    "                loc = np.argwhere(loc).flatten()[0]\n",
    "                dic['index'][(r,c)] = loc\n",
    "                wt = mos[loc]\n",
    "                _, psi, x = get_wavefun(wt)\n",
    "\n",
    "                dic['wt'][(r,c)] = wt\n",
    "                dic['psi'][(r,c)] = psi  \n",
    "                dic['x'][(r,c)] = x\n",
    "                dic['lamL1wave'][(r,c)] = lamL1wave_grid[r]\n",
    "                dic['lamL1attr'][(r,c)] = lamL1attr_grid[c]\n",
    "    dics.append(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5677446126937866 0.7035102248191833\n"
     ]
    }
   ],
   "source": [
    "# optimal AWD\n",
    "idx1, idx2 = 4, 11\n",
    "idx = dics[0]['index'][(idx1, idx2)]\n",
    "wt = models[0][idx].to(device)\n",
    "\n",
    "# standard wavelet\n",
    "wt_o = DWT1d(wave='db5', mode='zero', J=4, init_factor=1, noise_factor=0).to(device)\n",
    "\n",
    "# load data and pre-trained model\n",
    "(train_loader, test_loader) = get_dataloader(p.data_path, \n",
    "                                             batch_size=p.batch_size,\n",
    "                                             is_continuous=False)   \n",
    "\n",
    "model = load_pretrained_model(p.model_path, device=device)  \n",
    "\n",
    "# define trim model\n",
    "mt = TrimModel(model, wt.inverse, use_residuals=True)    \n",
    "mt_o = TrimModel(model, wt_o.inverse, use_residuals=True)  \n",
    "attributer = Attributer(mt, attr_methods='Saliency', device='cuda')\n",
    "attributer_o = Attributer(mt_o, attr_methods='Saliency', device='cuda')\n",
    "\n",
    "# compute compression rate and representations\n",
    "attrs = {'AWD': torch.tensor([]).to(device),\n",
    "         'DB5': torch.tensor([]).to(device)}\n",
    "reps = {'AWD': torch.tensor([]).to(device),\n",
    "        'DB5': torch.tensor([]).to(device)}\n",
    "\n",
    "for data, _ in test_loader:\n",
    "    data = data.to(device)\n",
    "    i = 0\n",
    "    for w in [wt, wt_o]:\n",
    "        if i == 0:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['AWD'] = torch.cat((reps['AWD'], y), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['AWD'] = torch.cat((attrs['AWD'], z), dim=0)\n",
    "        else:\n",
    "            data_t = w(data)\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                attributions = attributer_o(data_t, target=0, additional_forward_args=deepcopy(data))   \n",
    "            y, _ = tuple_to_tensor(data_t)\n",
    "            reps['DB5'] = torch.cat((reps['DB5'], y), dim=0)\n",
    "            z, _ = tuple_to_tensor(attributions)\n",
    "            attrs['DB5'] = torch.cat((attrs['DB5'], z), dim=0)\n",
    "        i += 1\n",
    "reps['AWD'] = reps['AWD'].reshape(-1)\n",
    "reps['DB5'] = reps['DB5'].reshape(-1) \n",
    "attrs['AWD'] = attrs['AWD'].reshape(-1)\n",
    "attrs['DB5'] = attrs['DB5'].reshape(-1) \n",
    "\n",
    "thresh1 = 1e-3\n",
    "thresh2 = 1e-3\n",
    "c_rate_AWD = 1.0*((abs(reps['AWD']) > thresh1) & (abs(attrs['AWD']) > thresh2)).sum() / reps['AWD'].shape[0]\n",
    "c_rate_DB5 = 1.0*((abs(reps['DB5']) > thresh1) & (abs(attrs['DB5']) > thresh2)).sum() / reps['DB5'].shape[0]\n",
    "print(c_rate_AWD.item(), c_rate_DB5.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
