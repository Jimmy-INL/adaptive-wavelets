{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os,sys\n",
    "opj = os.path.join\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "from model_mnist import LeNet5\n",
    "from visualize import *\n",
    "import dset_mnist as dset\n",
    "import foolbox\n",
    "sys.path.append('../trim')\n",
    "from transforms_torch import transform_bandpass, tensor_t_augment, batch_fftshift2d, batch_ifftshift2d\n",
    "from trim import *\n",
    "from util import *\n",
    "from attributions import *\n",
    "from captum.attr import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# disentangled vae\n",
    "sys.path.append('../disentangling-vae')\n",
    "from collections import defaultdict\n",
    "import vae_trim, vae_trim_viz\n",
    "from disvae.utils.modelIO import save_model, load_model, load_metadata\n",
    "from disvae.models.losses import get_loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 6 # total correlation reg\n",
    "args.attr_lamb = 0 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: 47.867859\n",
      "Average loss: 45.49810778001732 - Average reg: 0\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: 41.915894\n",
      "Average loss: 12.368510833935442 - Average reg: 0\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: 44.490677\n",
      "Average loss: 7.7154602180920175 - Average reg: 0\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: 32.978821\n",
      "Average loss: 4.973543276918977 - Average reg: 0\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: 38.364502\n",
      "Average loss: 3.2386133157368153 - Average reg: 0\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()    \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.056267093747918 0.582081064105288\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 18 # total correlation reg\n",
    "args.attr_lamb = 0 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: -330.127533\n",
      "Average loss: -386.7244673608971 - Average reg: 0\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: -300.014130\n",
      "Average loss: -416.68307049391365 - Average reg: 0\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: -305.318176\n",
      "Average loss: -421.13327055648443 - Average reg: 0\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: -284.912109\n",
      "Average loss: -423.2155005936938 - Average reg: 0\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: -310.272949\n",
      "Average loss: -424.6122833772509 - Average reg: 0\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()    \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-424.3654141293914 0.2639923615019713\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTCVAE-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disvae import init_specific_model\n",
    "from utils.datasets import get_img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = vae_trim.parse_arguments()\n",
    "args.loss = \"btcvae\"\n",
    "args.reg_anneal = 0\n",
    "args.btcvae_B = 50 # total correlation reg\n",
    "args.attr_lamb = 0 # change of attribute wrt other attributes\n",
    "name = args.loss + \"_B_\" + str(args.btcvae_B) + \"_attr_\" + str(args.attr_lamb)\n",
    "args.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloaders\n",
    "train_loader, test_loader = dset.load_data(args.batch_size, args.eval_batchsize, device)\n",
    "\n",
    "# initialize model\n",
    "args.img_size = get_img_size(args.dataset)\n",
    "model = init_specific_model(args.model_type, args.img_size, args.latent_dim).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# loss\n",
    "L1Loss = torch.nn.L1Loss()\n",
    "loss_f = get_loss_f(args.loss,\n",
    "                    n_data=len(train_loader.dataset),\n",
    "                    device=device,\n",
    "                    **vars(args))\n",
    "\n",
    "m = DecoderEncoder(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [29984/60000 (100%)]\tLoss: -1276.518188\n",
      "Average loss: -1568.6629865113607 - Average reg: 0\n",
      "Train Epoch: 1 [29984/60000 (100%)]\tLoss: -1270.752563\n",
      "Average loss: -1592.5403731291228 - Average reg: 0\n",
      "Train Epoch: 2 [29984/60000 (100%)]\tLoss: -1269.401367\n",
      "Average loss: -1596.2460808662463 - Average reg: 0\n",
      "Train Epoch: 3 [29984/60000 (100%)]\tLoss: -1275.217773\n",
      "Average loss: -1598.9513557613022 - Average reg: 0\n",
      "Train Epoch: 4 [29984/60000 (100%)]\tLoss: -1305.985474\n",
      "Average loss: -1601.7783611761226 - Average reg: 0\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    epoch_reg = 0\n",
    "    # one epoch\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        batch_size, channel, height, width = inputs.size()\n",
    "        recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "        loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                           None, latent_sample=latent_sample)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        epoch_loss += loss.item()    \n",
    "        print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss.data.item()), end='')        \n",
    "    mean_epoch_loss = epoch_loss / len(train_loader)\n",
    "    mean_epoch_reg = epoch_reg / len(train_loader)\n",
    "    print('\\nAverage loss: {} - Average reg: {}'.format(mean_epoch_loss, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1596.7284784052672 0.2097970805982791\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "epoch_reg = 0\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):     \n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)    \n",
    "    batch_size, channel, height, width = inputs.size()\n",
    "    recon_batch, latent_dist, latent_sample = model(inputs)\n",
    "\n",
    "    loss = loss_f(inputs, recon_batch, latent_dist, model.training,\n",
    "                       None, latent_sample=latent_sample)          \n",
    "    # penalize change in one attribute wrt the other attributes\n",
    "    s = deepcopy(latent_dist[0].detach())\n",
    "    s = s.requires_grad_(True)\n",
    "    s_output = m(s)\n",
    "    attr_reg = 0\n",
    "    for i in range(model.latent_dim):\n",
    "        col_idx = np.arange(model.latent_dim)!=i\n",
    "        gradients = torch.autograd.grad(s_output[:,i], s, grad_outputs=torch.ones_like(s_output[:,i]), \n",
    "                                        retain_graph=True, create_graph=True, only_inputs=True)[0]\n",
    "        gradient_pairwise = gradients[:,col_idx]\n",
    "        attr_reg += L1Loss(gradient_pairwise, torch.zeros_like(gradient_pairwise))\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_reg += attr_reg.item()  \n",
    "print(epoch_loss/len(train_loader), epoch_reg/len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
