{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pickle as pkl\n",
    "\n",
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import transform_wrappers\n",
    "from tqdm import tqdm\n",
    "import acd\n",
    "from copy import deepcopy\n",
    "sys.path.append('..')\n",
    "device = 'cuda'\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5000, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 80)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(80, 2)\n",
    "        self.use_softmax = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        if self.use_softmax:\n",
    "            x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = './datasets'\n",
    "\n",
    "# Import `fake_or_real_news.csv`\n",
    "df = pd.read_csv(data_dir_path + \"/train.csv\")\n",
    "\n",
    "# Set `y`\n",
    "X = df['text']\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "df.head()\n",
    "\n",
    "# must use raw term counts for LDA because it is a probabilistic graphical model\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')\n",
    "\n",
    "# fit and transform the training data into bag of words\n",
    "bag_of_words_train = count_vectorizer.fit_transform(X_train.astype('U'))\n",
    "bag_of_words_test = count_vectorizer.transform(X_test.astype('U'))\n",
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run/load lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "Topic 0: said percent new year company million 000 money years companies...\n",
      "Topic 1: mr said trump president ms court new mrs campaign house...\n",
      "Topic 2: trump president obama donald people house election said party white...\n",
      "Topic 3: said police people state city syria attack officers killed military...\n",
      "Topic 4: news twitter com 2016 media facebook 2017 breitbart video на...\n",
      "Topic 5: people world black israel political state women war america students...\n",
      "Topic 6: like just people time don know way life make good...\n",
      "Topic 7: russia united states government russian war foreign china president military...\n",
      "Topic 8: said mr ms new la like york year city years...\n",
      "Topic 9: clinton hillary election campaign fbi trump emails investigation comey email...\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx), end=' ')\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]) + '...')\n",
    "\n",
    "\n",
    "\n",
    "# Run LDA\n",
    "num_topics = 10\n",
    "# lda = LatentDirichletAllocation(n_components=num_topics, random_state=42).fit(bag_of_words_train)\n",
    "# pkl.dump(lda, open('lda_10.pkl', 'wb'))\n",
    "lda = pkl.load(open('lda_10.pkl', 'rb'))\n",
    "\n",
    "# display        \n",
    "num_top_words = 10\n",
    "display_topics(lda, feature_names, num_top_words)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embeddings\n",
    "Previously was using from doc2vec to embed each document as a 300-dim vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print('getting doc2vec embdeddings...')\n",
    "xtr, xte, ytr, yte = getEmbeddings(\"datasets/train.csv\")\n",
    "np.save('./xtr', xtr)\n",
    "np.save('./xte', xte)\n",
    "np.save('./ytr', ytr)\n",
    "np.save('./yte', yte)\n",
    "\n",
    "# prepare data\n",
    "xtr = np.load('./xtr.npy').astype(np.float32)\n",
    "xte = np.load('./xte.npy').astype(np.float32)\n",
    "ytr = np.load('./ytr.npy')\n",
    "yte = np.load('./yte.npy')\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train))).astype(np.int64)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test))).astype(np.int64)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6851\u001b[0m  5.8618\n",
      "      2        \u001b[36m0.6314\u001b[0m  5.8392\n",
      "      3        \u001b[36m0.5183\u001b[0m  5.9091\n",
      "      4        \u001b[36m0.4159\u001b[0m  5.8590\n",
      "      5        \u001b[36m0.3370\u001b[0m  5.8460\n",
      "      6        \u001b[36m0.2740\u001b[0m  5.8234\n",
      "      7        \u001b[36m0.2240\u001b[0m  5.8654\n",
      "      8        \u001b[36m0.1848\u001b[0m  5.8531\n",
      "      9        \u001b[36m0.1551\u001b[0m  5.8403\n",
      "     10        \u001b[36m0.1319\u001b[0m  6.1565\n",
      "Model Trained!\n"
     ]
    }
   ],
   "source": [
    "# # fit model (only takes about a minute on gpu)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# net = NeuralNetClassifier(\n",
    "#     FNN,\n",
    "#     max_epochs=10,\n",
    "#     lr=0.01,\n",
    "#     # Shuffle training data on each epoch\n",
    "#     iterator_train__shuffle=True,\n",
    "#     train_split=None,\n",
    "# )\n",
    "# net.fit(bag_of_words_train.astype(np.float32), y_train)\n",
    "# pkl.dump(net, open('model.pkl', 'wb'))\n",
    "# print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load + interpret the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.948\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "net = pkl.load(open('model.pkl', 'rb'))\n",
    "probabs = net.predict_proba(bag_of_words_test.astype(np.float32))\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(f\"acc: {acc:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=5000, out_features=256, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=256, out_features=80, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=80, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fnn = net.module_\n",
    "fnn.use_softmax = False\n",
    "print(fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "D = lda.components_\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transform_wrappers.lay_from_w(D)\n",
    "# norm = transform_wrappers.NormLayer(mu=0.1307, std=0.3081)\n",
    "# reshape = transform_wrappers.ReshapeLayer(shape=(1, 28, 28))\n",
    "net = transform_wrappers.Net_with_transform(fnn, transform=transform).to(device)\n",
    "#                                             reshape=reshape,\n",
    "#                                             use_logits=True).to(device)\n",
    "# print(list(net.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_test = lda.transform(bag_of_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(topics_test[0:1]).to(device)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4160/4160 [03:02<00:00, 22.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def calc_scores(topics, net):\n",
    "    results = {\n",
    "        'scores': [],\n",
    "        'logits': [],\n",
    "        'class_pred': []\n",
    "    }\n",
    "    for i in tqdm(range(topics.shape[0])): # 10000 pts total\n",
    "        x_t = topics[i: i + 1]\n",
    "        x_t_tensor = torch.Tensor(x_t).to(device)\n",
    "\n",
    "        pred = net(deepcopy(x_t_tensor)).cpu().detach().numpy()\n",
    "        results['logits'].append(deepcopy(pred))\n",
    "        results['class_pred'].append(np.argmax(pred))\n",
    "        # x.requires_grad = False\n",
    "        sweep_dim = 1\n",
    "        tiles = acd.tiling_2d.gen_tiles(x_t, fill=0, method='cd', sweep_dim=sweep_dim)\n",
    "        cd_scores_im = acd.get_scores_2d(net, method='cd', ims=tiles, im_torch=x_t_tensor, device='cuda')\n",
    "        results['scores'].append(deepcopy(cd_scores_im))\n",
    "\n",
    "        if i == 100 or i % 1000 == 0:\n",
    "            # \n",
    "            pkl.dump(results, open(f'lda_scores_{i}.pkl', 'wb'))\n",
    "    pkl.dump(results, open(f'lda_scores_{i}.pkl', 'wb'))\n",
    "calc_scores(topics_test, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores across bases\n",
    "results = pkl.load(open(f'lda_scores_{4159}.pkl', 'rb'))\n",
    "scores = np.array(results['scores']) # (num points, num_bases, num_classes)\n",
    "logits = np.array(results['logits']) # (num points, 1, num_classes)\n",
    "class_preds = np.array(results['class_pred']) # (num points, )\n",
    "titles = []\n",
    "scores_all = np.mean(scores, axis=0)\n",
    "titles = np.argmax(scores, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean contribution to \"fake\" class for each topic for points which were correctly predicted as fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic\t&\tMean contribution\t&\tTop words for topic \\\\\n",
      "1\t&\t-121.5\t&\tsaid percent new year company million 000 money years companies\\\\\n",
      "2\t&\t-111.8\t&\tmr said trump president ms court new mrs campaign house\\\\\n",
      "3\t&\t-205.4\t&\ttrump president obama donald people house election said party white\\\\\n",
      "4\t&\t-62.3\t&\tsaid police people state city syria attack officers killed military\\\\\n",
      "5\t&\t59.9\t&\tnews twitter com 2016 media facebook 2017 breitbart video на\\\\\n",
      "6\t&\t87.1\t&\tpeople world black israel political state women war america students\\\\\n",
      "7\t&\t443.6\t&\tlike just people time don know way life make good\\\\\n",
      "8\t&\t-29.5\t&\trussia united states government russian war foreign china president military\\\\\n",
      "9\t&\t-95.4\t&\tsaid mr ms new la like york year city years\\\\\n",
      "10\t&\t1199.6\t&\tclinton hillary election campaign fbi trump emails investigation comey email\\\\\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "# idxs = np.ones(y_test.shape).astype(np.int) # look at all points\n",
    "idxs = np.logical_and(y_test == 1, class_preds == 1)\n",
    "print('Topic\\t&\\tMean contribution\\t&\\tTop words for topic \\\\\\\\')\n",
    "for i in range(10):\n",
    "    topic = lda.components_[i]\n",
    "    words = \" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]])\n",
    "    print(f\"{i + 1}\\t&\\t{np.mean(scores[:, i, 1][idxs]):0.1f}\\t&\\t{words}\\\\\\\\\")\n",
    "#     hist = plt.hist(scores[:, i, 1][idxs], alpha=0.5, label=str(i), bins=100)    \n",
    "#     hist = plt.hist(scores[:, i, 1][idxs] / logits[idxs, 0, 1], alpha=1, label=str(i), bins=100)    \n",
    "#     plt.yscale('log')\n",
    "#     plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
