{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = './datasets'\n",
    "\n",
    "# Import `fake_or_real_news.csv`\n",
    "df = pd.read_csv(data_dir_path + \"/train.csv\")\n",
    "\n",
    "# Set `y`\n",
    "X_train = df['text']\n",
    "y = df.label\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "count_train = count_vectorizer.fit_transform(X_train.astype('U'))\n",
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "Topic 0: said like just time people new years don life way\n",
      "Topic 1: trump president clinton donald said election people party campaign republican\n",
      "Topic 2: school students clinton university state million foundation public money education\n",
      "Topic 3: people percent world like new years government money economic year\n",
      "Topic 4: russia war military syria united russian government states american world\n",
      "Topic 5: said law court federal health state new states care immigration\n",
      "Topic 6: clinton hillary fbi media news election emails investigation email comey\n",
      "Topic 7: said police people city state officers man killed officials according\n",
      "Topic 8: mr said ms trump new president officials united company did\n",
      "Topic 9: la twitter el 2017 en que obama com 2016 european\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx), end=' ')\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "\n",
    "# Run LDA\n",
    "num_topics = 10\n",
    "# lda = LatentDirichletAllocation(n_components=num_topics, random_state=42).fit(count_train)\n",
    "# pkl.dump(lda, open('lda_10.pkl', 'wb'))\n",
    "lda = pkl.load(open('lda_10.pkl', 'rb'))\n",
    "print(lda.components_.shape)\n",
    "\n",
    "# display        \n",
    "num_top_words = 10\n",
    "display_topics(lda, feature_names, num_top_words)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6273\u001b[0m  0.7848\n",
      "      2        \u001b[36m0.3111\u001b[0m  0.7765\n",
      "      3        \u001b[36m0.2602\u001b[0m  0.7738\n",
      "      4        \u001b[36m0.2406\u001b[0m  0.7747\n",
      "      5        \u001b[36m0.2307\u001b[0m  0.7816\n",
      "      6        \u001b[36m0.2154\u001b[0m  0.7750\n",
      "      7        \u001b[36m0.2062\u001b[0m  0.7731\n",
      "      8        \u001b[36m0.1935\u001b[0m  0.7811\n",
      "      9        \u001b[36m0.1900\u001b[0m  0.7511\n",
      "     10        \u001b[36m0.1812\u001b[0m  0.7797\n",
      "Model Trained!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/vision/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/accounts/projects/vision/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/accounts/projects/vision/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/accounts/projects/vision/.local/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "from getEmbeddings import getEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 80)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(80, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "'''\n",
    "print('getting embdeddings...')\n",
    "xtr,xte,ytr,yte = getEmbeddings(\"datasets/train.csv\")\n",
    "np.save('./xtr', xtr)\n",
    "np.save('./xte', xte)\n",
    "np.save('./ytr', ytr)\n",
    "np.save('./yte', yte)\n",
    "'''\n",
    "\n",
    "# prepare data\n",
    "xtr = np.load('./xtr.npy').astype(np.float32)\n",
    "xte = np.load('./xte.npy').astype(np.float32)\n",
    "ytr = np.load('./ytr.npy')\n",
    "yte = np.load('./yte.npy')\n",
    "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train))).astype(np.int64)\n",
    "encoded_y = np.argmax(encoded_y, axis=1)\n",
    "label_encoder.fit(y_test)\n",
    "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test))).astype(np.int64)\n",
    "\n",
    "# fit model\n",
    "net = NeuralNetClassifier(\n",
    "    FNN,\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    train_split=None,\n",
    ")\n",
    "net.fit(x_train, encoded_y)\n",
    "print(\"Model Trained!\")\n",
    "pkl.dump(net, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.92\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "net = pkl.load(open('model.pkl', 'rb'))\n",
    "probabs = net.predict_proba(x_test)\n",
    "y_pred = np.argmax(probabs, axis=1)\n",
    "acc = np.mean(y_pred == np.argmax(encoded_y_test, axis=1))\n",
    "print(f\"acc: {acc:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
