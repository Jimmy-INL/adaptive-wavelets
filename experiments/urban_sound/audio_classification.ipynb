{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3QId2itzZlq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as oj\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = '/scratch/users/vision/data/cosmo/UrbanSound8K'\n",
    "out_dir = '/scratch/users/vision/data/cosmo/audio_models'\n",
    "from audio_helper import *\n",
    "from model import Net\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")\n",
    "from util import to_freq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import transform_wrappers\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rK59zdAezZlv"
   },
   "source": [
    "# import the data\n",
    "\n",
    "We will use the UrbanSound8K dataset to train our network. It is\n",
    "available for free `here <https://urbansounddataset.weebly.com/>`_ and contains\n",
    "10 audio classes with over 8000 audio samples! Once you have downloaded\n",
    "the compressed dataset, extract it to your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9Fyk6j-zZlw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice_file_name    100032-3-0-0.wav\n",
      "fsID                         100032\n",
      "start                             0\n",
      "end                        0.317551\n",
      "salience                          1\n",
      "fold                              5\n",
      "classID                           3\n",
      "class                      dog_bark\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "csvData = pd.read_csv(oj(data_dir, 'metadata/UrbanSound8K.csv'))\n",
    "print(csvData.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NsSsz18zZly"
   },
   "source": [
    "The 10 audio classes in the UrbanSound8K dataset are air_conditioner,\n",
    "car_horn, children_playing, dog_bark, drilling, enginge_idling,\n",
    "gun_shot, jackhammer, siren, and street_music. Let’s play a couple files\n",
    "and see what they sound like. The first file is street music and the\n",
    "second is an air conditioner.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2ma0FxmzZly"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(oj(data_dir, 'audio/fold1/108041-9-0-5.wav'))\n",
    "# ipd.Audio(oj(data_dir, 'audio/fold5/100852-0-0-19.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFyX2qeDzZl0"
   },
   "source": [
    "# create dataloader\n",
    "\n",
    "The UrbanSound8K dataset is separated\n",
    "into 10 folders. We will use the data from 9 of these folders to train\n",
    "our network and then use the 10th folder to test the network.\n",
    "\n",
    "We use ``torchaudio.load()`` to convert the wav\n",
    "files to tensors. ``torchaudio.load()`` returns a tuple containing the\n",
    "newly created tensor along with the sampling frequency of the audio file\n",
    "(44.1kHz for UrbanSound8K). The dataset uses two channels for audio so\n",
    "we will use ``torchaudio.transforms.DownmixMono()`` to convert the audio\n",
    "data to one channel. \n",
    "\n",
    "Next, we need to format the audio data. The network\n",
    "we will make takes an input size of 32,000, while most of the audio\n",
    "files have well over 100,000 samples. The UrbanSound8K audio is sampled\n",
    "at 44.1kHz, so 32,000 samples only covers around 700 milliseconds. By\n",
    "downsampling the audio to aproximately 8kHz, we can represent 4 seconds\n",
    "with the 32,000 samples. This downsampling is achieved by taking every\n",
    "fifth sample of the original audio tensor. Not every audio tensor is\n",
    "long enough to handle the downsampling so these tensors will need to be\n",
    "padded with zeros. The minimum length that won’t require padding is\n",
    "160,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWeTIFBwzZl1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 7895\n",
      "Test set size: 837\n"
     ]
    }
   ],
   "source": [
    "csv_path = oj(data_dir, 'metadata/UrbanSound8K.csv')\n",
    "file_path = oj(data_dir, 'audio/')\n",
    "\n",
    "train_set = UrbanSoundDataset(csv_path, file_path, range(1, 10))\n",
    "test_set = UrbanSoundDataset(csv_path, file_path, [10])\n",
    "print(\"Train set size: \" + str(len(train_set)))\n",
    "print(\"Test set size: \" + str(len(test_set)))\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vltxeEaezZl3"
   },
   "source": [
    "# training / testing\n",
    "\n",
    "For this tutorial we will use a convolutional neural network to process\n",
    "the raw audio data. Usually more advanced transforms are applied to the\n",
    "audio data, however CNNs can be used to accurately process the raw data.\n",
    "The specific architecture is modeled after the M5 network architecture\n",
    "described in https://arxiv.org/pdf/1610.00087.pdf. An important aspect\n",
    "of models processing raw audio data is the receptive field of their\n",
    "first layer’s filters. Our model’s first filter is length 80 so when\n",
    "processing audio sampled at 8kHz the receptive field is around 10ms.\n",
    "This size is similar to speech processing applications that often use\n",
    "receptive fields ranging from 20ms to 40ms.\n",
    "\n",
    "If trained on 9 folders, the network should be more than 50% accurate by\n",
    "the end of the training process. Training on less folders will result in\n",
    "a lower overall accuracy but may be necessary if long runtimes are a\n",
    "problem. Greater accuracies can be achieved using deeper CNNs at the\n",
    "expense of a larger memory footprint.\n",
    "\n",
    "For more advanced audio applications, such as speech recognition,\n",
    "recurrent neural networks (RNNs) are commonly used. There are also other\n",
    "data preprocessing methods, such as finding the mel frequency cepstral\n",
    "coefficients (MFCC), that can reduce the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t_BxNxLzZl4"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLOJbHX_zZmD"
   },
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "for epoch in range(1, 21):\n",
    "    # maybe set lr to 0.001 at epoch 31\n",
    "    scheduler.step()\n",
    "    train(model, epoch, optimizer, train_loader, device)\n",
    "    acc = test(model, test_loader, device)\n",
    "    pkl.dump(deepcopy(model), open(oj(out_dir, f'audio_model_{epoch}_{acc:.1f}.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 479/837 (57%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.22819593787336"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pkl.load(open(oj(out_dir, 'audio_model_7_57.2.pkl'), 'rb'))\n",
    "test(model, test_loader, device) # should print the test acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at individual example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set[0]\n",
    "x = x.to(device)\n",
    "# x_np = x.cpu().flatten().numpy()\n",
    "# plt.plot(x_np[:2000])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.rfft(deepcopy(x), signal_ndim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.irfft(xt, signal_ndim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lambda x: torch.irfft(x, signal_ndim=1)[:, :-1] #transform_wrappers.lay_from_w(D)\n",
    "net = transform_wrappers.Net_with_transform(model, \n",
    "                                            transform=transform,\n",
    "                                            reshape=transform_wrappers.ReshapeLayer((1, 32000))).to(device)\n",
    "# print(list(net.modules()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(xt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8851, -1.8586, -3.2695, -1.4533, -2.9915, -5.7169, -1.2332, -4.4807,\n",
       "         -1.5578, -5.4086]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 645/837 [44:54<13:26,  4.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 646/837 [44:58<13:17,  4.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 647/837 [45:02<13:10,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 648/837 [45:06<13:05,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 649/837 [45:10<13:00,  4.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 650/837 [45:14<12:54,  4.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 651/837 [45:19<12:51,  4.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 652/837 [45:23<12:47,  4.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 653/837 [45:27<12:44,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 654/837 [45:31<12:41,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 655/837 [45:35<12:37,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 656/837 [45:39<12:35,  4.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 78%|███████▊  | 657/837 [45:44<12:28,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▊  | 658/837 [45:48<12:22,  4.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▊  | 659/837 [45:52<12:17,  4.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 660/837 [45:56<12:17,  4.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 661/837 [46:01<12:45,  4.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 662/837 [46:05<12:35,  4.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 79%|███████▉  | 663/837 [46:09<12:39,  4.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    key: [] for key in range(10)\n",
    "}\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    x, y = test_set[i]\n",
    "    xt = torch.rfft(deepcopy(x), signal_ndim=1)\n",
    "\n",
    "    # this only works with cpu\n",
    "    device_captum = 'cpu'\n",
    "    net = net.to(device_captum)\n",
    "    xt = xt.to(device_captum)\n",
    "    # x = xt.unsqueeze(0).to(device_captum)\n",
    "    xt.requires_grad = True\n",
    "    baseline = torch.zeros(xt.shape).to(device_captum)\n",
    "    ig = IntegratedGradients(net.to(device_captum))\n",
    "    attributions_ig, delta_ig = ig.attribute(deepcopy(xt), deepcopy(baseline),\n",
    "                                             target=int(y), return_convergence_delta=True)\n",
    "    attributions_ig = to_freq(attributions_ig)\n",
    "    results[y].append(deepcopy(attributions_ig))\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        pkl.dump(results, open(oj(out_dir, 'audio_ig_correct.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "audio_classifier_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
